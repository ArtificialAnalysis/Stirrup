{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Stirrup","text":"<p>Stirrup is a lightweight framework, or starting point template, for building agents. It differs from other agent frameworks by:</p> <ul> <li>Working with the model, not against it: Stirrup gets out of the way and lets the model choose its own approach to completing tasks (similar to Claude Code). Many frameworks impose rigid workflows that can degrade results.</li> <li>Best practices and tools built-in: We analyzed the leading agents (Claude Code, Codex, and others) to understand and incorporate best practices relating to topics like context management and foundational tools (e.g., code execution).</li> <li>Fully customizable: Use Stirrup as a package or as a starting template to build your own fully customized agents.</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Essential tools built-in:<ul> <li>Online search / web browsing</li> <li>Code execution (local, Docker container, E2B sandbox)</li> <li>MCP client</li> <li>Document input and output</li> </ul> </li> <li>Skills system: Extend agent capabilities with modular, domain-specific instruction packages</li> <li>Flexible tool execution: A generic <code>Tool</code> class allows easy tool definition and extension</li> <li>Context management: Automatically summarizes conversation history when approaching context limits</li> <li>Flexible provider support: Pre-built support for OpenAI-compatible APIs and LiteLLM, or bring your own client</li> <li>Multimodal support: Process images, video, and audio with automatic format conversion</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Core framework\npip install stirrup      # or: uv add stirrup\n\n# With all optional components\npip install 'stirrup[all]'  # or: uv add 'stirrup[all]'\n\n# Individual extras\npip install 'stirrup[litellm]'  # or: uv add 'stirrup[litellm]'\npip install 'stirrup[docker]'   # or: uv add 'stirrup[docker]'\npip install 'stirrup[e2b]'      # or: uv add 'stirrup[e2b]'\npip install 'stirrup[mcp]'      # or: uv add 'stirrup[mcp]'\npip install 'stirrup[browser]'  # or: uv add 'stirrup[browser]'\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent that searches the web and creates a chart.\"\"\"\n\n    # Create client using ChatCompletionsClient\n    # Automatically uses OPENROUTER_API_KEY environment variable\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # As no tools are provided, the agent will use the default tools, which consist of:\n    # - Web tools (web search and web fetching, note web search requires BRAVE_API_KEY)\n    # - Local code execution tool (to execute shell commands)\n    agent = Agent(client=client, name=\"agent\", max_turns=15)\n\n    # Run with session context - handles tool lifecycle, logging and file outputs\n    async with agent.session(output_dir=\"output/getting_started_example\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"\"\"\n        What is the population of the US over the last 3 years? Search the web to\n        find out and create a chart using matplotlib showing the population per year.\n        \"\"\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Environment Variables</p> <p>This example uses OpenRouter. Set <code>OPENROUTER_API_KEY</code> in your environment before running.</p> <p>Web search requires a <code>BRAVE_API_KEY</code>. The agent will still work without it, but web search will be unavailable.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<ul> <li><code>Agent</code> - Configures and runs the agent loop until a finish tool is called or max turns reached</li> <li><code>session()</code> - Context manager that sets up tools, manages files, and handles cleanup</li> <li><code>Tool</code> - Define tools with Pydantic parameters</li> <li><code>ToolProvider</code> - Manage tools that require lifecycle (connections, temp directories, etc.)</li> <li><code>DEFAULT_TOOLS</code> - Standard tools included by default: code execution and web tools</li> </ul>"},{"location":"#using-other-llm-providers","title":"Using Other LLM Providers","text":"<p>For non-OpenAI providers, change the base URL of the <code>ChatCompletionsClient</code>, use the <code>LiteLLMClient</code> (requires installation of optional <code>stirrup[litellm]</code> dependencies), or create your own client.</p>"},{"location":"#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<pre><code>    # Create client using Deepseek's OpenAI-compatible endpoint\n    client = ChatCompletionsClient(\n        base_url=\"https://api.deepseek.com\",\n        model=\"deepseek-chat\",  # or \"deepseek-reasoner\" for R1\n        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n    )\n\n    agent = Agent(client=client, name=\"deepseek_agent\")\n</code></pre>"},{"location":"#litellm-anthropic-google-etc","title":"LiteLLM (Anthropic, Google, etc.)","text":"<pre><code>    # Create LiteLLM client for Anthropic Claude\n    # See https://docs.litellm.ai/docs/providers for all supported providers\n    client = LiteLLMClient(model_slug=\"anthropic/claude-sonnet-4-5\", max_tokens=64_000)\n\n    # Pass client to Agent - model info comes from client.model_slug\n    agent = Agent(\n        client=client,\n        name=\"claude_agent\",\n    )\n</code></pre> <p>See LiteLLM Example or Deepseek Example for complete examples.</p>"},{"location":"#default-tools","title":"Default Tools","text":"<p>When you create an <code>Agent</code> without specifying tools, it uses <code>DEFAULT_TOOLS</code>:</p> Tool Provider Tools Provided Description <code>LocalCodeExecToolProvider</code> <code>code_exec</code> Execute shell commands in an isolated temp directory <code>WebToolProvider</code> <code>web_fetch</code>, <code>web_search</code> Fetch web pages and search (search requires <code>BRAVE_API_KEY</code>)"},{"location":"#extending-with-pre-built-tools","title":"Extending with Pre-Built Tools","text":"<pre><code>import argparse\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL\nfrom stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\nfrom stirrup.tools.web import WebToolProvider\n\nDEFAULT_OPENROUTER_SLUG = \"anthropic/claude-sonnet-4.5\"\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=DEFAULT_OPENROUTER_SLUG,\n)\n\n# Create agent with E2B execution + web tools + calculator\n# (This is just for the docs snippet above \u2014 the actual runnable code is in main() below)\nagent = Agent(\n    client=client,\n    name=\"web_calculator_agent\",\n    tools=[E2BCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL],\n)\n</code></pre>"},{"location":"#defining-custom-tools","title":"Defining Custom Tools","text":"<pre><code>class GreetParams(BaseModel):\n    \"\"\"Parameters for the greet tool.\"\"\"\n\n    name: str = Field(description=\"Name of the person to greet\")\n    formal: bool = Field(default=False, description=\"Use formal greeting\")\n\n\ndef greet(params: GreetParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    greeting = f\"Good day, {params.name}.\" if params.formal else f\"Hey {params.name}!\"\n\n    return ToolResult(\n        content=greeting,\n        metadata=ToolUseCountMetadata(),\n    )\n\n\nGREET_TOOL = Tool(\n    name=\"greet\",\n    description=\"Greet someone by name\",\n    parameters=GreetParams,\n    executor=greet,\n)\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"anthropic/claude-sonnet-4.5\",\n)\n\n# Add custom tool to default tools\nagent = Agent(\n    client=client,\n    name=\"greeting_agent\",\n    tools=[*DEFAULT_TOOLS, GREET_TOOL],\n)\n</code></pre>"},{"location":"#full-customization","title":"Full Customization","text":"<p>For deep customization of the framework internals, you can clone and import Stirrup locally:</p> <pre><code># Clone the repository\ngit clone https://github.com/ArtificialAnalysis/Stirrup.git\ncd stirrup\n\n# Install in editable mode\npip install -e .      # or: uv venv &amp;&amp; uv pip install -e .\n\n# Or with all optional dependencies\npip install -e '.[all]'  # or: uv venv &amp;&amp; uv pip install -e '.[all]'\n</code></pre> <p>See the Full Customization guide for more details.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Installation and first agent tutorial</li> <li>Core Concepts - Understand Agent, Tools, and Sessions</li> <li>Examples - Working examples for common patterns</li> <li>Creating Tools - Build your own tools</li> <li>Skills - Extend agents with domain-specific expertise</li> </ul>"},{"location":"concepts/","title":"Core Concepts","text":"<p>This page explains the fundamental concepts in Stirrup.</p>"},{"location":"concepts/#agent","title":"Agent","text":"<p>The <code>Agent</code> class is the main entry point. It manages the agent loop: generating LLM responses, executing tools, and accumulating messages until a task is complete.</p>"},{"location":"concepts/#configuration-options","title":"Configuration Options","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\nclient = ChatCompletionsClient(...)\n\nagent = Agent(\n    client=client,                                        # (required) LLM client for generating responses\n    name=\"my_agent\",                                      # (required) Agent name for logging\n    max_turns=30,                                         # (default: 30) Max iterations before stopping\n    system_prompt=\"You are an agent specializing in ...\", # (default: None) Instructions prepended to runs\n    tools=None,                                           # (default: DEFAULT_TOOLS) Available tools\n    finish_tool=None,                                     # (default: SIMPLE_FINISH_TOOL) Completion signal\n    context_summarization_cutoff=0.7,                     # (default: 0.7) Context % before summarization\n    run_sync_in_thread=True,                              # (default: True) Run sync tools in thread\n    text_only_tool_responses=True,                        # (default: True) Extract images from responses\n    block_successive_assistant_messages=True,               # (default: True) Inject continue prompt between assistant messages\n    logger=None,                                          # (default: None) Custom logger instance\n)\n</code></pre> Full Parameter Reference Parameter Type Default Description <code>client</code> <code>LLMClient</code> required LLM client (use factory methods or create directly) <code>name</code> <code>str</code> required Agent name for logging <code>max_turns</code> <code>int</code> <code>30</code> Maximum turns before stopping <code>system_prompt</code> <code>str \\| None</code> <code>None</code> System prompt prepended to runs <code>tools</code> <code>list[Tool \\| ToolProvider] \\| None</code> <code>DEFAULT_TOOLS</code> Available tools <code>finish_tool</code> <code>Tool</code> <code>SIMPLE_FINISH_TOOL</code> Tool to signal completion <code>context_summarization_cutoff</code> <code>float</code> <code>0.7</code> Context % before summarization <code>run_sync_in_thread</code> <code>bool</code> <code>True</code> Run sync tools in separate thread <code>text_only_tool_responses</code> <code>bool</code> <code>True</code> Extract images to user messages <code>block_successive_assistant_messages</code> <code>bool</code> <code>True</code> Inject continue prompt to prevent back-to-back assistant messages <code>logger</code> <code>AgentLoggerBase \\| None</code> <code>None</code> Custom logger instance"},{"location":"concepts/#understanding-agent-output","title":"Understanding Agent Output","text":"<p>The <code>run()</code> method returns a tuple of three values:</p> <pre><code>finish_params, history, metadata = await session.run(\"Your task\")\n</code></pre>"},{"location":"concepts/#finish_params","title":"<code>finish_params</code>","text":"<p>Contains the agent's final response when it calls the finish tool:</p> <ul> <li><code>reason</code>: Explanation of what was accomplished</li> <li><code>paths</code>: List of files created/modified in the execution environment</li> </ul> <pre><code>finish_params = {\n    \"reason\": \"Successfully found Australia's population for 2022-2024 and created a chart.\",\n    \"paths\": [\"australia_population_chart.png\"]\n}\n</code></pre>"},{"location":"concepts/#history","title":"<code>history</code>","text":"<p>A list of message groups representing the conversation history. Each group contains:</p> <ul> <li><code>SystemMessage</code>: System prompts</li> <li><code>UserMessage</code>: User inputs and file contents</li> <li><code>AssistantMessage</code>: LLM responses with tool calls</li> <li><code>ToolMessage</code>: Results from tool executions</li> </ul> <pre><code>history = [\n    SystemMessage(role='system', content=\"You are an AI agent...\"),\n    UserMessage(role='user', content=\"What is the population of Australia...\"),\n    AssistantMessage(\n        role='assistant',\n        content=\"I'll search for Australia's population data...\",\n        tool_calls=[ToolCall(name='web_search', arguments='{\"query\": \"...\"}', tool_call_id='...')],\n        token_usage=TokenUsage(input=1523, answer=156, reasoning=0)\n    ),\n    ToolMessage(role='tool', content=\"&lt;results&gt;...ABS data...&lt;/results&gt;\", name='web_search', ...),\n    # ... additional turns ...\n    AssistantMessage(\n        role='assistant',\n        content=\"All files are ready. Let me finish the task.\",\n        tool_calls=[ToolCall(name='finish', arguments='{\"reason\": \"...\", \"paths\": [...]}', ...)],\n        token_usage=TokenUsage(input=25102, answer=285, reasoning=0)\n    ),\n    ToolMessage(role='tool', content=\"Successfully completed...\", name='finish', ...),\n]\n</code></pre>"},{"location":"concepts/#metadata","title":"<code>metadata</code>","text":"<p>A dictionary containing metadata from tool executions:</p> <ul> <li><code>token_usage</code>: Total token counts (input, output, reasoning)</li> <li>Per-tool metadata (e.g., <code>code_exec</code>, <code>web_search</code>, <code>web_fetch</code>)</li> </ul> <pre><code>metadata = {\n    \"web_search\": [WebSearchMetadata(num_uses=1, pages_returned=5)],\n    \"fetch_web_page\": [WebFetchMetadata(num_uses=1, pages_fetched=['https://...'])],\n    \"code_exec\": [ToolUseCountMetadata(num_uses=3)],\n    \"finish\": [ToolUseCountMetadata(num_uses=1)],\n    \"token_usage\": [TokenUsage(input=239283, answer=4189, reasoning=0)]\n}\n</code></pre> <p>Use <code>aggregate_metadata</code> to combine metadata across tool calls:</p> <pre><code>from stirrup import aggregate_metadata\n\naggregated = aggregate_metadata(metadata)\nprint(f\"Total tokens: {aggregated['token_usage'].total}\")\n</code></pre> <p>Speed metrics are available directly on each <code>AssistantMessage</code> via <code>request_start_time</code>, <code>request_end_time</code>, and the derived <code>e2e_otps</code> property. Similarly, <code>ToolMessage</code> has <code>tool_start_time</code>, <code>tool_end_time</code>, and a <code>tool_duration</code> property.</p>"},{"location":"concepts/#session","title":"Session","text":"<p>The <code>session()</code> method returns the agent configured as an async context manager. Sessions handle:</p> <ul> <li>Tool lifecycle (setup and teardown of ToolProviders)</li> <li>File uploads to execution environment</li> <li>Skills loading and system prompt addition</li> <li>Output file saving</li> <li>Logging</li> </ul> <pre><code>async with agent.session(\n    output_dir=\"./output\",           # Where to save output files\n    input_files=[\"data.csv\"],        # Files to upload\n    skills_dir=\"skills\",             # Directory containing skills\n) as session:\n    result = await session.run(\"Your task\")\n</code></pre>"},{"location":"concepts/#passing-input-files-to-the-agent","title":"Passing Input Files to the Agent","text":"<p>Provide files to the agent's execution environment via <code>input_files</code>:</p> <pre><code>async with agent.session(\n    input_files=[\"data.csv\", \"config.json\"],\n    output_dir=\"./output\",\n) as session:\n    await session.run(\"Analyze the data in data.csv\")\n</code></pre> <p>Supported formats:</p> Format Example Description Single file <code>\"data.csv\"</code> Upload one file Multiple files <code>[\"file1.txt\", \"file2.txt\"]</code> Upload a list of files Directory <code>\"./data/\"</code> Upload directory contents recursively Glob pattern <code>\"data/*.csv\"</code>, <code>\"**/*.py\"</code> Upload files matching pattern"},{"location":"concepts/#receiving-output-files-from-the-agent","title":"Receiving Output Files from the Agent","text":"<p>When the agent creates files, save them to a local directory via <code>output_dir</code>:</p> <pre><code>async with agent.session(output_dir=\"./results\") as session:\n    finish_params, _, _ = await session.run(\n        \"Create a Python script that prints hello world\"\n    )\n    # Files listed in finish_params.paths are saved to ./results/\n</code></pre> <p>The agent signals which files to save by including their paths in <code>finish_params.paths</code> when calling the finish tool.</p>"},{"location":"concepts/#loading-skills","title":"Loading Skills","text":"<p>Skills are modular packages that extend agent capabilities with domain-specific instructions and scripts. Pass a skills directory to make them available:</p> <pre><code>async with agent.session(\n    skills_dir=\"skills\",\n    output_dir=\"./output\",\n) as session:\n    await session.run(\"Analyze the data using the data_analysis skill\")\n</code></pre> <p>The agent receives a list of available skills in its system prompt and can read the full instructions via <code>cat skills/&lt;skill_name&gt;/SKILL.md</code>.</p> <p>\u2192 See Skills Guide for full documentation.</p>"},{"location":"concepts/#client","title":"Client","text":"<p>Stirrup supports multiple ways to connect to LLM providers.</p>"},{"location":"concepts/#chatcompletionsclient","title":"ChatCompletionsClient","text":"<p>Use <code>ChatCompletionsClient</code> for OpenAI or OpenAI-compatible APIs:</p> <pre><code>    # Create client using Deepseek's OpenAI-compatible endpoint\n    client = ChatCompletionsClient(\n        base_url=\"https://api.deepseek.com\",\n        model=\"deepseek-chat\",  # or \"deepseek-reasoner\" for R1\n        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n    )\n\n    agent = Agent(client=client, name=\"deepseek_agent\")\n</code></pre> Parameter Type Default Description <code>model</code> <code>str</code> required Model identifier (e.g., <code>\"gpt-5\"</code>, <code>\"deepseek-chat\"</code>) <code>max_tokens</code> <code>int</code> <code>64_000</code> Context window size <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom API URL (for Deepseek, vLLM, etc.) <code>api_key</code> <code>str \\| None</code> <code>None</code> API key (defaults to <code>OPENROUTER_API_KEY</code> env var) <code>timeout</code> <code>float \\| None</code> <code>None</code> Request timeout in seconds <code>max_retries</code> <code>int</code> <code>2</code> Number of retries for transient errors"},{"location":"concepts/#litellmclient","title":"LiteLLMClient","text":"<p>Use <code>LiteLLMClient</code> for Anthropic, Google, and other providers via LiteLLM:</p> <pre><code>    # Create LiteLLM client for Anthropic Claude\n    # See https://docs.litellm.ai/docs/providers for all supported providers\n    client = LiteLLMClient(model_slug=\"anthropic/claude-sonnet-4-5\", max_tokens=64_000)\n\n    # Pass client to Agent - model info comes from client.model_slug\n    agent = Agent(\n        client=client,\n        name=\"claude_agent\",\n    )\n</code></pre> Parameter Type Default Description <code>model_slug</code> <code>str</code> required Provider/model string (e.g., <code>\"anthropic/claude-sonnet-4-5\"</code>) <code>max_tokens</code> <code>int</code> required Context window size <code>reasoning_effort</code> <code>str \\| None</code> <code>None</code> For reasoning models (o1/o3) <code>kwargs</code> <code>dict \\| None</code> <code>None</code> Additional provider-specific arguments <p>LiteLLM Installation</p> <p>Requires <code>pip install stirrup[litellm]</code> (or: <code>uv add stirrup[litellm]</code>)</p>"},{"location":"concepts/#creating-your-own-client","title":"Creating Your Own Client","text":"<p>Implement the <code>LLMClient</code> protocol to create a custom client:</p> <pre><code>from stirrup.core.models import LLMClient, AssistantMessage, ChatMessage, Tool\n\nclass MyCustomClient(LLMClient):\n    async def generate(self, messages: list[ChatMessage], tools: dict[str, Tool]) -&gt; AssistantMessage:\n        # Make API call and return AssistantMessage\n        ...\n\n    @property\n    def model_slug(self) -&gt; str:\n        return \"my-model\"\n\n    @property\n    def max_tokens(self) -&gt; int:\n        return 128_000\n</code></pre> <p>\u2192 See Custom Clients for full documentation.</p>"},{"location":"concepts/#tools","title":"Tools","text":""},{"location":"concepts/#default_tools","title":"DEFAULT_TOOLS","text":"<p>When you create an Agent without specifying tools, it uses <code>DEFAULT_TOOLS</code>:</p> <pre><code>from stirrup.tools import DEFAULT_TOOLS\n\n# DEFAULT_TOOLS contains:\n# - LocalCodeExecToolProvider() \u2192 provides \"code_exec\" tool\n# - WebToolProvider() \u2192 provides \"web_fetch\" and \"web_search\" tools\n</code></pre> Tool Provider Tools Provided Description <code>LocalCodeExecToolProvider</code> <code>code_exec</code> Execute shell commands in an isolated temp directory <code>WebToolProvider</code> <code>web_fetch</code>, <code>web_search</code> Fetch web pages and search (search requires <code>BRAVE_API_KEY</code>)"},{"location":"concepts/#extending-vs-replacing","title":"Extending vs Replacing","text":"<pre><code>import argparse\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL\nfrom stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\nfrom stirrup.tools.web import WebToolProvider\n\nDEFAULT_OPENROUTER_SLUG = \"anthropic/claude-sonnet-4.5\"\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=DEFAULT_OPENROUTER_SLUG,\n)\n\n# Create agent with E2B execution + web tools + calculator\n# (This is just for the docs snippet above \u2014 the actual runnable code is in main() below)\nagent = Agent(\n    client=client,\n    name=\"web_calculator_agent\",\n    tools=[E2BCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL],\n)\n</code></pre>"},{"location":"concepts/#tool","title":"Tool","text":"<p>A <code>Tool</code> has the following attributes:</p> <ul> <li>name: Unique identifier</li> <li>description: What the tool does (shown to the LLM)</li> <li>parameters: Pydantic model defining the input schema</li> <li>executor: Function that executes the tool</li> </ul> <pre><code>class GreetParams(BaseModel):\n    \"\"\"Parameters for the greet tool.\"\"\"\n\n    name: str = Field(description=\"Name of the person to greet\")\n    formal: bool = Field(default=False, description=\"Use formal greeting\")\n\n\ndef greet(params: GreetParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    greeting = f\"Good day, {params.name}.\" if params.formal else f\"Hey {params.name}!\"\n\n    return ToolResult(\n        content=greeting,\n        metadata=ToolUseCountMetadata(),\n    )\n\n\nGREET_TOOL = Tool(\n    name=\"greet\",\n    description=\"Greet someone by name\",\n    parameters=GreetParams,\n    executor=greet,\n)\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"anthropic/claude-sonnet-4.5\",\n)\n\n# Add custom tool to default tools\nagent = Agent(\n    client=client,\n    name=\"greeting_agent\",\n    tools=[*DEFAULT_TOOLS, GREET_TOOL],\n)\n</code></pre> <p>\u2192 See Creating Tools for full documentation.</p>"},{"location":"concepts/#sub-agents","title":"Sub-agents","text":"<p>Convert any agent into a tool using <code>agent.to_tool()</code>. This enables hierarchical agent patterns where a supervisor delegates to specialized workers:</p> <pre><code>    research_agent = Agent(\n        client=client,\n        name=\"research_sub_agent\",\n        tools=[WebToolProvider(), LocalCodeExecToolProvider()],\n        max_turns=5,\n        system_prompt=(\n            \"You are a research agent. When asked to complete research, save it all to a markdown file \"\n            \"(using a code executor tool) and pass the filepath to the finish tool and mention it in the \"\n            \"finish_reason. Remember you will need a turn to write the markdown file and a separate turn to finish.\"\n        ),\n    )\n\n    # Convert agent to a tool for use by supervisor\n    research_subagent_tool = research_agent.to_tool(\n        description=\"Agent that can search the web and return the results.\",\n    )\n</code></pre> <p>The supervisor can then use sub-agents as tools:</p> <pre><code>supervisor_agent = Agent(\n    client=client,\n    name=\"supervisor\",\n    tools=[research_subagent_tool, writer_subagent_tool],\n)\n</code></pre> <p>\u2192 See Sub-Agents Guide for full documentation.</p>"},{"location":"concepts/#tool-provider","title":"Tool Provider","text":"<p>A <code>ToolProvider</code> is a class that manages resources and returns tools via async context manager. Use for tools requiring:</p> <ul> <li>Connections (HTTP clients, databases)</li> <li>Temporary directories</li> <li>Cleanup logic</li> </ul> <pre><code>from stirrup import ToolProvider, Tool\n\nclass MyToolProvider(ToolProvider):\n    async def __aenter__(self) -&gt; Tool | list[Tool]:\n        # Setup resources\n        self.client = await create_client()\n        return self._create_tool()\n\n    async def __aexit__(self, *args):\n        # Cleanup\n        await self.client.close()\n</code></pre> <p>The agent's <code>session()</code> automatically calls <code>__aenter__</code> and <code>__aexit__</code> for all ToolProviders.</p> <p>\u2192 See Tool Providers for full documentation.</p>"},{"location":"concepts/#finish-tools","title":"Finish Tools","text":"<p>The finish tool signals task completion. By default, agents use <code>SIMPLE_FINISH_TOOL</code>:</p> <pre><code>from stirrup.tools.finish import FinishParams, SIMPLE_FINISH_TOOL\n\n# Default FinishParams has:\n# - reason: str - Explanation of what was accomplished\n# - paths: list[str] - Files created/modified\n</code></pre> <p>Create custom finish tools for structured output:</p> <pre><code>from pydantic import BaseModel, Field\nfrom stirrup import Tool, ToolResult, ToolUseCountMetadata\n\nclass AnalysisResult(BaseModel):\n    summary: str = Field(description=\"Analysis summary\")\n    confidence: float = Field(description=\"Confidence score 0-1\")\n    paths: list[str] = Field(default_factory=list)\n\ncustom_finish = Tool(\n    name=\"finish\",\n    description=\"Complete the analysis task\",\n    parameters=AnalysisResult,\n    executor=lambda p: ToolResult(\n        content=p.summary,\n        metadata=ToolUseCountMetadata()\n    ),\n)\n\nagent = Agent(client=client, name=\"analyst\", finish_tool=custom_finish)\n</code></pre>"},{"location":"concepts/#tool-metadata","title":"Tool Metadata","text":"<p>Tools return <code>ToolResult[M]</code> where <code>M</code> is the metadata type:</p> <pre><code>from stirrup import ToolResult, ToolUseCountMetadata\n\ndef my_tool(params: MyParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    return ToolResult(\n        content=\"Result text\",\n        metadata=ToolUseCountMetadata(),  # Tracks number of uses\n    )\n</code></pre> <p>Metadata aggregates across tool calls during a run. Built-in metadata types:</p> Type Description <code>ToolUseCountMetadata</code> Counts number of tool invocations <code>TokenUsage</code> Tracks input/output/reasoning tokens <code>SubAgentMetadata</code> Captures sub-agent message history <p>Access aggregated metadata:</p> <pre><code>from stirrup import aggregate_metadata\n\n_, _, metadata = await session.run(\"task\")\naggregated = aggregate_metadata(metadata)\nprint(f\"Total tokens: {aggregated['token_usage'].total}\")\n</code></pre>"},{"location":"concepts/#logging","title":"Logging","text":"<p>The agent uses <code>AgentLogger</code> by default, which provides rich console output with:</p> <ul> <li>Progress spinners showing steps, tool calls, and token usage</li> <li>Visual hierarchy for sub-agents</li> <li>Syntax-highlighted tool results</li> </ul> <pre><code>from stirrup.utils.logging import AgentLogger\nimport logging\n\n# Custom log level\nlogger = AgentLogger(level=logging.DEBUG)\nagent = Agent(client=client, name=\"assistant\", logger=logger)\n</code></pre>"},{"location":"concepts/#custom-loggers","title":"Custom Loggers","text":"<p>Implement <code>AgentLoggerBase</code> for custom logging:</p> <pre><code>from stirrup.utils.logging import AgentLoggerBase\n\nclass MyLogger(AgentLoggerBase):\n    def __enter__(self):\n        # Setup logging\n        return self\n\n    def __exit__(self, *args):\n        # Cleanup\n        pass\n\n    def on_step(self, step: int, tool_calls: int = 0, input_tokens: int = 0, output_tokens: int = 0):\n        # Called after each step\n        print(f\"Step {step}: {tool_calls} tool calls\")\n\n    # Implement other required methods...\n</code></pre> <p>\u2192 See Custom Loggers for full documentation.</p>"},{"location":"concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Examples - Working examples for common patterns</li> <li>Creating Tools - Build your own tools</li> <li>Code Execution - Execution backends</li> <li>Sub-Agents - Hierarchical agent patterns</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Working examples demonstrating common Stirrup patterns. Full source code is in the <code>examples/</code> directory.</p>"},{"location":"examples/#web-search-agent","title":"Web Search Agent","text":"<p>A simple agent using default tools to search the web.</p> <pre><code>\"\"\"Getting started example for Stirrup.\n\nDemonstrates the core pattern:\n1. Create a ChatCompletionsClient for your LLM provider\n2. Create an Agent with the client\n3. Run it in a session context\n4. Agent searches the web and creates a chart as output\n\"\"\"\n\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent that searches the web and creates a chart.\"\"\"\n\n    # Create client using ChatCompletionsClient\n    # Automatically uses OPENROUTER_API_KEY environment variable\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # As no tools are provided, the agent will use the default tools, which consist of:\n    # - Web tools (web search and web fetching, note web search requires BRAVE_API_KEY)\n    # - Local code execution tool (to execute shell commands)\n    agent = Agent(client=client, name=\"agent\", max_turns=15)\n\n    # Run with session context - handles tool lifecycle, logging and file outputs\n    async with agent.session(output_dir=\"output/getting_started_example\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"\"\"\n        What is the population of the US over the last 3 years? Search the web to\n        find out and create a chart using matplotlib showing the population per year.\n        \"\"\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Note</p> <p>Web search requires <code>BRAVE_API_KEY</code> environment variable.</p>"},{"location":"examples/#web-calculator","title":"Web Calculator","text":"<p>An agent with calculator added to default tools.</p> <pre><code>\"\"\"Example: Web-enabled calculator agent with simplified session API.\n\nThis example demonstrates how to create an agent that can:\n1. Perform calculations\n2. Search the web (requires BRAVE_API_KEY)\n3. Fetch web page content\n\"\"\"\n\nimport argparse\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL\nfrom stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\nfrom stirrup.tools.web import WebToolProvider\n\nDEFAULT_OPENROUTER_SLUG = \"anthropic/claude-sonnet-4.5\"\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=DEFAULT_OPENROUTER_SLUG,\n)\n\n# Create agent with E2B execution + web tools + calculator\n# (This is just for the docs snippet above \u2014 the actual runnable code is in main() below)\nagent = Agent(\n    client=client,\n    name=\"web_calculator_agent\",\n    tools=[E2BCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL],\n)\n\n\nasync def main(openrouter_slug: str) -&gt; None:\n    \"\"\"Run a simple web-enabled calculator agent.\"\"\"\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=openrouter_slug,\n        max_tokens=50_000,\n    )\n\n    # Create agent with E2B execution + web tools + calculator\n    tools = [E2BCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL]\n    agent = Agent(\n        client=client,\n        name=\"web_calculator_agent\",\n        tools=tools,\n        max_turns=10,\n    )\n\n    # Run with session context - handles all tool lifecycle and logging\n    async with agent.session(output_dir=\"output\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"\"\"Find the current world population and calculate what 10% of it would be.\n            Use the web_search tool to find the current world population, then use\n            the calculator to compute 10% of that number.\n            When you're done, call the finish tool with your findings.\"\"\"\n        )\n\n\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    \"\"\"Parse CLI arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run the web calculator example with an OpenRouter model slug.\")\n    parser.add_argument(\n        \"openrouter_slug\",\n        nargs=\"?\",\n        default=DEFAULT_OPENROUTER_SLUG,\n        help=f\"OpenRouter model slug to use (default: {DEFAULT_OPENROUTER_SLUG})\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    asyncio.run(main(args.openrouter_slug))\n</code></pre>"},{"location":"examples/#code-execution","title":"Code Execution","text":"<p>Execute code in isolated environments with multiple backend options.</p> <pre><code>\"\"\"Example: Code execution agent (E2B, Docker, or local).\n\nThis example demonstrates how to create an agent that can execute shell or Python\ncode in an isolated execution environment using one of several backends (E2B, Docker, or local).\nYou can switch between backends by commenting/uncommenting the relevant execution\nenvironment instantiation.\n\nPrerequisites for E2B:\n- Set E2B_API_KEY environment variable.\n\nSee accompanying comments in the file for backend options.\n\"\"\"\n\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools.code_backends.local import LocalCodeExecToolProvider\n\n\nasync def main() -&gt; None:\n    \"\"\"Run a simple code execution agent.\"\"\"\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # Choose your backend:\n    code_exec_tool_provider = LocalCodeExecToolProvider()  # Local\n    # code_exec_tool_provider = DockerCodeExecToolProvider.from_image(\"python:3.12-slim\")  # Docker\n    # code_exec_tool_provider = E2BCodeExecToolProvider()  # E2B (requires E2B_API_KEY)\n\n    agent = Agent(\n        client=client,\n        name=\"code_executor_agent\",\n        tools=[code_exec_tool_provider],\n        max_turns=20,\n    )\n\n    async with agent.session(\n        input_files=\"examples/code_executor/task.txt\",\n        output_dir=\"output/code_executor_example/\",\n    ) as session:\n        task = \"\"\"\n        You are a helpful coding assistant with access to a Python execution environment.\n        Read the task from the input files and execute it. Use the code_exec tool to run the Python code.\n        When you're done, call the finish tool with the results.\n        \"\"\"\n        await session.run(task)\n\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Backend Options</p> <ul> <li>Local: <code>LocalCodeExecToolProvider()</code> - runs in temp directory</li> <li>Docker: <code>DockerCodeExecToolProvider.from_image(\"python:3.12-slim\")</code> - requires Docker</li> <li>E2B: <code>E2BCodeExecToolProvider()</code> - requires <code>E2B_API_KEY</code></li> </ul>"},{"location":"examples/#mcp-integration","title":"MCP Integration","text":"<p>Connect to MCP servers for additional tools.</p> <pre><code>\"\"\"Example: MCP-powered agent with simplified session API.\n\nThis example demonstrates how to create an agent with access to MCP server tools\nusing the new session-based API.\n\nPrerequisites:\n- Create mcp.json in .mcp/ directory with server configuration\n\nExample .mcp/mcp.json:\n    {\n      \"mcpServers\": {\n        \"deepwiki\": {\n          \"url\": \"https://mcp.deepwiki.com/sse\"\n        }\n      }\n    }\n\"\"\"\n\nimport asyncio\nfrom pathlib import Path\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import DEFAULT_TOOLS\nfrom stirrup.tools.mcp import MCPToolProvider\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent with MCP tools.\"\"\"\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n        max_tokens=50_000,\n    )\n\n    # Create agent with default tools + MCP tools\n    agent = Agent(\n        client=client,\n        name=\"mcp_example_agent\",\n        tools=[*DEFAULT_TOOLS, MCPToolProvider.from_config(\".mcp/mcp.json\")],\n        max_turns=20,\n    )\n\n    # Run with session context - handles tool lifecycle, logging, and file saving\n    async with agent.session(output_dir=Path(\"./output/mcp_example\")) as session:\n        task = \"\"\"You have access to MCP server tools and a code execution environment.\n            Using the same implementation as TheAlgorithms/Python (you can use DeepWiki MCP\n            to research), write a Python file quicksort.py that implements quicksort and\n            another that tests (and times) it.\n            When done, call the finish tool including your findings.\"\"\"\n\n        _finish_params, _history, _metadata = await session.run(task)\n\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Example <code>.mcp/mcp.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"deepwiki\": {\n      \"url\": \"https://mcp.deepwiki.com/sse\"\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>Requires <code>pip install stirrup[mcp]</code> (or: <code>uv add stirrup[mcp]</code>).</p>"},{"location":"examples/#image-processing","title":"Image Processing","text":"<p>An agent that can download and view images.</p> <pre><code>\"\"\"Example: Web search and image viewing.\n\nThis example demonstrates an agent that can:\n1. Search the web for images\n2. Download images to the execution environment\n3. View the downloaded images using ViewImageToolProvider\n\"\"\"\n\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, ViewImageToolProvider, WebToolProvider\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent that can search the web and view images.\"\"\"\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # ViewImageToolProvider will automatically use the Agent's CodeExecToolProvider\n    agent = Agent(\n        client=client,\n        name=\"image_agent\",\n        tools=[\n            LocalCodeExecToolProvider(),\n            WebToolProvider(),\n            ViewImageToolProvider(),  # Auto-detects exec_env\n        ],\n        max_turns=20,\n    )\n\n    async with agent.session(output_dir=\"output/view_image_example/\") as session:\n        _finish_params, _message_history, _run_metadata = await session.run(\n            \"\"\"Download an image of a kangaroo and describe what you see in it.\"\"\"\n        )\n\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/#sub-agent-pattern","title":"Sub-Agent Pattern","text":"<p>Use one agent as a tool for another. This example shows a supervisor agent coordinating specialized sub-agents for research and report writing.</p> <pre><code>\"\"\"Example: Sub-agent delegation.\n\nThis example demonstrates how to create a supervisor agent that delegates tasks\nto specialized sub-agents. The supervisor coordinates:\n- A research sub-agent with web search and local code execution\n- A report writing sub-agent with Docker-based code execution\n\"\"\"\n\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, WebToolProvider\nfrom stirrup.tools.code_backends.docker import DockerCodeExecToolProvider\n\n\nasync def main() -&gt; None:\n    \"\"\"Run a supervisor agent that delegates tasks to sub-agents.\"\"\"\n    # Create client for OpenRouter (shared across all agents)\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n        max_tokens=64_000,\n    )\n\n    ## ------- define research sub-agent ------- ##\n    research_agent = Agent(\n        client=client,\n        name=\"research_sub_agent\",\n        tools=[WebToolProvider(), LocalCodeExecToolProvider()],\n        max_turns=5,\n        system_prompt=(\n            \"You are a research agent. When asked to complete research, save it all to a markdown file \"\n            \"(using a code executor tool) and pass the filepath to the finish tool and mention it in the \"\n            \"finish_reason. Remember you will need a turn to write the markdown file and a separate turn to finish.\"\n        ),\n    )\n\n    # Convert agent to a tool for use by supervisor\n    research_subagent_tool = research_agent.to_tool(\n        description=\"Agent that can search the web and return the results.\",\n    )\n\n    ## ------- define report writing sub-agent ------- ##\n    report_writing_agent = Agent(\n        client=client,\n        name=\"report_writing_sub_agent\",\n        tools=[DockerCodeExecToolProvider.from_image(\"ghcr.io/astral-sh/uv:python3.13-bookworm-slim\")],\n        max_turns=6,\n    )\n\n    report_writing_subagent_tool = report_writing_agent.to_tool(\n        description=\"Create final reports using coding tools. Use uv to install any dependencies needed.\",\n    )\n\n    ## ------- define supervisor agent ------- ##\n    supervisor_agent = Agent(\n        client=client,\n        name=\"supervisor_agent\",\n        tools=[research_subagent_tool, report_writing_subagent_tool, LocalCodeExecToolProvider()],\n        max_turns=6,\n    )\n\n    async with supervisor_agent.session(output_dir=\"output/sub_agent_example/\") as session:\n        _finish_params, _message_history, _run_metadata = await session.run(\n            init_msgs=\"\"\"\n            Create a report on the latest hallucination benchmarks.\n            Use your research sub-agent and report writing sub-agent to create the report.\n            Output the report as a PDF file and pass the path in the finish tool.\n            \"\"\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>File Transfer Requirement</p> <p>If a sub-agent has a code execution environment and produces files, the parent agent must also have a <code>CodeExecToolProvider</code> to receive those files.</p>"},{"location":"examples/#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<p>Connect to any OpenAI-compatible API by specifying a custom <code>base_url</code>. Ensure you have set the correct environment variables required for the specific provider.</p> <pre><code>\"\"\"Example: Using Agent with Deepseek's OpenAI-compatible API.\n\nDemonstrates connecting to Deepseek using ChatCompletionsClient with a custom base_url.\nThis same pattern works for any OpenAI-compatible API (vLLM, Ollama, Azure OpenAI,\nlocal models, etc.).\n\nRequires: DEEPSEEK_API_KEY environment variable\n\"\"\"\n\nimport asyncio\nimport os\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent using Deepseek's API.\"\"\"\n    # Create client using Deepseek's OpenAI-compatible endpoint\n    client = ChatCompletionsClient(\n        base_url=\"https://api.deepseek.com\",\n        model=\"deepseek-chat\",  # or \"deepseek-reasoner\" for R1\n        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n    )\n\n    agent = Agent(client=client, name=\"deepseek_agent\")\n\n    async with agent.session(output_dir=\"./output\") as session:\n        await session.run(\"What is 2 + 2? Explain your reasoning step by step.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Note</p> <p>Requires <code>DEEPSEEK_API_KEY</code> environment variable (or the appropriate key for your provider).</p>"},{"location":"examples/#litellm-multi-provider-support","title":"LiteLLM Multi-Provider Support","text":"<p>Use LiteLLM to connect to non-OpenAI providers like Anthropic Claude, Google Gemini, and many others.</p> <pre><code>\"\"\"Example: Using Agent with LiteLLM for multi-provider support.\n\nDemonstrates using LiteLLM to connect to non-OpenAI providers like Anthropic Claude,\nGoogle Gemini, etc. For LiteLLM, create the client directly and pass it to the Agent.\n\nRequires:\n- uv pip install stirrup[litellm]\n- ANTHROPIC_API_KEY environment variable (for this example)\n\"\"\"\n\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.litellm_client import LiteLLMClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent using Anthropic Claude via LiteLLM.\"\"\"\n    # Create LiteLLM client for Anthropic Claude\n    # See https://docs.litellm.ai/docs/providers for all supported providers\n    client = LiteLLMClient(model_slug=\"anthropic/claude-sonnet-4-5\", max_tokens=64_000)\n\n    # Pass client to Agent - model info comes from client.model_slug\n    agent = Agent(\n        client=client,\n        name=\"claude_agent\",\n    )\n\n    async with agent.session(output_dir=\"./output/litellm_example\") as session:\n        await session.run(\n            \"What has the temperature been in the last 3 days in San Francisco? \"\n            \"Provide a brief summary and output a pdf file with the summary and a graph.\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Note</p> <p>Requires <code>pip install stirrup[litellm]</code> (or: <code>uv add stirrup[litellm]</code>) and the appropriate API key for your chosen provider (e.g., <code>ANTHROPIC_API_KEY</code> for Claude).</p>"},{"location":"examples/#custom-finish-tool","title":"Custom Finish Tool","text":"<p>Define structured output with a custom finish tool:</p> <pre><code>import asyncio\n\nfrom pydantic import BaseModel, Field\n\nfrom stirrup import Agent, Tool, ToolResult, ToolUseCountMetadata\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n\nclass AnalysisResult(BaseModel):\n    \"\"\"Structured analysis output.\"\"\"\n    summary: str = Field(description=\"Brief summary of findings\")\n    confidence: float = Field(description=\"Confidence score 0-1\")\n    sources: list[str] = Field(description=\"URLs of sources used\")\n\n\nasync def main():\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # Create custom finish tool\n    finish_tool = Tool(\n        name=\"finish\",\n        description=\"Complete the analysis with structured results\",\n        parameters=AnalysisResult,\n        executor=lambda p: ToolResult(\n            content=\"Analysis complete\",\n            metadata=ToolUseCountMetadata()\n        ),\n    )\n\n    agent = Agent(\n        client=client,\n        name=\"analyst\",\n        finish_tool=finish_tool,\n    )\n\n    async with agent.session() as session:\n        finish_params, _, _ = await session.run(\n            \"Analyze the current state of renewable energy adoption globally.\"\n        )\n\n        # finish_params is now typed as AnalysisResult\n        print(f\"Summary: {finish_params.summary}\")\n        print(f\"Confidence: {finish_params.confidence}\")\n        print(f\"Sources: {finish_params.sources}\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through installing Stirrup and creating your first agent.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or higher</li> <li>An API key from your LLM provider (OpenRouter, Anthropic, OpenAI, etc.)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install the core framework:</p> <pre><code>pip install stirrup      # or: uv add stirrup\n</code></pre> <p>Or with all optional components:</p> <pre><code>pip install 'stirrup[all]'  # or: uv add 'stirrup[all]'\n</code></pre>"},{"location":"getting-started/#optional-extras","title":"Optional Extras","text":"Extra Description <code>litellm</code> Multi-provider support via LiteLLM (Anthropic, Google, etc.) <code>docker</code> Docker-based code execution <code>e2b</code> E2B cloud sandboxes <code>mcp</code> MCP server support <code>all</code> All optional components"},{"location":"getting-started/#your-first-agent","title":"Your First Agent","text":"<p>Create a simple agent that can search the web and execute code:</p> <pre><code>import asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent that searches the web and creates a chart.\"\"\"\n\n    # Create client using ChatCompletionsClient\n    # Automatically uses OPENROUTER_API_KEY environment variable\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    # As no tools are provided, the agent will use the default tools, which consist of:\n    # - Web tools (web search and web fetching, note web search requires BRAVE_API_KEY)\n    # - Local code execution tool (to execute shell commands)\n    agent = Agent(client=client, name=\"agent\", max_turns=15)\n\n    # Run with session context - handles tool lifecycle, logging and file outputs\n    async with agent.session(output_dir=\"output/getting_started_example\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"\"\"\n        What is the population of the US over the last 3 years? Search the web to\n        find out and create a chart using matplotlib showing the population per year.\n        \"\"\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Environment Variables</p> <p>This example uses OpenRouter. Set <code>OPENROUTER_API_KEY</code> in your environment before running.</p> <p>Web search requires a <code>BRAVE_API_KEY</code>. The agent will still work without it, but web search will be unavailable.</p>"},{"location":"getting-started/#tools","title":"Tools","text":"<p>By default, agents include code execution and web tools:</p> Tool Description <code>code_exec</code> Execute shell commands in an isolated temp directory <code>web_fetch</code> Fetch and parse web pages <code>web_search</code> Search the web (requires <code>BRAVE_API_KEY</code>) <p>Extend with additional tools:</p> <pre><code>import argparse\nimport asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL\nfrom stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\nfrom stirrup.tools.web import WebToolProvider\n\nDEFAULT_OPENROUTER_SLUG = \"anthropic/claude-sonnet-4.5\"\n\n# Create client for OpenRouter\nclient = ChatCompletionsClient(\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=DEFAULT_OPENROUTER_SLUG,\n)\n\n# Create agent with E2B execution + web tools + calculator\n# (This is just for the docs snippet above \u2014 the actual runnable code is in main() below)\nagent = Agent(\n    client=client,\n    name=\"web_calculator_agent\",\n    tools=[E2BCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL],\n)\n</code></pre> <p>\u2192 See Tools for full documentation on DEFAULT_TOOLS, custom tools, sub-agents, and tool providers.</p>"},{"location":"getting-started/#choosing-a-client","title":"Choosing a Client","text":"<p><code>stirrup</code> ships with support for OpenAI-Compatible APIs and <code>LiteLLM</code>, with the open to build your own client.</p>"},{"location":"getting-started/#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<p>Use <code>ChatCompletionsClient</code> to use OpenAI models or OpenAI-compatible APIs:</p> <pre><code>    # Create client using Deepseek's OpenAI-compatible endpoint\n    client = ChatCompletionsClient(\n        base_url=\"https://api.deepseek.com\",\n        model=\"deepseek-chat\",  # or \"deepseek-reasoner\" for R1\n        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n    )\n\n    agent = Agent(client=client, name=\"deepseek_agent\")\n</code></pre> <p>\u2192 See Client for parameter tables and creating custom clients.</p>"},{"location":"getting-started/#understanding-the-output","title":"Understanding the Output","text":"<p>The <code>run()</code> method returns three values:</p> <pre><code>finish_params, history, metadata = await session.run(\"Your task\")\n</code></pre> <ul> <li><code>finish_params</code>: Agent's final response (<code>reason</code>, <code>paths</code>)</li> <li><code>history</code>: Conversation message history</li> <li><code>metadata</code>: Aggregated tool metadata and token usage</li> </ul> <p>\u2192 See Understanding Agent Output for details.</p>"},{"location":"getting-started/#uploading-input-files","title":"Uploading Input Files","text":"<p>Provide files to the agent's execution environment:</p> <pre><code>async with agent.session(\n    input_files=[\"data.csv\", \"config.json\"],\n    output_dir=\"./output\",\n) as session:\n    await session.run(\"Analyze the data\")\n</code></pre> <p>Supports single files, lists, directories, and glob patterns (<code>\"data/*.csv\"</code>).</p> <p>\u2192 See Passing Input Files for details.</p>"},{"location":"getting-started/#saving-output-files","title":"Saving Output Files","text":"<p>Save files created by the agent by providing an output directory through the session <code>output_dir</code> argument:</p> <pre><code>async with agent.session(output_dir=\"./results\") as session:\n    finish_params, _, _ = await session.run(\"Create a chart\")\n    # Files in finish_params.paths are saved to ./results/\n</code></pre> <p>\u2192 See Receiving Output Files for details.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Deep dive into Agent, Session, Client, Tools, and Logging</li> <li>Examples - Working examples for common patterns</li> <li>Creating Tools - Build your own tools</li> <li>Code Execution - Different execution backends</li> </ul>"},{"location":"api/clients/chat_completions/","title":"ChatCompletions Client","text":""},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client","title":"stirrup.clients.chat_completions_client","text":"<p>OpenAI SDK-based LLM client for chat completions.</p> <p>This client uses the official OpenAI Python SDK directly, supporting both OpenAI's API and any OpenAI-compatible endpoint via the <code>base_url</code> parameter (e.g., vLLM, Ollama, Azure OpenAI, local models).</p> <p>This is the default client for Stirrup.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['ChatCompletionsClient']\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.LOGGER","title":"LOGGER  <code>module-attribute</code>","text":"<pre><code>LOGGER = getLogger(__name__)\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatMessage","title":"ChatMessage","text":"<pre><code>ChatMessage = Annotated[\n    SystemMessage\n    | UserMessage\n    | AssistantMessage\n    | ToolMessage,\n    Field(discriminator=role),\n]\n</code></pre> <p>Discriminated union of all message types, automatically parsed based on role field.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ContextOverflowError","title":"ContextOverflowError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when LLM context window is exceeded (max_tokens or length finish_reason).</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for LLM client implementations.</p> <p>Any LLM client must implement this protocol to work with the Agent class. Provides text generation with tool support and model capability inspection.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.Reasoning","title":"Reasoning","text":"<p>               Bases: <code>BaseModel</code></p> <p>Extended thinking/reasoning content from models that support chain-of-thought reasoning.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.TokenUsage","title":"TokenUsage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token counts for LLM usage.</p> <p>Token terminology: output = reasoning + answer.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.TokenUsage.output","title":"output  <code>property</code>","text":"<pre><code>output: int\n</code></pre> <p>Total output tokens (reasoning + answer).</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.TokenUsage.total","title":"total  <code>property</code>","text":"<pre><code>total: int\n</code></pre> <p>Total token count across input, answer, and reasoning.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.TokenUsage.__add__","title":"__add__","text":"<pre><code>__add__(other: TokenUsage) -&gt; TokenUsage\n</code></pre> <p>Add two TokenUsage objects together, summing each field independently.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"TokenUsage\") -&gt; \"TokenUsage\":\n    \"\"\"Add two TokenUsage objects together, summing each field independently.\"\"\"\n    return TokenUsage(\n        input=self.input + other.input,\n        answer=self.answer + other.answer,\n        reasoning=self.reasoning + other.reasoning,\n    )\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition with name, description, parameter schema, and executor function.</p> Generic over <p>P: Parameter model type (Pydantic BaseModel subclass, or EmptyParams for parameterless tools) M: Metadata type (should implement Addable for aggregation; use None for tools without metadata)</p> <p>Tools are simple, stateless callables. For tools requiring lifecycle management (setup/teardown, resource pooling), use a ToolProvider instead.</p> Example with parameters <pre><code>class CalcParams(BaseModel):\n    expression: str\n\ncalc_tool = Tool[CalcParams, None](\n    name=\"calc\",\n    description=\"Evaluate math\",\n    parameters=CalcParams,\n    executor=lambda p: ToolResult(content=str(eval(p.expression))),\n)\n</code></pre> <p>Example without parameters (uses EmptyParams by default):     <pre><code>time_tool = Tool[EmptyParams, None](\n    name=\"time\",\n    description=\"Get current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre></p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool invocation request from the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the tool to invoke</p> <code>arguments</code> <code>str</code> <p>JSON string containing tool parameters</p> <code>tool_call_id</code> <code>str | None</code> <p>Unique identifier for tracking this tool call and its result</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient","title":"ChatCompletionsClient","text":"<pre><code>ChatCompletionsClient(\n    model: str,\n    max_tokens: int = 64000,\n    *,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: str | None = None,\n    timeout: float | None = None,\n    max_retries: int = 2,\n    kwargs: dict[str, Any] | None = None,\n)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI SDK-based client supporting OpenAI and OpenAI-compatible APIs.</p> <p>Uses the official OpenAI Python SDK directly for chat completions. Supports custom base_url for OpenAI-compatible providers (vLLM, Ollama, Azure OpenAI, local models, etc.).</p> <p>Includes automatic retries for transient failures and token usage tracking.</p> Example <p>Initialize OpenAI SDK client with model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., 'gpt-5', 'gpt-4o', 'o1-preview').</p> required <code>max_tokens</code> <code>int</code> <p>Maximum context window size in tokens. Defaults to 64,000.</p> <code>64000</code> <code>base_url</code> <code>str | None</code> <p>API base URL. If None, uses OpenAI's standard URL. Use for OpenAI-compatible providers (e.g., 'http://localhost:8000/v1').</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for authentication. If None, reads from OPENROUTER_API_KEY environment variable.</p> <code>None</code> <code>reasoning_effort</code> <code>str | None</code> <p>Reasoning effort level for extended thinking models (e.g., 'low', 'medium', 'high'). Only used with o1/o3 style models.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Request timeout in seconds. If None, uses OpenAI SDK default.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Number of retries for transient errors. Defaults to 2. The OpenAI SDK handles retries internally with exponential backoff.</p> <code>2</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Additional arguments passed to chat.completions.create().</p> <code>None</code> Source code in <code>src/stirrup/clients/chat_completions_client.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    max_tokens: int = 64_000,\n    *,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: str | None = None,\n    timeout: float | None = None,\n    max_retries: int = 2,\n    kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initialize OpenAI SDK client with model configuration.\n\n    Args:\n        model: Model identifier (e.g., 'gpt-5', 'gpt-4o', 'o1-preview').\n        max_tokens: Maximum context window size in tokens. Defaults to 64,000.\n        base_url: API base URL. If None, uses OpenAI's standard URL.\n            Use for OpenAI-compatible providers (e.g., 'http://localhost:8000/v1').\n        api_key: API key for authentication. If None, reads from OPENROUTER_API_KEY\n            environment variable.\n        reasoning_effort: Reasoning effort level for extended thinking models\n            (e.g., 'low', 'medium', 'high'). Only used with o1/o3 style models.\n        timeout: Request timeout in seconds. If None, uses OpenAI SDK default.\n        max_retries: Number of retries for transient errors. Defaults to 2.\n            The OpenAI SDK handles retries internally with exponential backoff.\n        kwargs: Additional arguments passed to chat.completions.create().\n    \"\"\"\n    self._model = model\n    self._max_tokens = max_tokens\n    self._reasoning_effort = reasoning_effort\n    self._kwargs = kwargs or {}\n\n    # Initialize AsyncOpenAI client\n    # Read from OPENROUTER_API_KEY if no api_key provided\n    resolved_api_key = api_key or os.environ.get(\"OPENROUTER_API_KEY\")\n    self._client = AsyncOpenAI(\n        api_key=resolved_api_key,\n        base_url=base_url,\n        timeout=timeout,\n        max_retries=max_retries,\n    )\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient--standard-openai-usage","title":"Standard OpenAI usage","text":"<p>client = ChatCompletionsClient(model=\"gpt-4o\", max_tokens=128_000)</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient--custom-openai-compatible-endpoint","title":"Custom OpenAI-compatible endpoint","text":"<p>client = ChatCompletionsClient( ...     model=\"llama-3.1-70b\", ...     base_url=\"http://localhost:8000/v1\", ...     api_key=\"your-api-key\", ... )</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient.max_tokens","title":"max_tokens  <code>property</code>","text":"<pre><code>max_tokens: int\n</code></pre> <p>Maximum context window size in tokens.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient.model_slug","title":"model_slug  <code>property</code>","text":"<pre><code>model_slug: str\n</code></pre> <p>Model identifier.</p>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.ChatCompletionsClient.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(\n    messages: list[ChatMessage], tools: dict[str, Tool]\n) -&gt; AssistantMessage\n</code></pre> <p>Generate assistant response with optional tool calls.</p> <p>Retries up to 3 times on transient errors (connection, timeout, rate limit, internal server errors) with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatMessage]</code> <p>List of conversation messages.</p> required <code>tools</code> <code>dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool objects.</p> required <p>Returns:</p> Type Description <code>AssistantMessage</code> <p>AssistantMessage containing the model's response, any tool calls,</p> <code>AssistantMessage</code> <p>and token usage statistics.</p> <p>Raises:</p> Type Description <code>ContextOverflowError</code> <p>If the context window is exceeded.</p> Source code in <code>src/stirrup/clients/chat_completions_client.py</code> <pre><code>@retry(\n    retry=retry_if_exception_type(\n        (\n            APIConnectionError,\n            APITimeoutError,\n            RateLimitError,\n            InternalServerError,\n        )\n    ),\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10),\n)\nasync def generate(\n    self,\n    messages: list[ChatMessage],\n    tools: dict[str, Tool],\n) -&gt; AssistantMessage:\n    \"\"\"Generate assistant response with optional tool calls.\n\n    Retries up to 3 times on transient errors (connection, timeout, rate limit,\n    internal server errors) with exponential backoff.\n\n    Args:\n        messages: List of conversation messages.\n        tools: Dictionary mapping tool names to Tool objects.\n\n    Returns:\n        AssistantMessage containing the model's response, any tool calls,\n        and token usage statistics.\n\n    Raises:\n        ContextOverflowError: If the context window is exceeded.\n    \"\"\"\n    # Build request kwargs\n    request_kwargs: dict[str, Any] = {\n        \"model\": self._model,\n        \"messages\": to_openai_messages(messages),\n        \"max_completion_tokens\": self._max_tokens,\n        **self._kwargs,\n    }\n\n    # Add tools if provided\n    if tools:\n        request_kwargs[\"tools\"] = to_openai_tools(tools)\n        request_kwargs[\"tool_choice\"] = \"auto\"\n\n    # Add reasoning effort if configured (for o1/o3 models)\n    if self._reasoning_effort:\n        request_kwargs[\"reasoning_effort\"] = self._reasoning_effort\n\n    # Make API call\n    request_start_time = perf_counter()\n    response = await self._client.chat.completions.create(**request_kwargs)\n    request_end_time = perf_counter()\n\n    choice = response.choices[0]\n\n    # Check for context overflow\n    if choice.finish_reason in (\"max_tokens\", \"length\"):\n        raise ContextOverflowError(\n            f\"Maximal context window tokens reached for model {self.model_slug}, \"\n            f\"resulting in finish reason: {choice.finish_reason}. \"\n            \"Reduce agent.max_tokens and try again.\"\n        )\n\n    msg = choice.message\n\n    # Parse reasoning content (for o1/o3 models with extended thinking)\n    reasoning: Reasoning | None = None\n    if hasattr(msg, \"reasoning_content\") and msg.reasoning_content:\n        reasoning = Reasoning(content=msg.reasoning_content)\n\n    # Parse tool calls\n    tool_calls = [\n        ToolCall(\n            tool_call_id=tc.id,\n            name=tc.function.name,\n            arguments=tc.function.arguments or \"\",\n        )\n        for tc in (msg.tool_calls or [])\n    ]\n\n    # Parse token usage\n    usage = response.usage\n    input_tokens = usage.prompt_tokens if usage else 0\n    output_tokens = usage.completion_tokens if usage else 0\n\n    # Handle reasoning tokens if available (for o1/o3 models)\n    reasoning_tokens = 0\n    if usage and hasattr(usage, \"completion_tokens_details\") and usage.completion_tokens_details:\n        reasoning_tokens = getattr(usage.completion_tokens_details, \"reasoning_tokens\", 0) or 0\n\n    answer_tokens = output_tokens - reasoning_tokens\n\n    return AssistantMessage(\n        reasoning=reasoning,\n        content=msg.content or \"\",\n        tool_calls=tool_calls,\n        token_usage=TokenUsage(\n            input=input_tokens,\n            answer=answer_tokens,\n            reasoning=reasoning_tokens,\n        ),\n        request_start_time=request_start_time,\n        request_end_time=request_end_time,\n    )\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.to_openai_messages","title":"to_openai_messages","text":"<pre><code>to_openai_messages(\n    msgs: list[ChatMessage],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert ChatMessage list to OpenAI-compatible message dictionaries.</p> <p>Handles all message types: SystemMessage, UserMessage, AssistantMessage, and ToolMessage. Preserves reasoning content and tool calls for assistant messages.</p> <p>Parameters:</p> Name Type Description Default <code>msgs</code> <code>list[ChatMessage]</code> <p>List of ChatMessage objects (System, User, Assistant, or Tool messages).</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of message dictionaries ready for the OpenAI API.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If an unsupported message type is encountered.</p> Source code in <code>src/stirrup/clients/utils.py</code> <pre><code>def to_openai_messages(msgs: list[ChatMessage]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert ChatMessage list to OpenAI-compatible message dictionaries.\n\n    Handles all message types: SystemMessage, UserMessage, AssistantMessage,\n    and ToolMessage. Preserves reasoning content and tool calls for assistant\n    messages.\n\n    Args:\n        msgs: List of ChatMessage objects (System, User, Assistant, or Tool messages).\n\n    Returns:\n        List of message dictionaries ready for the OpenAI API.\n\n    Raises:\n        NotImplementedError: If an unsupported message type is encountered.\n    \"\"\"\n    out: list[dict[str, Any]] = []\n    for m in msgs:\n        if isinstance(m, SystemMessage):\n            out.append({\"role\": \"system\", \"content\": content_to_openai(m.content)})\n        elif isinstance(m, UserMessage):\n            out.append({\"role\": \"user\", \"content\": content_to_openai(m.content)})\n        elif isinstance(m, AssistantMessage):\n            msg: dict[str, Any] = {\"role\": \"assistant\", \"content\": content_to_openai(m.content)}\n\n            if m.reasoning:\n                if m.reasoning.content:\n                    msg[\"reasoning_content\"] = m.reasoning.content\n\n                if m.reasoning.signature:\n                    msg[\"thinking_blocks\"] = [\n                        {\"type\": \"thinking\", \"signature\": m.reasoning.signature, \"thinking\": m.reasoning.content}\n                    ]\n\n            if m.tool_calls:\n                msg[\"tool_calls\"] = []\n                for tool in m.tool_calls:\n                    tool_dict = tool.model_dump()\n                    tool_dict[\"id\"] = tool.tool_call_id\n                    tool_dict[\"type\"] = \"function\"\n                    if tool.signature is not None:\n                        tool_dict[\"provider_specific_fields\"] = {\n                            \"thought_signature\": tool.signature,\n                        }\n                    tool_dict[\"function\"] = {\n                        \"name\": tool.name,\n                        \"arguments\": tool.arguments,\n                    }\n                    msg[\"tool_calls\"].append(tool_dict)\n\n            out.append(msg)\n        elif isinstance(m, ToolMessage):\n            out.append(\n                {\n                    \"role\": \"tool\",\n                    \"content\": content_to_openai(m.content),\n                    \"tool_call_id\": m.tool_call_id,\n                    \"name\": m.name,\n                }\n            )\n        else:\n            raise NotImplementedError(f\"Unsupported message type: {type(m)}\")\n\n    return out\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.to_openai_tools","title":"to_openai_tools","text":"<pre><code>to_openai_tools(\n    tools: dict[str, Tool],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert Tool objects to OpenAI function calling format.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool objects.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of tool definitions in OpenAI's function calling format.</p> Example <p>tools = {\"calculator\": calculator_tool} openai_tools = to_openai_tools(tools)</p> Source code in <code>src/stirrup/clients/utils.py</code> <pre><code>def to_openai_tools(tools: dict[str, Tool]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects to OpenAI function calling format.\n\n    Args:\n        tools: Dictionary mapping tool names to Tool objects.\n\n    Returns:\n        List of tool definitions in OpenAI's function calling format.\n\n    Example:\n        &gt;&gt;&gt; tools = {\"calculator\": calculator_tool}\n        &gt;&gt;&gt; openai_tools = to_openai_tools(tools)\n        &gt;&gt;&gt; # Returns: [{\"type\": \"function\", \"function\": {\"name\": \"calculator\", ...}}]\n    \"\"\"\n    out: list[dict[str, Any]] = []\n    for t in tools.values():\n        function: dict[str, Any] = {\n            \"name\": t.name,\n            \"description\": t.description,\n        }\n        if t.parameters is not EmptyParams:\n            function[\"parameters\"] = t.parameters.model_json_schema()\n        tool_payload: dict[str, Any] = {\n            \"type\": \"function\",\n            \"function\": function,\n        }\n        out.append(tool_payload)\n    return out\n</code></pre>"},{"location":"api/clients/chat_completions/#stirrup.clients.chat_completions_client.to_openai_tools--returns-type-function-function-name-calculator","title":"Returns: [{\"type\": \"function\", \"function\": {\"name\": \"calculator\", ...}}]","text":""},{"location":"api/clients/litellm/","title":"LiteLLM Client","text":"<p>Optional Dependency</p> <p>LiteLLM is an optional dependency. Install with: <pre><code>pip install stirrup[litellm]  # or: uv add stirrup[litellm]\n</code></pre></p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client","title":"stirrup.clients.litellm_client","text":"<p>LiteLLM-based LLM client for multi-provider support.</p> <p>This client uses LiteLLM to provide a unified interface to multiple LLM providers (OpenAI, Anthropic, Google, etc.) with automatic retries for transient failures.</p> <p>Requires the litellm extra: <code>pip install stirrup[litellm]</code></p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['LiteLLMClient']\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LOGGER","title":"LOGGER  <code>module-attribute</code>","text":"<pre><code>LOGGER = getLogger(__name__)\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.ChatMessage","title":"ChatMessage","text":"<pre><code>ChatMessage = Annotated[\n    SystemMessage\n    | UserMessage\n    | AssistantMessage\n    | ToolMessage,\n    Field(discriminator=role),\n]\n</code></pre> <p>Discriminated union of all message types, automatically parsed based on role field.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.ReasoningEffort","title":"ReasoningEffort","text":"<pre><code>ReasoningEffort = Literal[\n    \"none\",\n    \"minimal\",\n    \"low\",\n    \"medium\",\n    \"high\",\n    \"xhigh\",\n    \"default\",\n]\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.ContextOverflowError","title":"ContextOverflowError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when LLM context window is exceeded (max_tokens or length finish_reason).</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for LLM client implementations.</p> <p>Any LLM client must implement this protocol to work with the Agent class. Provides text generation with tool support and model capability inspection.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.Reasoning","title":"Reasoning","text":"<p>               Bases: <code>BaseModel</code></p> <p>Extended thinking/reasoning content from models that support chain-of-thought reasoning.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.TokenUsage","title":"TokenUsage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token counts for LLM usage.</p> <p>Token terminology: output = reasoning + answer.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.TokenUsage.output","title":"output  <code>property</code>","text":"<pre><code>output: int\n</code></pre> <p>Total output tokens (reasoning + answer).</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.TokenUsage.total","title":"total  <code>property</code>","text":"<pre><code>total: int\n</code></pre> <p>Total token count across input, answer, and reasoning.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.TokenUsage.__add__","title":"__add__","text":"<pre><code>__add__(other: TokenUsage) -&gt; TokenUsage\n</code></pre> <p>Add two TokenUsage objects together, summing each field independently.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"TokenUsage\") -&gt; \"TokenUsage\":\n    \"\"\"Add two TokenUsage objects together, summing each field independently.\"\"\"\n    return TokenUsage(\n        input=self.input + other.input,\n        answer=self.answer + other.answer,\n        reasoning=self.reasoning + other.reasoning,\n    )\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition with name, description, parameter schema, and executor function.</p> Generic over <p>P: Parameter model type (Pydantic BaseModel subclass, or EmptyParams for parameterless tools) M: Metadata type (should implement Addable for aggregation; use None for tools without metadata)</p> <p>Tools are simple, stateless callables. For tools requiring lifecycle management (setup/teardown, resource pooling), use a ToolProvider instead.</p> Example with parameters <pre><code>class CalcParams(BaseModel):\n    expression: str\n\ncalc_tool = Tool[CalcParams, None](\n    name=\"calc\",\n    description=\"Evaluate math\",\n    parameters=CalcParams,\n    executor=lambda p: ToolResult(content=str(eval(p.expression))),\n)\n</code></pre> <p>Example without parameters (uses EmptyParams by default):     <pre><code>time_tool = Tool[EmptyParams, None](\n    name=\"time\",\n    description=\"Get current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre></p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool invocation request from the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the tool to invoke</p> <code>arguments</code> <code>str</code> <p>JSON string containing tool parameters</p> <code>tool_call_id</code> <code>str | None</code> <p>Unique identifier for tracking this tool call and its result</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LiteLLMClient","title":"LiteLLMClient","text":"<pre><code>LiteLLMClient(\n    model: str | None = None,\n    max_tokens: int = 64000,\n    *,\n    model_slug: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: ReasoningEffort | None = None,\n    kwargs: dict[str, Any] | None = None,\n)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>LiteLLM-based client supporting multiple LLM providers with unified interface.</p> <p>Includes automatic retries for transient failures and token usage tracking.</p> <p>Initialize LiteLLM client with model configuration and capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>Model identifier for LiteLLM (e.g., 'anthropic/claude-3-5-sonnet-20241022')</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum context window size in tokens</p> <code>64000</code> <code>model_slug</code> <code>str | None</code> <p>Deprecated. Use model instead.</p> <code>None</code> <code>reasoning_effort</code> <code>ReasoningEffort | None</code> <p>Reasoning effort level for extended thinking models (e.g., 'medium', 'high')</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Additional arguments to pass to LiteLLM completion calls</p> <code>None</code> Source code in <code>src/stirrup/clients/litellm_client.py</code> <pre><code>def __init__(\n    self,\n    model: str | None = None,\n    max_tokens: int = 64_000,\n    *,\n    model_slug: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: ReasoningEffort | None = None,\n    kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initialize LiteLLM client with model configuration and capabilities.\n\n    Args:\n        model: Model identifier for LiteLLM (e.g., 'anthropic/claude-3-5-sonnet-20241022')\n        max_tokens: Maximum context window size in tokens\n        model_slug: Deprecated. Use model instead.\n        reasoning_effort: Reasoning effort level for extended thinking models (e.g., 'medium', 'high')\n        kwargs: Additional arguments to pass to LiteLLM completion calls\n    \"\"\"\n    if model_slug is not None:\n        warnings.warn(\n            \"The 'model_slug' parameter is deprecated. Use 'model' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if model is None:\n            model = model_slug\n    if model is None:\n        raise ValueError(\"model is required\")\n    self._model = model\n    self._max_tokens = max_tokens\n    self._reasoning_effort: ReasoningEffort | None = reasoning_effort\n    self._api_key = api_key\n    self._kwargs = kwargs or {}\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LiteLLMClient.max_tokens","title":"max_tokens  <code>property</code>","text":"<pre><code>max_tokens: int\n</code></pre> <p>Maximum context window size in tokens.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LiteLLMClient.model_slug","title":"model_slug  <code>property</code>","text":"<pre><code>model_slug: str\n</code></pre> <p>Model identifier used by LiteLLM.</p>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.LiteLLMClient.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(\n    messages: list[ChatMessage], tools: dict[str, Tool]\n) -&gt; AssistantMessage\n</code></pre> <p>Generate assistant response with optional tool calls. Retries up to 3 times on timeout/connection errors.</p> Source code in <code>src/stirrup/clients/litellm_client.py</code> <pre><code>@retry(\n    retry=retry_if_exception_type((Timeout, APIConnectionError, RateLimitError)),\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10),\n)\nasync def generate(self, messages: list[ChatMessage], tools: dict[str, Tool]) -&gt; AssistantMessage:\n    \"\"\"Generate assistant response with optional tool calls. Retries up to 3 times on timeout/connection errors.\"\"\"\n    request_start_time = perf_counter()\n    r = await acompletion(\n        model=self.model_slug,\n        messages=to_openai_messages(messages),\n        tools=to_openai_tools(tools) if tools else None,\n        tool_choice=\"auto\" if tools else None,\n        max_tokens=self._max_tokens,\n        reasoning_effort=self._reasoning_effort,\n        api_key=self._api_key,\n        **self._kwargs,\n    )\n    request_end_time = perf_counter()\n\n    choice = r[\"choices\"][0]\n\n    if choice.finish_reason in [\"max_tokens\", \"length\"]:\n        raise ContextOverflowError(\n            f\"Maximal context window tokens reached for model {self.model_slug}, resulting in finish reason: {choice.finish_reason}. Reduce agent.max_tokens and try again.\"\n        )\n\n    msg = choice[\"message\"]\n    reasoning: Reasoning | None = None\n    if getattr(msg, \"reasoning_content\", None) is not None:\n        reasoning = Reasoning(content=msg.reasoning_content)\n    if getattr(msg, \"thinking_blocks\", None) is not None and len(msg.thinking_blocks) &gt; 0:\n        if len(msg.thinking_blocks) &gt; 1:\n            raise ValueError(\"Found multiple thinking blocks in the response\")\n\n        signature = msg.thinking_blocks[0].get(\"thinking_signature\", None)\n        content = msg.thinking_blocks[0].get(\"thinking\", None)\n\n        if signature is None and content is None:\n            raise ValueError(\"Signature and content not found in the thinking block response\")\n\n        reasoning = Reasoning(signature=signature, content=content)\n\n    usage = r[\"usage\"]\n\n    calls = [\n        ToolCall(\n            tool_call_id=tc.get(\"id\"),\n            name=tc[\"function\"][\"name\"],\n            arguments=tc[\"function\"].get(\"arguments\", \"\") or \"\",\n            signature=tc.get(\"provider_specific_fields\", {}).get(\"thought_signature\", None),\n        )\n        for tc in (msg.get(\"tool_calls\") or [])\n    ]\n\n    input_tokens = usage.prompt_tokens\n    reasoning_tokens = 0\n    if usage.completion_tokens_details:\n        reasoning_tokens = usage.completion_tokens_details.reasoning_tokens or 0\n    output_tokens = usage.completion_tokens\n    answer_tokens = output_tokens - reasoning_tokens\n\n    return AssistantMessage(\n        reasoning=reasoning,\n        content=msg.get(\"content\") or \"\",\n        tool_calls=calls,\n        token_usage=TokenUsage(\n            input=input_tokens,\n            answer=answer_tokens,\n            reasoning=reasoning_tokens,\n        ),\n        request_start_time=request_start_time,\n        request_end_time=request_end_time,\n    )\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.to_openai_messages","title":"to_openai_messages","text":"<pre><code>to_openai_messages(\n    msgs: list[ChatMessage],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert ChatMessage list to OpenAI-compatible message dictionaries.</p> <p>Handles all message types: SystemMessage, UserMessage, AssistantMessage, and ToolMessage. Preserves reasoning content and tool calls for assistant messages.</p> <p>Parameters:</p> Name Type Description Default <code>msgs</code> <code>list[ChatMessage]</code> <p>List of ChatMessage objects (System, User, Assistant, or Tool messages).</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of message dictionaries ready for the OpenAI API.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If an unsupported message type is encountered.</p> Source code in <code>src/stirrup/clients/utils.py</code> <pre><code>def to_openai_messages(msgs: list[ChatMessage]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert ChatMessage list to OpenAI-compatible message dictionaries.\n\n    Handles all message types: SystemMessage, UserMessage, AssistantMessage,\n    and ToolMessage. Preserves reasoning content and tool calls for assistant\n    messages.\n\n    Args:\n        msgs: List of ChatMessage objects (System, User, Assistant, or Tool messages).\n\n    Returns:\n        List of message dictionaries ready for the OpenAI API.\n\n    Raises:\n        NotImplementedError: If an unsupported message type is encountered.\n    \"\"\"\n    out: list[dict[str, Any]] = []\n    for m in msgs:\n        if isinstance(m, SystemMessage):\n            out.append({\"role\": \"system\", \"content\": content_to_openai(m.content)})\n        elif isinstance(m, UserMessage):\n            out.append({\"role\": \"user\", \"content\": content_to_openai(m.content)})\n        elif isinstance(m, AssistantMessage):\n            msg: dict[str, Any] = {\"role\": \"assistant\", \"content\": content_to_openai(m.content)}\n\n            if m.reasoning:\n                if m.reasoning.content:\n                    msg[\"reasoning_content\"] = m.reasoning.content\n\n                if m.reasoning.signature:\n                    msg[\"thinking_blocks\"] = [\n                        {\"type\": \"thinking\", \"signature\": m.reasoning.signature, \"thinking\": m.reasoning.content}\n                    ]\n\n            if m.tool_calls:\n                msg[\"tool_calls\"] = []\n                for tool in m.tool_calls:\n                    tool_dict = tool.model_dump()\n                    tool_dict[\"id\"] = tool.tool_call_id\n                    tool_dict[\"type\"] = \"function\"\n                    if tool.signature is not None:\n                        tool_dict[\"provider_specific_fields\"] = {\n                            \"thought_signature\": tool.signature,\n                        }\n                    tool_dict[\"function\"] = {\n                        \"name\": tool.name,\n                        \"arguments\": tool.arguments,\n                    }\n                    msg[\"tool_calls\"].append(tool_dict)\n\n            out.append(msg)\n        elif isinstance(m, ToolMessage):\n            out.append(\n                {\n                    \"role\": \"tool\",\n                    \"content\": content_to_openai(m.content),\n                    \"tool_call_id\": m.tool_call_id,\n                    \"name\": m.name,\n                }\n            )\n        else:\n            raise NotImplementedError(f\"Unsupported message type: {type(m)}\")\n\n    return out\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.to_openai_tools","title":"to_openai_tools","text":"<pre><code>to_openai_tools(\n    tools: dict[str, Tool],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert Tool objects to OpenAI function calling format.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool objects.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of tool definitions in OpenAI's function calling format.</p> Example <p>tools = {\"calculator\": calculator_tool} openai_tools = to_openai_tools(tools)</p> Source code in <code>src/stirrup/clients/utils.py</code> <pre><code>def to_openai_tools(tools: dict[str, Tool]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects to OpenAI function calling format.\n\n    Args:\n        tools: Dictionary mapping tool names to Tool objects.\n\n    Returns:\n        List of tool definitions in OpenAI's function calling format.\n\n    Example:\n        &gt;&gt;&gt; tools = {\"calculator\": calculator_tool}\n        &gt;&gt;&gt; openai_tools = to_openai_tools(tools)\n        &gt;&gt;&gt; # Returns: [{\"type\": \"function\", \"function\": {\"name\": \"calculator\", ...}}]\n    \"\"\"\n    out: list[dict[str, Any]] = []\n    for t in tools.values():\n        function: dict[str, Any] = {\n            \"name\": t.name,\n            \"description\": t.description,\n        }\n        if t.parameters is not EmptyParams:\n            function[\"parameters\"] = t.parameters.model_json_schema()\n        tool_payload: dict[str, Any] = {\n            \"type\": \"function\",\n            \"function\": function,\n        }\n        out.append(tool_payload)\n    return out\n</code></pre>"},{"location":"api/clients/litellm/#stirrup.clients.litellm_client.to_openai_tools--returns-type-function-function-name-calculator","title":"Returns: [{\"type\": \"function\", \"function\": {\"name\": \"calculator\", ...}}]","text":""},{"location":"api/clients/open_responses/","title":"OpenResponses Client","text":"<p>The <code>OpenResponsesClient</code> uses OpenAI's Responses API (<code>POST /v1/responses</code>) instead of the Chat Completions API. This client is useful for providers that implement the newer Responses API format.</p>"},{"location":"api/clients/open_responses/#key-differences-from-chatcompletionsclient","title":"Key Differences from ChatCompletionsClient","text":"Feature ChatCompletionsClient OpenResponsesClient API endpoint <code>chat.completions.create()</code> <code>responses.create()</code> System messages Included in <code>messages</code> array Passed as <code>instructions</code> parameter Message format <code>{\"role\": \"user\", \"content\": [...]}</code> <code>{\"role\": \"user\", \"content\": [{\"type\": \"input_text\", ...}]}</code> Tool call IDs <code>tool_call_id</code> <code>call_id</code> Reasoning config <code>reasoning_effort</code> param <code>reasoning: {\"effort\": ...}</code> object"},{"location":"api/clients/open_responses/#usage","title":"Usage","text":"<p>For models that support extended thinking (like o1/o3), you can configure the reasoning effort:</p> <pre><code>import asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients import OpenResponsesClient\n\n\nasync def main() -&gt; None:\n    \"\"\"Run an agent using the OpenResponses API with a reasoning model.\"\"\"\n\n    # Create client using OpenResponsesClient\n    # Uses the OpenAI Responses API (responses.create)\n    # For reasoning models, you can set reasoning_effort\n    client = OpenResponsesClient(\n        model=\"gpt-5.2\",\n        reasoning_effort=\"medium\",\n    )\n\n    agent = Agent(client=client, name=\"reasoning-agent\", max_turns=19)\n\n    async with agent.session(output_dir=\"output/open_responses_example\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"Plan a software release with these tasks: Design (5 days), Backend (10 days, needs Design), \"\n            \"Frontend (8 days, needs Design), Testing (4 days, needs Backend and Frontend), \"\n            \"Documentation (3 days, can start after Backend). Two developers are available. \"\n            \"What's the minimum time to complete? Output an Excel Gantt chart with the schedule.\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/clients/open_responses/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>model</code> <code>str</code> required Model identifier (e.g., <code>\"gpt-4o\"</code>, <code>\"o1\"</code>) <code>max_tokens</code> <code>int</code> <code>64_000</code> Maximum output tokens <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom API base URL <code>api_key</code> <code>str \\| None</code> <code>None</code> API key (falls back to <code>OPENAI_API_KEY</code> env var) <code>reasoning_effort</code> <code>str \\| None</code> <code>None</code> Reasoning effort for o1/o3 models: <code>\"low\"</code>, <code>\"medium\"</code>, <code>\"high\"</code> <code>timeout</code> <code>float \\| None</code> <code>None</code> Request timeout in seconds <code>max_retries</code> <code>int</code> <code>2</code> Number of retries for transient errors <code>instructions</code> <code>str \\| None</code> <code>None</code> Default system instructions <code>kwargs</code> <code>dict \\| None</code> <code>None</code> Additional arguments passed to <code>responses.create()</code>"},{"location":"api/clients/open_responses/#api-reference","title":"API Reference","text":""},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client","title":"stirrup.clients.open_responses_client","text":"<p>OpenAI SDK-based LLM client for the Responses API.</p> <p>This client uses the official OpenAI Python SDK's responses.create() method, supporting both OpenAI's API and any OpenAI-compatible endpoint that implements the Responses API via the <code>base_url</code> parameter.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['OpenResponsesClient']\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.LOGGER","title":"LOGGER  <code>module-attribute</code>","text":"<pre><code>LOGGER = getLogger(__name__)\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ChatMessage","title":"ChatMessage","text":"<pre><code>ChatMessage = Annotated[\n    SystemMessage\n    | UserMessage\n    | AssistantMessage\n    | ToolMessage,\n    Field(discriminator=role),\n]\n</code></pre> <p>Discriminated union of all message types, automatically parsed based on role field.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.Content","title":"Content","text":"<pre><code>Content = list[ContentBlock] | str\n</code></pre> <p>Message content: either a plain string or list of mixed content blocks.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ContextOverflowError","title":"ContextOverflowError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when LLM context window is exceeded (max_tokens or length finish_reason).</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.AudioContentBlock","title":"AudioContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>Audio content supporting MPEG, WAV, AAC, and other common audio formats.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.AudioContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(bitrate: str = '192k') -&gt; str\n</code></pre> <p>Transcode to MP3 and return base64 data URL.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, bitrate: str = \"192k\") -&gt; str:\n    \"\"\"Transcode to MP3 and return base64 data URL.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"moviepy.*\")\n        with NamedTemporaryFile(suffix=\".bin\") as fin, NamedTemporaryFile(suffix=\".mp3\") as fout:\n            fin.write(self.data)\n            fin.flush()\n            clip = AudioFileClip(fin.name)\n            clip.write_audiofile(fout.name, codec=\"libmp3lame\", bitrate=bitrate, logger=None)\n            clip.close()\n            return f\"data:audio/mpeg;base64,{b64encode(fout.read()).decode()}\"\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.EmptyParams","title":"EmptyParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Empty parameter model for tools that don't require parameters.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ImageContentBlock","title":"ImageContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>Image content supporting PNG, JPEG, WebP, PSD formats with automatic downscaling.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ImageContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(\n    max_pixels: int | None = RESOLUTION_1MP,\n) -&gt; str\n</code></pre> <p>Convert image to base64 data URL, optionally resizing to max pixel count.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, max_pixels: int | None = RESOLUTION_1MP) -&gt; str:\n    \"\"\"Convert image to base64 data URL, optionally resizing to max pixel count.\"\"\"\n    img: Image.Image = Image.open(BytesIO(self.data))\n    if max_pixels is not None and img.width * img.height &gt; max_pixels:\n        tw, th = downscale_image(img.width, img.height, max_pixels)\n        img.thumbnail((tw, th), Image.Resampling.LANCZOS)\n    if img.mode != \"RGB\":\n        img = img.convert(\"RGB\")\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    return f\"data:image/png;base64,{b64encode(buf.getvalue()).decode()}\"\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for LLM client implementations.</p> <p>Any LLM client must implement this protocol to work with the Agent class. Provides text generation with tool support and model capability inspection.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.Reasoning","title":"Reasoning","text":"<p>               Bases: <code>BaseModel</code></p> <p>Extended thinking/reasoning content from models that support chain-of-thought reasoning.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>System-level instructions and context for the LLM.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.TokenUsage","title":"TokenUsage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token counts for LLM usage.</p> <p>Token terminology: output = reasoning + answer.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.TokenUsage.output","title":"output  <code>property</code>","text":"<pre><code>output: int\n</code></pre> <p>Total output tokens (reasoning + answer).</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.TokenUsage.total","title":"total  <code>property</code>","text":"<pre><code>total: int\n</code></pre> <p>Total token count across input, answer, and reasoning.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.TokenUsage.__add__","title":"__add__","text":"<pre><code>__add__(other: TokenUsage) -&gt; TokenUsage\n</code></pre> <p>Add two TokenUsage objects together, summing each field independently.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"TokenUsage\") -&gt; \"TokenUsage\":\n    \"\"\"Add two TokenUsage objects together, summing each field independently.\"\"\"\n    return TokenUsage(\n        input=self.input + other.input,\n        answer=self.answer + other.answer,\n        reasoning=self.reasoning + other.reasoning,\n    )\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition with name, description, parameter schema, and executor function.</p> Generic over <p>P: Parameter model type (Pydantic BaseModel subclass, or EmptyParams for parameterless tools) M: Metadata type (should implement Addable for aggregation; use None for tools without metadata)</p> <p>Tools are simple, stateless callables. For tools requiring lifecycle management (setup/teardown, resource pooling), use a ToolProvider instead.</p> Example with parameters <pre><code>class CalcParams(BaseModel):\n    expression: str\n\ncalc_tool = Tool[CalcParams, None](\n    name=\"calc\",\n    description=\"Evaluate math\",\n    parameters=CalcParams,\n    executor=lambda p: ToolResult(content=str(eval(p.expression))),\n)\n</code></pre> <p>Example without parameters (uses EmptyParams by default):     <pre><code>time_tool = Tool[EmptyParams, None](\n    name=\"time\",\n    description=\"Get current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre></p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool invocation request from the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the tool to invoke</p> <code>arguments</code> <code>str</code> <p>JSON string containing tool parameters</p> <code>tool_call_id</code> <code>str | None</code> <p>Unique identifier for tracking this tool call and its result</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool execution result returned to the LLM.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['tool']</code> <p>Always \"tool\"</p> <code>content</code> <code>Content</code> <p>The tool result content</p> <code>tool_call_id</code> <code>str | None</code> <p>ID linking this result to the corresponding tool call</p> <code>name</code> <code>str | None</code> <p>Name of the tool that was called</p> <code>args_was_valid</code> <code>bool</code> <p>Whether the tool arguments were valid</p> <code>success</code> <code>bool</code> <p>Whether the tool executed successfully (used by finish tool to control termination)</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.ToolMessage.tool_duration","title":"tool_duration  <code>property</code>","text":"<pre><code>tool_duration: float | None\n</code></pre> <p>Tool execution duration in seconds.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.UserMessage","title":"UserMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>User input message to the LLM.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.VideoContentBlock","title":"VideoContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>MP4 video content with automatic transcoding and resolution downscaling.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.VideoContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(\n    max_pixels: int | None = RESOLUTION_480P,\n    fps: int | None = None,\n) -&gt; str\n</code></pre> <p>Transcode to MP4 and return base64 data URL.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, max_pixels: int | None = RESOLUTION_480P, fps: int | None = None) -&gt; str:\n    \"\"\"Transcode to MP4 and return base64 data URL.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"moviepy.*\")\n        with NamedTemporaryFile(suffix=\".mp4\") as fin, NamedTemporaryFile(suffix=\".mp4\") as fout:\n            fin.write(self.data)\n            fin.flush()\n            clip = VideoFileClip(fin.name)\n            tw, th = downscale_image(int(clip.w), int(clip.h), max_pixels)\n            clip = clip.with_effects([Resize(new_size=(tw, th))])\n\n            clip.write_videofile(\n                fout.name,\n                codec=\"libx264\",\n                fps=fps,\n                audio=clip.audio is not None,\n                audio_codec=\"aac\",\n                preset=\"veryfast\",\n                logger=None,\n            )\n            clip.close()\n            return f\"data:video/mp4;base64,{b64encode(fout.read()).decode()}\"\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient","title":"OpenResponsesClient","text":"<pre><code>OpenResponsesClient(\n    model: str,\n    max_tokens: int = 64000,\n    *,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: str | None = None,\n    timeout: float | None = None,\n    max_retries: int = 2,\n    instructions: str | None = None,\n    kwargs: dict[str, Any] | None = None,\n)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI SDK-based client using the Responses API.</p> <p>Uses the official OpenAI Python SDK's responses.create() method. Supports custom base_url for OpenAI-compatible providers that implement the Responses API.</p> <p>Includes automatic retries for transient failures and token usage tracking.</p> Example <p>Initialize OpenAI SDK client with model configuration for Responses API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., 'gpt-4o', 'o1-preview').</p> required <code>max_tokens</code> <code>int</code> <p>Maximum output tokens. Defaults to 64,000.</p> <code>64000</code> <code>base_url</code> <code>str | None</code> <p>API base URL. If None, uses OpenAI's standard URL. Use for OpenAI-compatible providers.</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for authentication. If None, reads from OPENROUTER_API_KEY environment variable.</p> <code>None</code> <code>reasoning_effort</code> <code>str | None</code> <p>Reasoning effort level for extended thinking models (e.g., 'low', 'medium', 'high'). Only used with o1/o3 style models.</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Request timeout in seconds. If None, uses OpenAI SDK default.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Number of retries for transient errors. Defaults to 2.</p> <code>2</code> <code>instructions</code> <code>str | None</code> <p>Default system-level instructions. Can be overridden by SystemMessage in the messages list.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Additional arguments passed to responses.create().</p> <code>None</code> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    max_tokens: int = 64_000,\n    *,\n    base_url: str | None = None,\n    api_key: str | None = None,\n    reasoning_effort: str | None = None,\n    timeout: float | None = None,\n    max_retries: int = 2,\n    instructions: str | None = None,\n    kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initialize OpenAI SDK client with model configuration for Responses API.\n\n    Args:\n        model: Model identifier (e.g., 'gpt-4o', 'o1-preview').\n        max_tokens: Maximum output tokens. Defaults to 64,000.\n        base_url: API base URL. If None, uses OpenAI's standard URL.\n            Use for OpenAI-compatible providers.\n        api_key: API key for authentication. If None, reads from OPENROUTER_API_KEY\n            environment variable.\n        reasoning_effort: Reasoning effort level for extended thinking models\n            (e.g., 'low', 'medium', 'high'). Only used with o1/o3 style models.\n        timeout: Request timeout in seconds. If None, uses OpenAI SDK default.\n        max_retries: Number of retries for transient errors. Defaults to 2.\n        instructions: Default system-level instructions. Can be overridden by\n            SystemMessage in the messages list.\n        kwargs: Additional arguments passed to responses.create().\n    \"\"\"\n    self._model = model\n    self._max_tokens = max_tokens\n    self._reasoning_effort = reasoning_effort\n    self._default_instructions = instructions\n    self._kwargs = kwargs or {}\n\n    # Initialize AsyncOpenAI client\n    resolved_api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n\n    # Strip /responses suffix if present - SDK appends it automatically\n    resolved_base_url = base_url\n    if resolved_base_url and resolved_base_url.rstrip(\"/\").endswith(\"/responses\"):\n        resolved_base_url = resolved_base_url.rstrip(\"/\").removesuffix(\"/responses\")\n\n    self._client = AsyncOpenAI(\n        api_key=resolved_api_key,\n        base_url=resolved_base_url,\n        timeout=timeout,\n        max_retries=max_retries,\n    )\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient--standard-openai-usage","title":"Standard OpenAI usage","text":"<p>client = OpenResponsesClient(model=\"gpt-4o\", max_tokens=128_000)</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient--custom-openai-compatible-endpoint","title":"Custom OpenAI-compatible endpoint","text":"<p>client = OpenResponsesClient( ...     model=\"gpt-4o\", ...     base_url=\"http://localhost:8000/v1\", ...     api_key=\"your-api-key\", ... )</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient.max_tokens","title":"max_tokens  <code>property</code>","text":"<pre><code>max_tokens: int\n</code></pre> <p>Maximum output tokens.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient.model_slug","title":"model_slug  <code>property</code>","text":"<pre><code>model_slug: str\n</code></pre> <p>Model identifier.</p>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client.OpenResponsesClient.generate","title":"generate  <code>async</code>","text":"<pre><code>generate(\n    messages: list[ChatMessage], tools: dict[str, Tool]\n) -&gt; AssistantMessage\n</code></pre> <p>Generate assistant response with optional tool calls using Responses API.</p> <p>Retries up to 3 times on transient errors (connection, timeout, rate limit, internal server errors) with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatMessage]</code> <p>List of conversation messages.</p> required <code>tools</code> <code>dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool objects.</p> required <p>Returns:</p> Type Description <code>AssistantMessage</code> <p>AssistantMessage containing the model's response, any tool calls,</p> <code>AssistantMessage</code> <p>and token usage statistics.</p> <p>Raises:</p> Type Description <code>ContextOverflowError</code> <p>If the response is incomplete due to token limits.</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>@retry(\n    retry=retry_if_exception_type(\n        (\n            APIConnectionError,\n            APITimeoutError,\n            RateLimitError,\n            InternalServerError,\n        )\n    ),\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10),\n)\nasync def generate(\n    self,\n    messages: list[ChatMessage],\n    tools: dict[str, Tool],\n) -&gt; AssistantMessage:\n    \"\"\"Generate assistant response with optional tool calls using Responses API.\n\n    Retries up to 3 times on transient errors (connection, timeout, rate limit,\n    internal server errors) with exponential backoff.\n\n    Args:\n        messages: List of conversation messages.\n        tools: Dictionary mapping tool names to Tool objects.\n\n    Returns:\n        AssistantMessage containing the model's response, any tool calls,\n        and token usage statistics.\n\n    Raises:\n        ContextOverflowError: If the response is incomplete due to token limits.\n    \"\"\"\n    # Convert messages to OpenResponses format\n    instructions, input_items = _to_open_responses_input(messages)\n\n    # Use provided instructions or fall back to default\n    final_instructions = instructions or self._default_instructions\n\n    # Build request kwargs\n    request_kwargs: dict[str, Any] = {\n        \"model\": self._model,\n        \"input\": input_items,\n        \"max_output_tokens\": self._max_tokens,\n        **self._kwargs,\n    }\n\n    # Add instructions if present\n    if final_instructions:\n        request_kwargs[\"instructions\"] = final_instructions\n\n    # Add tools if provided\n    if tools:\n        request_kwargs[\"tools\"] = _to_open_responses_tools(tools)\n        request_kwargs[\"tool_choice\"] = \"auto\"\n\n    # Add reasoning effort if configured (for o1/o3 models)\n    if self._reasoning_effort:\n        request_kwargs[\"reasoning\"] = {\"effort\": self._reasoning_effort}\n\n    # Make API call\n    request_start_time = perf_counter()\n    response = await self._client.responses.create(**request_kwargs)\n    request_end_time = perf_counter()\n\n    # Check for incomplete response (context overflow)\n    if response.status == \"incomplete\":\n        stop_reason = getattr(response, \"incomplete_details\", None)\n        raise ContextOverflowError(\n            f\"Response incomplete for model {self.model_slug}: {stop_reason}. \"\n            \"Reduce max_tokens or message length and try again.\"\n        )\n\n    # Parse response output\n    content, tool_calls, reasoning = _parse_response_output(response.output)\n\n    # Parse token usage\n    usage = response.usage\n    input_tokens = usage.input_tokens if usage else 0\n    output_tokens = usage.output_tokens if usage else 0\n\n    # Handle reasoning tokens if available\n    reasoning_tokens = 0\n    if usage and hasattr(usage, \"output_tokens_details\") and usage.output_tokens_details:\n        reasoning_tokens = getattr(usage.output_tokens_details, \"reasoning_tokens\", 0) or 0\n\n    answer_tokens = output_tokens - reasoning_tokens\n\n    return AssistantMessage(\n        reasoning=reasoning,\n        content=content,\n        tool_calls=tool_calls,\n        token_usage=TokenUsage(\n            input=input_tokens,\n            answer=answer_tokens,\n            reasoning=reasoning_tokens,\n        ),\n        request_start_time=request_start_time,\n        request_end_time=request_end_time,\n    )\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._content_to_open_responses_input","title":"_content_to_open_responses_input","text":"<pre><code>_content_to_open_responses_input(\n    content: Content,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert Content blocks to OpenResponses input content format.</p> <p>Uses input_text for text content (vs output_text for responses).</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _content_to_open_responses_input(content: Content) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Content blocks to OpenResponses input content format.\n\n    Uses input_text for text content (vs output_text for responses).\n    \"\"\"\n    if isinstance(content, str):\n        return [{\"type\": \"input_text\", \"text\": content}]\n\n    out: list[dict[str, Any]] = []\n    for block in content:\n        if isinstance(block, str):\n            out.append({\"type\": \"input_text\", \"text\": block})\n        elif isinstance(block, ImageContentBlock):\n            out.append({\"type\": \"input_image\", \"image_url\": block.to_base64_url()})\n        elif isinstance(block, AudioContentBlock):\n            out.append(\n                {\n                    \"type\": \"input_audio\",\n                    \"input_audio\": {\n                        \"data\": block.to_base64_url().split(\",\")[1],\n                        \"format\": block.extension,\n                    },\n                }\n            )\n        elif isinstance(block, VideoContentBlock):\n            out.append({\"type\": \"input_file\", \"file_data\": block.to_base64_url()})\n        else:\n            raise NotImplementedError(f\"Unsupported content block: {type(block)}\")\n    return out\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._content_to_open_responses_output","title":"_content_to_open_responses_output","text":"<pre><code>_content_to_open_responses_output(\n    content: Content,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert Content blocks to OpenResponses output content format.</p> <p>Uses output_text for assistant message content.</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _content_to_open_responses_output(content: Content) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Content blocks to OpenResponses output content format.\n\n    Uses output_text for assistant message content.\n    \"\"\"\n    if isinstance(content, str):\n        return [{\"type\": \"output_text\", \"text\": content}]\n\n    out: list[dict[str, Any]] = []\n    for block in content:\n        if isinstance(block, str):\n            out.append({\"type\": \"output_text\", \"text\": block})\n        else:\n            raise NotImplementedError(f\"Unsupported output content block: {type(block)}\")\n    return out\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._to_open_responses_tools","title":"_to_open_responses_tools","text":"<pre><code>_to_open_responses_tools(\n    tools: dict[str, Tool],\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Convert Tool objects to OpenResponses function format.</p> <p>OpenResponses API expects tools with name/description/parameters at top level, not nested under a 'function' key like Chat Completions API.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool objects.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of tool definitions in OpenResponses format.</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _to_open_responses_tools(tools: dict[str, Tool]) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert Tool objects to OpenResponses function format.\n\n    OpenResponses API expects tools with name/description/parameters at top level,\n    not nested under a 'function' key like Chat Completions API.\n\n    Args:\n        tools: Dictionary mapping tool names to Tool objects.\n\n    Returns:\n        List of tool definitions in OpenResponses format.\n    \"\"\"\n    out: list[dict[str, Any]] = []\n    for t in tools.values():\n        tool_def: dict[str, Any] = {\n            \"type\": \"function\",\n            \"name\": t.name,\n            \"description\": t.description,\n        }\n        if t.parameters is not EmptyParams:\n            tool_def[\"parameters\"] = t.parameters.model_json_schema()\n        out.append(tool_def)\n    return out\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._to_open_responses_input","title":"_to_open_responses_input","text":"<pre><code>_to_open_responses_input(\n    msgs: list[ChatMessage],\n) -&gt; tuple[str | None, list[dict[str, Any]]]\n</code></pre> <p>Convert ChatMessage list to OpenResponses (instructions, input) tuple.</p> <p>SystemMessage content is extracted as the instructions parameter. Other messages are converted to input items.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Tuple of (instructions, input_items) where instructions is the system</p> <code>list[dict[str, Any]]</code> <p>message content (or None) and input_items is the list of input items.</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _to_open_responses_input(\n    msgs: list[ChatMessage],\n) -&gt; tuple[str | None, list[dict[str, Any]]]:\n    \"\"\"Convert ChatMessage list to OpenResponses (instructions, input) tuple.\n\n    SystemMessage content is extracted as the instructions parameter.\n    Other messages are converted to input items.\n\n    Returns:\n        Tuple of (instructions, input_items) where instructions is the system\n        message content (or None) and input_items is the list of input items.\n    \"\"\"\n    instructions: str | None = None\n    input_items: list[dict[str, Any]] = []\n\n    for m in msgs:\n        if isinstance(m, SystemMessage):\n            # Extract system message as instructions\n            if isinstance(m.content, str):\n                instructions = m.content\n            else:\n                # Join text content blocks for instructions\n                instructions = \"\\n\".join(block if isinstance(block, str) else \"\" for block in m.content)\n        elif isinstance(m, UserMessage):\n            input_items.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": _content_to_open_responses_input(m.content),\n                }\n            )\n        elif isinstance(m, AssistantMessage):\n            # For assistant messages, we need to add them as response output items\n            # First add any text content as a message item\n            content_str = (\n                m.content\n                if isinstance(m.content, str)\n                else \"\\n\".join(block if isinstance(block, str) else \"\" for block in m.content)\n            )\n            if content_str:\n                input_items.append(\n                    {\n                        \"type\": \"message\",\n                        \"role\": \"assistant\",\n                        \"content\": [{\"type\": \"output_text\", \"text\": content_str}],\n                    }\n                )\n\n            # Add tool calls as separate function_call items\n            input_items.extend(\n                {\n                    \"type\": \"function_call\",\n                    \"call_id\": tc.tool_call_id,\n                    \"name\": tc.name,\n                    \"arguments\": tc.arguments,\n                }\n                for tc in m.tool_calls\n            )\n        elif isinstance(m, ToolMessage):\n            # Tool results are function_call_output items\n            content_str = m.content if isinstance(m.content, str) else str(m.content)\n            input_items.append(\n                {\n                    \"type\": \"function_call_output\",\n                    \"call_id\": m.tool_call_id,\n                    \"output\": content_str,\n                }\n            )\n        else:\n            raise NotImplementedError(f\"Unsupported message type: {type(m)}\")\n\n    return instructions, input_items\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._get_attr","title":"_get_attr","text":"<pre><code>_get_attr(obj: Any, name: str, default: Any = None) -&gt; Any\n</code></pre> <p>Get attribute from object or dict, with fallback default.</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _get_attr(obj: Any, name: str, default: Any = None) -&gt; Any:  # noqa: ANN401\n    \"\"\"Get attribute from object or dict, with fallback default.\"\"\"\n    if isinstance(obj, dict):\n        return obj.get(name, default)\n    return getattr(obj, name, default)\n</code></pre>"},{"location":"api/clients/open_responses/#stirrup.clients.open_responses_client._parse_response_output","title":"_parse_response_output","text":"<pre><code>_parse_response_output(\n    output: list[Any],\n) -&gt; tuple[str, list[ToolCall], Reasoning | None]\n</code></pre> <p>Parse response output items into content, tool_calls, and reasoning.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>list[Any]</code> <p>List of output items from the response.</p> required <p>Returns:</p> Type Description <code>tuple[str, list[ToolCall], Reasoning | None]</code> <p>Tuple of (content_text, tool_calls, reasoning).</p> Source code in <code>src/stirrup/clients/open_responses_client.py</code> <pre><code>def _parse_response_output(\n    output: list[Any],\n) -&gt; tuple[str, list[ToolCall], Reasoning | None]:\n    \"\"\"Parse response output items into content, tool_calls, and reasoning.\n\n    Args:\n        output: List of output items from the response.\n\n    Returns:\n        Tuple of (content_text, tool_calls, reasoning).\n    \"\"\"\n    content_parts: list[str] = []\n    tool_calls: list[ToolCall] = []\n    reasoning: Reasoning | None = None\n\n    for item in output:\n        item_type = _get_attr(item, \"type\")\n\n        if item_type == \"message\":\n            # Extract text content from message\n            msg_content = _get_attr(item, \"content\", [])\n            for content_item in msg_content:\n                content_type = _get_attr(content_item, \"type\")\n                if content_type == \"output_text\":\n                    text = _get_attr(content_item, \"text\", \"\")\n                    content_parts.append(text)\n\n        elif item_type == \"function_call\":\n            call_id = _get_attr(item, \"call_id\")\n            name = _get_attr(item, \"name\")\n            arguments = _get_attr(item, \"arguments\", \"\")\n            tool_calls.append(\n                ToolCall(\n                    tool_call_id=call_id,\n                    name=name,\n                    arguments=arguments,\n                )\n            )\n\n        elif item_type == \"reasoning\":\n            # Extract reasoning/thinking content - try multiple possible attribute names\n            # summary can be a list of Summary objects with .text attribute\n            summary = _get_attr(item, \"summary\")\n            if summary:\n                if isinstance(summary, list):\n                    # Extract text from Summary objects\n                    thinking = \"\\n\".join(_get_attr(s, \"text\", \"\") for s in summary if _get_attr(s, \"text\"))\n                else:\n                    thinking = str(summary)\n            else:\n                thinking = _get_attr(item, \"thinking\") or \"\"\n\n            if thinking:\n                reasoning = Reasoning(content=thinking)\n\n    return \"\\n\".join(content_parts), tool_calls, reasoning\n</code></pre>"},{"location":"api/core/agent/","title":"Agent","text":""},{"location":"api/core/agent/#stirrup.core.agent","title":"stirrup.core.agent","text":""},{"location":"api/core/agent/#stirrup.core.agent.AGENT_MAX_TURNS","title":"AGENT_MAX_TURNS  <code>module-attribute</code>","text":"<pre><code>AGENT_MAX_TURNS = 30\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CONTEXT_SUMMARIZATION_CUTOFF","title":"CONTEXT_SUMMARIZATION_CUTOFF  <code>module-attribute</code>","text":"<pre><code>CONTEXT_SUMMARIZATION_CUTOFF = 0.7\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.FINISH_TOOL_NAME","title":"FINISH_TOOL_NAME  <code>module-attribute</code>","text":"<pre><code>FINISH_TOOL_NAME: Literal['finish'] = 'finish'\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.TURNS_REMAINING_WARNING_THRESHOLD","title":"TURNS_REMAINING_WARNING_THRESHOLD  <code>module-attribute</code>","text":"<pre><code>TURNS_REMAINING_WARNING_THRESHOLD = 20\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.MESSAGE_SUMMARIZER","title":"MESSAGE_SUMMARIZER  <code>module-attribute</code>","text":"<pre><code>MESSAGE_SUMMARIZER = read_text(encoding='utf-8')\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.MESSAGE_SUMMARIZER_BRIDGE_TEMPLATE","title":"MESSAGE_SUMMARIZER_BRIDGE_TEMPLATE  <code>module-attribute</code>","text":"<pre><code>MESSAGE_SUMMARIZER_BRIDGE_TEMPLATE = read_text(\n    encoding=\"utf-8\"\n)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.DEFAULT_TOOLS","title":"DEFAULT_TOOLS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TOOLS: list[Tool[Any, Any] | ToolProvider] = [\n    LocalCodeExecToolProvider(),\n    WebToolProvider(),\n]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.SIMPLE_FINISH_TOOL","title":"SIMPLE_FINISH_TOOL  <code>module-attribute</code>","text":"<pre><code>SIMPLE_FINISH_TOOL: Tool[\n    FinishParams, ToolUseCountMetadata\n] = Tool[FinishParams, ToolUseCountMetadata](\n    name=FINISH_TOOL_NAME,\n    description=\"Signal task completion with a reason. Use when the task is finished or cannot proceed further. Note that you will need a separate turn to finish.\",\n    parameters=FinishParams,\n    executor=_validating_finish_executor,\n)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._PARENT_DEPTH","title":"_PARENT_DEPTH  <code>module-attribute</code>","text":"<pre><code>_PARENT_DEPTH: ContextVar[int] = ContextVar(\n    \"parent_depth\", default=0\n)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._SESSION_STATE","title":"_SESSION_STATE  <code>module-attribute</code>","text":"<pre><code>_SESSION_STATE: ContextVar[SessionState] = ContextVar(\n    \"session_state\"\n)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['Agent', 'SubAgentParams']\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LOGGER","title":"LOGGER  <code>module-attribute</code>","text":"<pre><code>LOGGER = getLogger(__name__)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.DEFAULT_SUB_AGENT_DESCRIPTION","title":"DEFAULT_SUB_AGENT_DESCRIPTION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_SUB_AGENT_DESCRIPTION = \"A sub agent that can be used to handle a contained, specific task.\"\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AGENT_NAME_PATTERN","title":"AGENT_NAME_PATTERN  <code>module-attribute</code>","text":"<pre><code>AGENT_NAME_PATTERN = compile('^[a-zA-Z0-9_-]{1,128}$')\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.ChatMessage","title":"ChatMessage","text":"<pre><code>ChatMessage = Annotated[\n    SystemMessage\n    | UserMessage\n    | AssistantMessage\n    | ToolMessage,\n    Field(discriminator=role),\n]\n</code></pre> <p>Discriminated union of all message types, automatically parsed based on role field.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager","title":"CacheManager","text":"<pre><code>CacheManager(\n    cache_base_dir: Path | None = None,\n    clear_on_success: bool = True,\n)\n</code></pre> <p>Manages cache operations for agent sessions.</p> <p>Handles saving/loading cache state and execution environment files.</p> <p>Initialize CacheManager.</p> <p>Parameters:</p> Name Type Description Default <code>cache_base_dir</code> <code>Path | None</code> <p>Base directory for cache storage.            Defaults to ~/.cache/stirrup/</p> <code>None</code> <code>clear_on_success</code> <code>bool</code> <p>If True (default), automatically clear the cache when              the agent completes successfully. Set to False to preserve              caches for inspection or manual management.</p> <code>True</code> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def __init__(\n    self,\n    cache_base_dir: Path | None = None,\n    clear_on_success: bool = True,\n) -&gt; None:\n    \"\"\"Initialize CacheManager.\n\n    Args:\n        cache_base_dir: Base directory for cache storage.\n                       Defaults to ~/.cache/stirrup/\n        clear_on_success: If True (default), automatically clear the cache when\n                         the agent completes successfully. Set to False to preserve\n                         caches for inspection or manual management.\n    \"\"\"\n    self._cache_base_dir = cache_base_dir or DEFAULT_CACHE_DIR\n    self.clear_on_success = clear_on_success\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.save_state","title":"save_state","text":"<pre><code>save_state(\n    task_hash: str,\n    state: CacheState,\n    exec_env_dir: Path | None = None,\n) -&gt; None\n</code></pre> <p>Save cache state and optionally archive execution environment files.</p> <p>Uses atomic writes to prevent corrupted cache files if interrupted mid-write.</p> <p>Parameters:</p> Name Type Description Default <code>task_hash</code> <code>str</code> <p>Unique identifier for this task/cache.</p> required <code>state</code> <code>CacheState</code> <p>CacheState to persist.</p> required <code>exec_env_dir</code> <code>Path | None</code> <p>Optional path to execution environment temp directory.          If provided, all files will be copied to cache.</p> <code>None</code> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def save_state(\n    self,\n    task_hash: str,\n    state: CacheState,\n    exec_env_dir: Path | None = None,\n) -&gt; None:\n    \"\"\"Save cache state and optionally archive execution environment files.\n\n    Uses atomic writes to prevent corrupted cache files if interrupted mid-write.\n\n    Args:\n        task_hash: Unique identifier for this task/cache.\n        state: CacheState to persist.\n        exec_env_dir: Optional path to execution environment temp directory.\n                     If provided, all files will be copied to cache.\n    \"\"\"\n    cache_dir = self._get_cache_dir(task_hash)\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save state JSON using atomic write (write to temp file, then rename)\n    state_file = self._get_state_file(task_hash)\n    temp_file = state_file.with_suffix(\".json.tmp\")\n\n    try:\n        state_data = state.to_dict()\n        logger.debug(\"Serialized cache state: turn=%d, msgs=%d\", state.turn, len(state.msgs))\n\n        with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(state_data, f, indent=2, ensure_ascii=False)\n            f.flush()\n            os.fsync(f.fileno())  # Ensure data is written to disk\n\n        logger.debug(\"Wrote temp file: %s\", temp_file)\n\n        # Atomic rename (on POSIX systems)\n        temp_file.replace(state_file)\n        logger.info(\"Saved cache state to %s (turn %d)\", state_file, state.turn)\n    except Exception as e:\n        logger.exception(\"Failed to save cache state: %s\", e)\n        # Try direct write as fallback\n        try:\n            logger.warning(\"Attempting direct write as fallback\")\n            with open(state_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(state_data, f, indent=2, ensure_ascii=False)\n                f.flush()\n                os.fsync(f.fileno())\n            logger.info(\"Fallback write succeeded to %s\", state_file)\n        except Exception as e2:\n            logger.exception(\"Fallback write also failed: %s\", e2)\n        # Clean up temp file if it exists\n        if temp_file.exists():\n            temp_file.unlink()\n        raise\n\n    # Copy execution environment files if provided\n    if exec_env_dir and exec_env_dir.exists():\n        files_dir = self._get_files_dir(task_hash)\n        if files_dir.exists():\n            shutil.rmtree(files_dir)  # Clear existing files\n        shutil.copytree(exec_env_dir, files_dir, dirs_exist_ok=True)\n        logger.info(\"Saved execution environment files to %s\", files_dir)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.load_state","title":"load_state","text":"<pre><code>load_state(task_hash: str) -&gt; CacheState | None\n</code></pre> <p>Load cached state for a task hash.</p> <p>Parameters:</p> Name Type Description Default <code>task_hash</code> <code>str</code> <p>Unique identifier for the task/cache.</p> required <p>Returns:</p> Type Description <code>CacheState | None</code> <p>CacheState if cache exists, None otherwise.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def load_state(self, task_hash: str) -&gt; CacheState | None:\n    \"\"\"Load cached state for a task hash.\n\n    Args:\n        task_hash: Unique identifier for the task/cache.\n\n    Returns:\n        CacheState if cache exists, None otherwise.\n    \"\"\"\n    state_file = self._get_state_file(task_hash)\n    if not state_file.exists():\n        logger.debug(\"No cache found for task %s\", task_hash)\n        return None\n\n    try:\n        with open(state_file, encoding=\"utf-8\") as f:\n            data = json.load(f)\n        state = CacheState.from_dict(data)\n        logger.info(\"Loaded cache state from %s (turn %d)\", state_file, state.turn)\n        return state\n    except (json.JSONDecodeError, KeyError, ValueError) as e:\n        logger.warning(\"Failed to load cache for task %s: %s\", task_hash, e)\n        return None\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.restore_files","title":"restore_files","text":"<pre><code>restore_files(task_hash: str, dest_dir: Path) -&gt; bool\n</code></pre> <p>Restore cached files to the destination directory.</p> <p>Parameters:</p> Name Type Description Default <code>task_hash</code> <code>str</code> <p>Unique identifier for the task/cache.</p> required <code>dest_dir</code> <code>Path</code> <p>Destination directory (typically the new exec env temp dir).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if files were restored, False if no files cache exists.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def restore_files(self, task_hash: str, dest_dir: Path) -&gt; bool:\n    \"\"\"Restore cached files to the destination directory.\n\n    Args:\n        task_hash: Unique identifier for the task/cache.\n        dest_dir: Destination directory (typically the new exec env temp dir).\n\n    Returns:\n        True if files were restored, False if no files cache exists.\n    \"\"\"\n    files_dir = self._get_files_dir(task_hash)\n    if not files_dir.exists():\n        logger.debug(\"No cached files for task %s\", task_hash)\n        return False\n\n    # Copy all files from cache to destination\n    for item in files_dir.iterdir():\n        dest_item = dest_dir / item.name\n        if item.is_file():\n            shutil.copy2(item, dest_item)\n        else:\n            shutil.copytree(item, dest_item, dirs_exist_ok=True)\n\n    logger.info(\"Restored cached files from %s to %s\", files_dir, dest_dir)\n    return True\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache(task_hash: str) -&gt; None\n</code></pre> <p>Remove cache for a specific task.</p> <p>Called after successful completion to clean up.</p> <p>Parameters:</p> Name Type Description Default <code>task_hash</code> <code>str</code> <p>Unique identifier for the task/cache.</p> required Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def clear_cache(self, task_hash: str) -&gt; None:\n    \"\"\"Remove cache for a specific task.\n\n    Called after successful completion to clean up.\n\n    Args:\n        task_hash: Unique identifier for the task/cache.\n    \"\"\"\n    cache_dir = self._get_cache_dir(task_hash)\n    if cache_dir.exists():\n        shutil.rmtree(cache_dir)\n        logger.info(\"Cleared cache for task %s\", task_hash)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.list_caches","title":"list_caches","text":"<pre><code>list_caches() -&gt; list[str]\n</code></pre> <p>List all available cache hashes.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of task hashes with existing caches.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def list_caches(self) -&gt; list[str]:\n    \"\"\"List all available cache hashes.\n\n    Returns:\n        List of task hashes with existing caches.\n    \"\"\"\n    if not self._cache_base_dir.exists():\n        return []\n\n    return [d.name for d in self._cache_base_dir.iterdir() if d.is_dir() and (d / \"state.json\").exists()]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheManager.get_cache_info","title":"get_cache_info","text":"<pre><code>get_cache_info(task_hash: str) -&gt; dict | None\n</code></pre> <p>Get metadata about a cache without fully loading it.</p> <p>Parameters:</p> Name Type Description Default <code>task_hash</code> <code>str</code> <p>Unique identifier for the task/cache.</p> required <p>Returns:</p> Type Description <code>dict | None</code> <p>Dictionary with cache info (turn, timestamp, agent_name) or None.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def get_cache_info(self, task_hash: str) -&gt; dict | None:\n    \"\"\"Get metadata about a cache without fully loading it.\n\n    Args:\n        task_hash: Unique identifier for the task/cache.\n\n    Returns:\n        Dictionary with cache info (turn, timestamp, agent_name) or None.\n    \"\"\"\n    state_file = self._get_state_file(task_hash)\n    if not state_file.exists():\n        return None\n\n    try:\n        with open(state_file, encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return {\n            \"task_hash\": task_hash,\n            \"turn\": data.get(\"turn\", 0),\n            \"timestamp\": data.get(\"timestamp\", \"\"),\n            \"agent_name\": data.get(\"agent_name\", \"\"),\n            \"has_files\": self._get_files_dir(task_hash).exists(),\n        }\n    except (json.JSONDecodeError, KeyError):\n        return None\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState","title":"CacheState  <code>dataclass</code>","text":"<pre><code>CacheState(\n    msgs: list[ChatMessage],\n    full_msg_history: list[list[ChatMessage]],\n    turn: int,\n    run_metadata: dict[str, list[Any]],\n    task_hash: str,\n    timestamp: str = (lambda: isoformat())(),\n    agent_name: str = \"\",\n)\n</code></pre> <p>Serializable state for resuming an agent run.</p> <p>Captures all necessary state to resume execution from a specific turn.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.msgs","title":"msgs  <code>instance-attribute</code>","text":"<pre><code>msgs: list[ChatMessage]\n</code></pre> <p>Current conversation messages in the active run loop.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.full_msg_history","title":"full_msg_history  <code>instance-attribute</code>","text":"<pre><code>full_msg_history: list[list[ChatMessage]]\n</code></pre> <p>Groups of messages (separated when context summarization occurs).</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.turn","title":"turn  <code>instance-attribute</code>","text":"<pre><code>turn: int\n</code></pre> <p>Current turn number (0-indexed) - resume will start from this turn.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.run_metadata","title":"run_metadata  <code>instance-attribute</code>","text":"<pre><code>run_metadata: dict[str, list[Any]]\n</code></pre> <p>Accumulated tool metadata from the run.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.task_hash","title":"task_hash  <code>instance-attribute</code>","text":"<pre><code>task_hash: str\n</code></pre> <p>Hash of the original init_msgs for verification on resume.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: str = field(default_factory=lambda: isoformat())\n</code></pre> <p>ISO timestamp when cache was created.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.agent_name","title":"agent_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_name: str = ''\n</code></pre> <p>Name of the agent that created this cache.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to JSON-serializable dictionary.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to JSON-serializable dictionary.\"\"\"\n    return {\n        \"msgs\": serialize_messages(self.msgs),\n        \"full_msg_history\": [serialize_messages(group) for group in self.full_msg_history],\n        \"turn\": self.turn,\n        \"run_metadata\": _serialize_run_metadata(self.run_metadata),\n        \"task_hash\": self.task_hash,\n        \"timestamp\": self.timestamp,\n        \"agent_name\": self.agent_name,\n    }\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CacheState.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict) -&gt; CacheState\n</code></pre> <p>Create CacheState from JSON dictionary.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"CacheState\":\n    \"\"\"Create CacheState from JSON dictionary.\"\"\"\n    return cls(\n        msgs=deserialize_messages(data[\"msgs\"]),\n        full_msg_history=[deserialize_messages(group) for group in data[\"full_msg_history\"]],\n        turn=data[\"turn\"],\n        run_metadata=data[\"run_metadata\"],\n        task_hash=data[\"task_hash\"],\n        timestamp=data.get(\"timestamp\", \"\"),\n        agent_name=data.get(\"agent_name\", \"\"),\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/core/agent/#stirrup.core.agent.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/core/agent/#stirrup.core.agent.ImageContentBlock","title":"ImageContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>Image content supporting PNG, JPEG, WebP, PSD formats with automatic downscaling.</p>"},{"location":"api/core/agent/#stirrup.core.agent.ImageContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(\n    max_pixels: int | None = RESOLUTION_1MP,\n) -&gt; str\n</code></pre> <p>Convert image to base64 data URL, optionally resizing to max pixel count.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, max_pixels: int | None = RESOLUTION_1MP) -&gt; str:\n    \"\"\"Convert image to base64 data URL, optionally resizing to max pixel count.\"\"\"\n    img: Image.Image = Image.open(BytesIO(self.data))\n    if max_pixels is not None and img.width * img.height &gt; max_pixels:\n        tw, th = downscale_image(img.width, img.height, max_pixels)\n        img.thumbnail((tw, th), Image.Resampling.LANCZOS)\n    if img.mode != \"RGB\":\n        img = img.convert(\"RGB\")\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    return f\"data:image/png;base64,{b64encode(buf.getvalue()).decode()}\"\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for LLM client implementations.</p> <p>Any LLM client must implement this protocol to work with the Agent class. Provides text generation with tool support and model capability inspection.</p>"},{"location":"api/core/agent/#stirrup.core.agent.SubAgentMetadata","title":"SubAgentMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata from sub-agent execution including token usage, message history, and child run metadata.</p> <p>Implements Addable protocol to support aggregation across multiple subagent calls.</p>"},{"location":"api/core/agent/#stirrup.core.agent.SubAgentMetadata.__add__","title":"__add__","text":"<pre><code>__add__(other: SubAgentMetadata) -&gt; SubAgentMetadata\n</code></pre> <p>Combine metadata from multiple subagent calls.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"SubAgentMetadata\") -&gt; \"SubAgentMetadata\":\n    \"\"\"Combine metadata from multiple subagent calls.\"\"\"\n    # Concatenate message histories\n    combined_history = self.message_history + other.message_history\n    # Merge run metadata (concatenate lists per key, keep last for non-list internal keys)\n    combined_meta: dict[str, Any] = dict(self.run_metadata)\n    for key, value in other.run_metadata.items():\n        if key in combined_meta and isinstance(combined_meta[key], list) and isinstance(value, list):\n            combined_meta[key] = combined_meta[key] + value\n        else:\n            combined_meta[key] = value\n    return SubAgentMetadata(\n        message_history=combined_history,\n        run_metadata=combined_meta,\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>System-level instructions and context for the LLM.</p>"},{"location":"api/core/agent/#stirrup.core.agent.TokenUsage","title":"TokenUsage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token counts for LLM usage.</p> <p>Token terminology: output = reasoning + answer.</p>"},{"location":"api/core/agent/#stirrup.core.agent.TokenUsage.output","title":"output  <code>property</code>","text":"<pre><code>output: int\n</code></pre> <p>Total output tokens (reasoning + answer).</p>"},{"location":"api/core/agent/#stirrup.core.agent.TokenUsage.total","title":"total  <code>property</code>","text":"<pre><code>total: int\n</code></pre> <p>Total token count across input, answer, and reasoning.</p>"},{"location":"api/core/agent/#stirrup.core.agent.TokenUsage.__add__","title":"__add__","text":"<pre><code>__add__(other: TokenUsage) -&gt; TokenUsage\n</code></pre> <p>Add two TokenUsage objects together, summing each field independently.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"TokenUsage\") -&gt; \"TokenUsage\":\n    \"\"\"Add two TokenUsage objects together, summing each field independently.\"\"\"\n    return TokenUsage(\n        input=self.input + other.input,\n        answer=self.answer + other.answer,\n        reasoning=self.reasoning + other.reasoning,\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition with name, description, parameter schema, and executor function.</p> Generic over <p>P: Parameter model type (Pydantic BaseModel subclass, or EmptyParams for parameterless tools) M: Metadata type (should implement Addable for aggregation; use None for tools without metadata)</p> <p>Tools are simple, stateless callables. For tools requiring lifecycle management (setup/teardown, resource pooling), use a ToolProvider instead.</p> Example with parameters <pre><code>class CalcParams(BaseModel):\n    expression: str\n\ncalc_tool = Tool[CalcParams, None](\n    name=\"calc\",\n    description=\"Evaluate math\",\n    parameters=CalcParams,\n    executor=lambda p: ToolResult(content=str(eval(p.expression))),\n)\n</code></pre> <p>Example without parameters (uses EmptyParams by default):     <pre><code>time_tool = Tool[EmptyParams, None](\n    name=\"time\",\n    description=\"Get current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre></p>"},{"location":"api/core/agent/#stirrup.core.agent.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool invocation request from the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the tool to invoke</p> <code>arguments</code> <code>str</code> <p>JSON string containing tool parameters</p> <code>tool_call_id</code> <code>str | None</code> <p>Unique identifier for tracking this tool call and its result</p>"},{"location":"api/core/agent/#stirrup.core.agent.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool execution result returned to the LLM.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['tool']</code> <p>Always \"tool\"</p> <code>content</code> <code>Content</code> <p>The tool result content</p> <code>tool_call_id</code> <code>str | None</code> <p>ID linking this result to the corresponding tool call</p> <code>name</code> <code>str | None</code> <p>Name of the tool that was called</p> <code>args_was_valid</code> <code>bool</code> <p>Whether the tool arguments were valid</p> <code>success</code> <code>bool</code> <p>Whether the tool executed successfully (used by finish tool to control termination)</p>"},{"location":"api/core/agent/#stirrup.core.agent.ToolMessage.tool_duration","title":"tool_duration  <code>property</code>","text":"<pre><code>tool_duration: float | None\n</code></pre> <p>Tool execution duration in seconds.</p>"},{"location":"api/core/agent/#stirrup.core.agent.ToolProvider","title":"ToolProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tool providers with lifecycle management.</p> <p>ToolProviders manage resources (HTTP clients, sandboxes, server connections) and return Tool instances when entering their async context. They implement the async context manager protocol.</p> <p>Use ToolProvider for: - Tools requiring setup/teardown (connections, temp directories) - Tools that return multiple Tool instances (e.g., MCP servers) - Tools with shared state across calls (e.g., HTTP client pooling)</p> Example <p>class MyToolProvider(ToolProvider):     async def aenter(self) -&gt; Tool | list[Tool]:         # Setup resources and return tool(s)         return self._create_tool()</p> <pre><code># __aexit__ is optional - default is no-op\n</code></pre> <p>Agent automatically manages ToolProvider lifecycle via its session() context.</p>"},{"location":"api/core/agent/#stirrup.core.agent.ToolProvider.__aenter__","title":"__aenter__  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool | list[Tool]\n</code></pre> <p>Enter async context: setup resources and return tool(s).</p> <p>Returns:</p> Type Description <code>Tool | list[Tool]</code> <p>A single Tool instance, or a list of Tool instances for providers</p> <code>Tool | list[Tool]</code> <p>that expose multiple tools (e.g., MCP servers).</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>@abstractmethod\nasync def __aenter__(self) -&gt; \"Tool | list[Tool]\":\n    \"\"\"Enter async context: setup resources and return tool(s).\n\n    Returns:\n        A single Tool instance, or a list of Tool instances for providers\n        that expose multiple tools (e.g., MCP servers).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.ToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit async context: cleanup resources. Default: no-op.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>async def __aexit__(  # noqa: B027\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit async context: cleanup resources. Default: no-op.\"\"\"\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.ToolResult","title":"ToolResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result from a tool executor with optional metadata.</p> <p>Generic over metadata type M. M should implement Addable protocol for aggregation support, but this is not enforced at the class level due to Pydantic schema generation limitations.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Content</code> <p>The result content (string, list of content blocks, or images)</p> <code>success</code> <code>bool</code> <p>Whether the tool call was successful. For finish tools, controls if agent terminates.</p> <code>metadata</code> <code>M | None</code> <p>Optional metadata (e.g., usage stats) that implements Addable for aggregation</p>"},{"location":"api/core/agent/#stirrup.core.agent.UserMessage","title":"UserMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>User input message to the LLM.</p>"},{"location":"api/core/agent/#stirrup.core.agent.SkillMetadata","title":"SkillMetadata  <code>dataclass</code>","text":"<pre><code>SkillMetadata(name: str, description: str, path: str)\n</code></pre> <p>Metadata extracted from a skill's SKILL.md frontmatter.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider","title":"CodeExecToolProvider","text":"<pre><code>CodeExecToolProvider(\n    *, allowed_commands: list[str] | None = None\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code>, <code>ABC</code></p> <p>Abstract base class for code execution tool providers.</p> <p>CodeExecToolProvider is a ToolProvider that manages code execution environments (sandboxes, containers, local temp directories) and returns a code_exec Tool.</p> <p>Subclasses must implement: - aenter(): Initialize environment and return the code_exec tool - aexit(): Cleanup the execution environment - run_command(): Execute a command and return raw result - read_file_bytes(): Read file content as bytes from the environment - write_file_bytes(): Write bytes to a file in the environment</p> <p>Default implementations are provided for: - save_output_files(): Save files to local dir or another exec env (uses primitives) - upload_files(): Upload files from local or another exec env (uses primitives)</p> <p>All code execution providers support an optional allowlist of command patterns. If provided, only commands matching at least one pattern are allowed. If None, all commands are allowed.</p> Usage with Agent <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>client = ChatCompletionsClient(model=\"gpt-5\") agent = Agent(     client=client,     name=\"assistant\",     tools=[LocalCodeExecToolProvider(), CALCULATOR_TOOL], )</p> <p>Initialize execution environment with optional command allowlist.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_commands</code> <code>list[str] | None</code> <p>Optional list of regex patterns. If provided, only              commands matching at least one pattern are allowed.              If None, all commands are allowed.</p> <code>None</code> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def __init__(self, *, allowed_commands: list[str] | None = None) -&gt; None:\n    \"\"\"Initialize execution environment with optional command allowlist.\n\n    Args:\n        allowed_commands: Optional list of regex patterns. If provided, only\n                         commands matching at least one pattern are allowed.\n                         If None, all commands are allowed.\n\n    \"\"\"\n    self._allowed_commands = allowed_commands\n    self._compiled_allowed: list[re.Pattern[str]] | None = None\n    if allowed_commands is not None:\n        self._compiled_allowed = [re.compile(p) for p in allowed_commands]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.temp_dir","title":"temp_dir  <code>property</code>","text":"<pre><code>temp_dir: Path | None\n</code></pre> <p>Return the temporary directory for this execution environment, if any.</p>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.__aenter__","title":"__aenter__  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool[\n    CodeExecutionParams, ToolUseCountMetadata\n]\n</code></pre> <p>Enter async context: set up environment and return code_exec tool.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def __aenter__(self) -&gt; Tool[CodeExecutionParams, ToolUseCountMetadata]:\n    \"\"\"Enter async context: set up environment and return code_exec tool.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.__aexit__","title":"__aexit__  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Exit async context: cleanup the execution environment.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Exit async context: cleanup the execution environment.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.run_command","title":"run_command  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>run_command(\n    cmd: str, *, timeout: int = SHELL_TIMEOUT\n) -&gt; CommandResult\n</code></pre> <p>Execute a shell command and return raw CommandResult.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def run_command(self, cmd: str, *, timeout: int = SHELL_TIMEOUT) -&gt; CommandResult:\n    \"\"\"Execute a shell command and return raw CommandResult.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.read_file_bytes","title":"read_file_bytes  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>read_file_bytes(path: str) -&gt; bytes\n</code></pre> <p>Read file content as bytes from this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path within this execution environment (relative or absolute   within the env's working directory).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>File contents as bytes.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file does not exist.</p> <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def read_file_bytes(self, path: str) -&gt; bytes:\n    \"\"\"Read file content as bytes from this execution environment.\n\n    Args:\n        path: File path within this execution environment (relative or absolute\n              within the env's working directory).\n\n    Returns:\n        File contents as bytes.\n\n    Raises:\n        FileNotFoundError: If file does not exist.\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.write_file_bytes","title":"write_file_bytes  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>write_file_bytes(path: str, content: bytes) -&gt; None\n</code></pre> <p>Write bytes to a file in this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Destination path within this execution environment.</p> required <code>content</code> <code>bytes</code> <p>File contents to write.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def write_file_bytes(self, path: str, content: bytes) -&gt; None:\n    \"\"\"Write bytes to a file in this execution environment.\n\n    Args:\n        path: Destination path within this execution environment.\n        content: File contents to write.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.file_exists","title":"file_exists  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>file_exists(path: str) -&gt; bool\n</code></pre> <p>Check if a file exists in this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path within this execution environment (relative or absolute   within the env's working directory).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def file_exists(self, path: str) -&gt; bool:\n    \"\"\"Check if a file exists in this execution environment.\n\n    Args:\n        path: File path within this execution environment (relative or absolute\n              within the env's working directory).\n\n    Returns:\n        True if the file exists, False otherwise.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.is_directory","title":"is_directory  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>is_directory(path: str) -&gt; bool\n</code></pre> <p>Check if a path is a directory in this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path within this execution environment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the path exists and is a directory, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def is_directory(self, path: str) -&gt; bool:\n    \"\"\"Check if a path is a directory in this execution environment.\n\n    Args:\n        path: Path within this execution environment.\n\n    Returns:\n        True if the path exists and is a directory, False otherwise.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.list_files","title":"list_files  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>list_files(path: str) -&gt; list[str]\n</code></pre> <p>List all files recursively in a directory within this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path within this execution environment.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths (relative to the given path) for all files in the directory.</p> <code>list[str]</code> <p>Returns an empty list if the path is a file or doesn't exist.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def list_files(self, path: str) -&gt; list[str]:\n    \"\"\"List all files recursively in a directory within this execution environment.\n\n    Args:\n        path: Directory path within this execution environment.\n\n    Returns:\n        List of file paths (relative to the given path) for all files in the directory.\n        Returns an empty list if the path is a file or doesn't exist.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.save_output_files","title":"save_output_files  <code>async</code>","text":"<pre><code>save_output_files(\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: CodeExecToolProvider | None = None,\n) -&gt; SaveOutputFilesResult\n</code></pre> <p>Save files from this execution environment to a destination.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths in this execution environment to save.</p> required <code>output_dir</code> <code>Path | str</code> <p>Directory path to save files to.</p> required <code>dest_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, output_dir is interpreted as a path within dest_env       (cross-environment transfer). If None, output_dir is a local       filesystem path.</p> <code>None</code> <p>Returns:</p> Type Description <code>SaveOutputFilesResult</code> <p>SaveOutputFilesResult containing lists of saved files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>async def save_output_files(\n    self,\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: \"CodeExecToolProvider | None\" = None,\n) -&gt; SaveOutputFilesResult:\n    \"\"\"Save files from this execution environment to a destination.\n\n    Args:\n        paths: List of file paths in this execution environment to save.\n        output_dir: Directory path to save files to.\n        dest_env: If provided, output_dir is interpreted as a path within dest_env\n                  (cross-environment transfer). If None, output_dir is a local\n                  filesystem path.\n\n    Returns:\n        SaveOutputFilesResult containing lists of saved files and any failures.\n\n    \"\"\"\n    result = SaveOutputFilesResult()\n    output_dir_str = str(output_dir)\n\n    for source_path in paths:\n        try:\n            content = await self.read_file_bytes(source_path)\n            filename = Path(source_path).name\n            dest_path = f\"{output_dir_str}/{filename}\"\n\n            if dest_env:\n                # Transfer to another exec env (cross-environment)\n                logger.debug(\n                    \"CROSS-ENV TRANSFER: %s (%d bytes) -&gt; %s (dest_env: %s)\",\n                    source_path,\n                    len(content),\n                    dest_path,\n                    type(dest_env).__name__,\n                )\n                await dest_env.write_file_bytes(dest_path, content)\n                result.saved.append(SavedFile(source_path, Path(dest_path), len(content)))\n            else:\n                # Save to local filesystem\n                output_path = Path(output_dir) / filename\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n                logger.debug(\n                    \"SAVE TO LOCAL: %s (%d bytes) -&gt; %s\",\n                    source_path,\n                    len(content),\n                    output_path,\n                )\n                output_path.write_bytes(content)\n                result.saved.append(SavedFile(source_path, output_path, len(content)))\n        except Exception as e:\n            logger.debug(\"TRANSFER FAILED: %s -&gt; %s: %s\", source_path, output_dir_str, e)\n            result.failed[source_path] = str(e)\n\n    return result\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.upload_files","title":"upload_files  <code>async</code>","text":"<pre><code>upload_files(\n    *paths: Path | str,\n    source_env: CodeExecToolProvider | None = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult\n</code></pre> <p>Upload files to this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>*paths</code> <code>Path | str</code> <p>File or directory paths to upload. If source_env is None, these     are local filesystem paths. If source_env is provided, these are     paths within source_env (cross-environment transfer).</p> <code>()</code> <code>source_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, paths are within source_env. If None, paths are         local filesystem paths.</p> <code>None</code> <code>dest_dir</code> <code>str | None</code> <p>Destination directory in this environment.       If None, uses the environment's working directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>UploadFilesResult</code> <p>UploadFilesResult containing lists of uploaded files and any failures.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>async def upload_files(\n    self,\n    *paths: Path | str,\n    source_env: \"CodeExecToolProvider | None\" = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult:\n    \"\"\"Upload files to this execution environment.\n\n    Args:\n        *paths: File or directory paths to upload. If source_env is None, these\n                are local filesystem paths. If source_env is provided, these are\n                paths within source_env (cross-environment transfer).\n        source_env: If provided, paths are within source_env. If None, paths are\n                    local filesystem paths.\n        dest_dir: Destination directory in this environment.\n                  If None, uses the environment's working directory.\n\n    Returns:\n        UploadFilesResult containing lists of uploaded files and any failures.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    result = UploadFilesResult()\n    dest_dir_str = dest_dir or \"\"\n\n    for path in paths:\n        path_str = str(path)\n        try:\n            if source_env:\n                # Cross-environment transfer: read from source_env\n                # Check if it's a directory first\n                if await source_env.is_directory(path_str):\n                    # Handle directory recursively\n                    # Preserve directory name when dest_dir not specified\n                    dir_name = Path(path_str).name\n                    files = await source_env.list_files(path_str)\n                    for rel_file_path in files:\n                        src_file_path = f\"{path_str}/{rel_file_path}\"\n                        # If dest_dir specified, put files directly there\n                        # Otherwise, preserve the source directory name\n                        if dest_dir_str:\n                            dest_path = f\"{dest_dir_str}/{rel_file_path}\"\n                        else:\n                            dest_path = f\"{dir_name}/{rel_file_path}\"\n                        content = await source_env.read_file_bytes(src_file_path)\n                        logger.debug(\n                            \"UPLOAD CROSS-ENV (dir): %s (%d bytes) from %s -&gt; %s\",\n                            src_file_path,\n                            len(content),\n                            type(source_env).__name__,\n                            dest_path,\n                        )\n                        await self.write_file_bytes(dest_path, content)\n                        result.uploaded.append(UploadedFile(Path(src_file_path), dest_path, len(content)))\n                else:\n                    # Single file transfer\n                    content = await source_env.read_file_bytes(path_str)\n                    filename = Path(path_str).name\n                    dest_path = f\"{dest_dir_str}/{filename}\" if dest_dir_str else filename\n                    logger.debug(\n                        \"UPLOAD CROSS-ENV: %s (%d bytes) from %s -&gt; %s\",\n                        path_str,\n                        len(content),\n                        type(source_env).__name__,\n                        dest_path,\n                    )\n                    await self.write_file_bytes(dest_path, content)\n                    result.uploaded.append(UploadedFile(Path(path_str), dest_path, len(content)))\n            else:\n                # Local filesystem upload - must be handled by subclass\n                # This is a fallback that reads from local fs and writes to env\n                local_path = Path(path)\n                if local_path.is_dir():\n                    # Handle directory recursively\n                    for file_path in local_path.rglob(\"*\"):\n                        if file_path.is_file():\n                            rel_path = file_path.relative_to(local_path)\n                            dest_path = f\"{dest_dir_str}/{rel_path}\" if dest_dir_str else str(rel_path)\n                            content = file_path.read_bytes()\n                            logger.debug(\n                                \"UPLOAD FROM LOCAL: %s (%d bytes) -&gt; %s\",\n                                file_path,\n                                len(content),\n                                dest_path,\n                            )\n                            await self.write_file_bytes(dest_path, content)\n                            result.uploaded.append(UploadedFile(file_path, dest_path, len(content)))\n                else:\n                    filename = local_path.name\n                    dest_path = f\"{dest_dir_str}/{filename}\" if dest_dir_str else filename\n                    content = local_path.read_bytes()\n                    logger.debug(\n                        \"UPLOAD FROM LOCAL: %s (%d bytes) -&gt; %s\",\n                        local_path,\n                        len(content),\n                        dest_path,\n                    )\n                    await self.write_file_bytes(dest_path, content)\n                    result.uploaded.append(UploadedFile(local_path, dest_path, len(content)))\n        except Exception as e:\n            logger.debug(\"UPLOAD FAILED: %s -&gt; %s: %s\", path_str, dest_dir_str, e)\n            result.failed[path_str] = str(e)\n\n    return result\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.get_code_exec_tool","title":"get_code_exec_tool","text":"<pre><code>get_code_exec_tool(\n    *,\n    name: str = \"code_exec\",\n    description: str | None = None,\n) -&gt; Tool[CodeExecutionParams, ToolUseCountMetadata]\n</code></pre> <p>Create a code execution tool for this environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name</p> <code>'code_exec'</code> <code>description</code> <code>str | None</code> <p>Tool description</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool[CodeExecutionParams, ToolUseCountMetadata]</code> <p>Tool[CodeExecutionParams] that executes commands in this environment</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def get_code_exec_tool(\n    self,\n    *,\n    name: str = \"code_exec\",\n    description: str | None = None,\n) -&gt; Tool[CodeExecutionParams, ToolUseCountMetadata]:\n    \"\"\"Create a code execution tool for this environment.\n\n    Args:\n        name: Tool name\n        description: Tool description\n\n    Returns:\n        Tool[CodeExecutionParams] that executes commands in this environment\n\n    \"\"\"\n    env = self\n\n    async def executor(params: CodeExecutionParams) -&gt; ToolResult[ToolUseCountMetadata]:\n        result = await env.run_command(params.cmd)\n        return format_result(result)\n\n    return Tool[CodeExecutionParams, ToolUseCountMetadata](\n        name=name,\n        description=description\n        or \"Execute a shell command in the execution environment. Returns exit code, stdout, and stderr as XML.\",\n        parameters=CodeExecutionParams,\n        executor=executor,  # ty: ignore[invalid-argument-type]\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.get_view_image_tool","title":"get_view_image_tool","text":"<pre><code>get_view_image_tool(\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n) -&gt; Tool[ViewImageParams, ToolUseCountMetadata]\n</code></pre> <p>Create a view_image tool for this environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name</p> <code>'view_image'</code> <code>description</code> <code>str | None</code> <p>Tool description</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool[ViewImageParams, ToolUseCountMetadata]</code> <p>Tool[ViewImageParams, ToolUseCountMetadata] that views images in this environment</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def get_view_image_tool(\n    self,\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n) -&gt; Tool[ViewImageParams, ToolUseCountMetadata]:\n    \"\"\"Create a view_image tool for this environment.\n\n    Args:\n        name: Tool name\n        description: Tool description\n\n    Returns:\n        Tool[ViewImageParams, ToolUseCountMetadata] that views images in this environment\n\n    \"\"\"\n    env = self\n\n    async def executor(params: ViewImageParams) -&gt; ToolResult[ToolUseCountMetadata]:\n        try:\n            image = await env.view_image(params.path)\n            return ToolResult(\n                content=[\"Viewing image at path: \" + params.path, image],\n                metadata=ToolUseCountMetadata(),\n            )\n        except FileNotFoundError:\n            return ToolResult(\n                content=f\"Image `{params.path}` not found.\",\n                success=False,\n                metadata=ToolUseCountMetadata(),\n            )\n        except ValueError as e:\n            return ToolResult(\n                content=str(e),\n                success=False,\n                metadata=ToolUseCountMetadata(),\n            )\n\n    return Tool[ViewImageParams, ToolUseCountMetadata](\n        name=name,\n        description=description or \"View an image file from the execution environment's filesystem.\",\n        parameters=ViewImageParams,\n        executor=executor,  # ty: ignore[invalid-argument-type]\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.CodeExecToolProvider.view_image","title":"view_image  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>view_image(path: str) -&gt; ImageContentBlock\n</code></pre> <p>Read and return an image file from the execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to image file in the execution environment (relative or absolute).</p> required <p>Returns:</p> Type Description <code>ImageContentBlock</code> <p>ImageContentBlock containing the image data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> <code>FileNotFoundError</code> <p>If file does not exist.</p> <code>ValueError</code> <p>If path is outside the execution environment, is a directory,         or the file is not a valid image.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def view_image(self, path: str) -&gt; ImageContentBlock:\n    \"\"\"Read and return an image file from the execution environment.\n\n    Args:\n        path: Path to image file in the execution environment (relative or absolute).\n\n    Returns:\n        ImageContentBlock containing the image data.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n        FileNotFoundError: If file does not exist.\n        ValueError: If path is outside the execution environment, is a directory,\n                    or the file is not a valid image.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider","title":"LocalCodeExecToolProvider","text":"<pre><code>LocalCodeExecToolProvider(\n    *,\n    allowed_commands: list[str] | None = None,\n    temp_base_dir: Path | str | None = None,\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>CodeExecToolProvider</code></p> <p>Local code execution tool provider using an isolated temp directory.</p> <p>Commands are executed with the temp directory as the working directory. An optional allowlist can restrict which commands are permitted.</p> Usage with Agent <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>client = ChatCompletionsClient(model=\"gpt-5\") agent = Agent(     client=client,     name=\"assistant\",     tools=[LocalCodeExecToolProvider(), CALCULATOR_TOOL], )</p> <p>async with agent.session(output_dir=\"./output\") as session:     await session.run(\"Run some Python code\")</p> Standalone usage <p>provider = LocalCodeExecToolProvider()</p> <p>async with provider as tool:     # tool is a Tool instance for code execution     result = await provider.run_command(\"python script.py\")     await provider.save_output_files([\"output.txt\"], \"/path/to/output\")</p> <p>Initialize LocalCodeExecToolProvider configuration.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_commands</code> <code>list[str] | None</code> <p>Optional list of regex patterns. If provided, only              commands matching at least one pattern are allowed.              If None, all commands are allowed.</p> <code>None</code> <code>temp_base_dir</code> <code>Path | str | None</code> <p>Optional base directory for creating the execution environment           temp directory. If None, uses the system default temp directory.</p> <code>None</code> <code>description</code> <code>str | None</code> <p>Optional description of the tool. If None, uses the default description.</p> <code>None</code> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>def __init__(\n    self,\n    *,\n    allowed_commands: list[str] | None = None,\n    temp_base_dir: Path | str | None = None,\n    description: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize LocalCodeExecToolProvider configuration.\n\n    Args:\n        allowed_commands: Optional list of regex patterns. If provided, only\n                         commands matching at least one pattern are allowed.\n                         If None, all commands are allowed.\n        temp_base_dir: Optional base directory for creating the execution environment\n                      temp directory. If None, uses the system default temp directory.\n        description: Optional description of the tool. If None, uses the default description.\n\n    \"\"\"\n    super().__init__(allowed_commands=allowed_commands)\n    self._temp_dir: Path | None = None\n    self._temp_base_dir: Path | None = Path(temp_base_dir) if temp_base_dir else None\n    self._description = (\n        description\n        or \"Execute a shell command in the execution environment. Returns exit code, stdout, and stderr as XML. Use `uv` to manage packages.\"\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.temp_dir","title":"temp_dir  <code>property</code>","text":"<pre><code>temp_dir: Path | None\n</code></pre> <p>Return the temp directory path, or None if not started.</p>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool[\n    CodeExecutionParams, ToolUseCountMetadata\n]\n</code></pre> <p>Create temp directory and return the code_exec tool.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def __aenter__(self) -&gt; \"Tool[CodeExecutionParams, ToolUseCountMetadata]\":\n    \"\"\"Create temp directory and return the code_exec tool.\"\"\"\n    if self._temp_base_dir:\n        self._temp_base_dir.mkdir(parents=True, exist_ok=True)\n    self._temp_dir = Path(tempfile.mkdtemp(prefix=\"local_exec_env_\", dir=self._temp_base_dir))\n    logger.debug(\"Created local execution environment temp directory: %s\", self._temp_dir)\n    return self.get_code_exec_tool(description=self._description)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Cleanup the local execution environment.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Cleanup the local execution environment.\"\"\"\n    if self._temp_dir and self._temp_dir.exists():\n        try:\n            shutil.rmtree(self._temp_dir)\n        except Exception as exc:\n            logger.warning(\"Failed to cleanup temp directory %s: %s\", self._temp_dir, exc)\n    self._temp_dir = None\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.read_file_bytes","title":"read_file_bytes  <code>async</code>","text":"<pre><code>read_file_bytes(path: str) -&gt; bytes\n</code></pre> <p>Read file content as bytes from the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>File contents as bytes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> <code>FileNotFoundError</code> <p>If file does not exist.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def read_file_bytes(self, path: str) -&gt; bytes:\n    \"\"\"Read file content as bytes from the temp directory.\n\n    Args:\n        path: File path (relative or absolute within the temp dir).\n\n    Returns:\n        File contents as bytes.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n        FileNotFoundError: If file does not exist.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    if not resolved.exists():\n        raise FileNotFoundError(f\"File not found: {path}\")\n    return resolved.read_bytes()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.write_file_bytes","title":"write_file_bytes  <code>async</code>","text":"<pre><code>write_file_bytes(path: str, content: bytes) -&gt; None\n</code></pre> <p>Write bytes to a file in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Destination path (relative or absolute within the temp dir).</p> required <code>content</code> <code>bytes</code> <p>File contents to write.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def write_file_bytes(self, path: str, content: bytes) -&gt; None:\n    \"\"\"Write bytes to a file in the temp directory.\n\n    Args:\n        path: Destination path (relative or absolute within the temp dir).\n        content: File contents to write.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    resolved.parent.mkdir(parents=True, exist_ok=True)\n    resolved.write_bytes(content)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.file_exists","title":"file_exists  <code>async</code>","text":"<pre><code>file_exists(path: str) -&gt; bool\n</code></pre> <p>Check if a file exists in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def file_exists(self, path: str) -&gt; bool:\n    \"\"\"Check if a file exists in the temp directory.\n\n    Args:\n        path: File path (relative or absolute within the temp dir).\n\n    Returns:\n        True if the file exists, False otherwise.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    return resolved.exists() and resolved.is_file()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.is_directory","title":"is_directory  <code>async</code>","text":"<pre><code>is_directory(path: str) -&gt; bool\n</code></pre> <p>Check if a path is a directory in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the path exists and is a directory, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def is_directory(self, path: str) -&gt; bool:\n    \"\"\"Check if a path is a directory in the temp directory.\n\n    Args:\n        path: Path (relative or absolute within the temp dir).\n\n    Returns:\n        True if the path exists and is a directory, False otherwise.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    return resolved.exists() and resolved.is_dir()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.list_files","title":"list_files  <code>async</code>","text":"<pre><code>list_files(path: str) -&gt; list[str]\n</code></pre> <p>List all files recursively in a directory within the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths (relative to the given path) for all files in the directory.</p> <code>list[str]</code> <p>Returns an empty list if the path is a file or doesn't exist.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def list_files(self, path: str) -&gt; list[str]:\n    \"\"\"List all files recursively in a directory within the temp directory.\n\n    Args:\n        path: Directory path (relative or absolute within the temp dir).\n\n    Returns:\n        List of file paths (relative to the given path) for all files in the directory.\n        Returns an empty list if the path is a file or doesn't exist.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    if not resolved.exists() or not resolved.is_dir():\n        return []\n\n    files = []\n    for file_path in resolved.rglob(\"*\"):\n        if file_path.is_file():\n            rel_path = file_path.relative_to(resolved)\n            files.append(str(rel_path))\n    return files\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.run_command","title":"run_command  <code>async</code>","text":"<pre><code>run_command(\n    cmd: str, *, timeout: int = SHELL_TIMEOUT\n) -&gt; CommandResult\n</code></pre> <p>Execute command in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Shell command to execute (bash syntax).</p> required <code>timeout</code> <code>int</code> <p>Maximum time in seconds to wait for command completion.</p> <code>SHELL_TIMEOUT</code> <p>Returns:</p> Type Description <code>CommandResult</code> <p>CommandResult with exit_code, stdout, stderr, and optional error info.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def run_command(self, cmd: str, *, timeout: int = SHELL_TIMEOUT) -&gt; CommandResult:\n    \"\"\"Execute command in the temp directory.\n\n    Args:\n        cmd: Shell command to execute (bash syntax).\n        timeout: Maximum time in seconds to wait for command completion.\n\n    Returns:\n        CommandResult with exit_code, stdout, stderr, and optional error info.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # Check allowlist\n    if not self._check_allowed(cmd):\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=f\"Command not allowed: '{cmd}' does not match any allowed patterns\",\n            error_kind=\"command_not_allowed\",\n            advice=\"Only commands matching the allowlist patterns are permitted.\",\n        )\n\n    # Check for absolute paths (local environment is not sandboxed)\n    absolute_path_error = self._check_absolute_paths(cmd)\n    if absolute_path_error:\n        return absolute_path_error\n\n    process = None\n    try:\n        with anyio.fail_after(timeout):\n            # Use shell=True by wrapping in a shell command\n            process = await anyio.open_process(\n                [\"bash\", \"-c\", cmd],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                cwd=self._temp_dir,\n            )\n\n            # Read all output from streams concurrently\n            stdout_chunks: list[bytes] = []\n            stderr_chunks: list[bytes] = []\n\n            async def read_stdout() -&gt; None:\n                if process.stdout:\n                    stdout_chunks.extend([chunk async for chunk in process.stdout])\n\n            async def read_stderr() -&gt; None:\n                if process.stderr:\n                    stderr_chunks.extend([chunk async for chunk in process.stderr])\n\n            async with anyio.create_task_group() as tg:\n                tg.start_soon(read_stdout)\n                tg.start_soon(read_stderr)\n\n            await process.wait()\n\n            return CommandResult(\n                exit_code=process.returncode or 0,\n                stdout=b\"\".join(stdout_chunks).decode(\"utf-8\", errors=\"replace\"),\n                stderr=b\"\".join(stderr_chunks).decode(\"utf-8\", errors=\"replace\"),\n            )\n\n    except TimeoutError:\n        if process:\n            process.kill()\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=f\"Command timed out after {timeout} seconds\",\n            error_kind=\"timeout\",\n        )\n    except Exception as exc:\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=str(exc),\n            error_kind=\"execution_error\",\n        )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.save_output_files","title":"save_output_files  <code>async</code>","text":"<pre><code>save_output_files(\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: CodeExecToolProvider | None = None,\n) -&gt; SaveOutputFilesResult\n</code></pre> <p>Move files from the temp directory to a destination.</p> <p>When dest_env is None (local filesystem), files are MOVED (not copied) - originals are deleted from the execution environment. Existing files in output_dir are silently overwritten.</p> <p>When dest_env is provided (cross-environment transfer), files are copied using the base class implementation via read/write primitives.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths in the execution environment (relative or absolute).    Relative paths are resolved against the execution environment temp directory.</p> required <code>output_dir</code> <code>Path | str</code> <p>Directory path to save files to.</p> required <code>dest_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, output_dir is interpreted as a path within dest_env       (cross-environment transfer). If None, output_dir is a local       filesystem path.</p> <code>None</code> <p>Returns:</p> Type Description <code>SaveOutputFilesResult</code> <p>SaveOutputFilesResult containing lists of saved files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def save_output_files(\n    self,\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: \"CodeExecToolProvider | None\" = None,\n) -&gt; SaveOutputFilesResult:\n    \"\"\"Move files from the temp directory to a destination.\n\n    When dest_env is None (local filesystem), files are MOVED (not copied) -\n    originals are deleted from the execution environment.\n    Existing files in output_dir are silently overwritten.\n\n    When dest_env is provided (cross-environment transfer), files are copied\n    using the base class implementation via read/write primitives.\n\n    Args:\n        paths: List of file paths in the execution environment (relative or absolute).\n               Relative paths are resolved against the execution environment temp directory.\n        output_dir: Directory path to save files to.\n        dest_env: If provided, output_dir is interpreted as a path within dest_env\n                  (cross-environment transfer). If None, output_dir is a local\n                  filesystem path.\n\n    Returns:\n        SaveOutputFilesResult containing lists of saved files and any failures.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # If dest_env is provided, use the base class implementation (cross-env transfer)\n    if dest_env is not None:\n        return await super().save_output_files(paths, output_dir, dest_env)\n\n    # Local filesystem - use optimized move operation\n    output_dir_path = Path(output_dir)\n    output_dir_path.mkdir(parents=True, exist_ok=True)\n\n    result = SaveOutputFilesResult()\n\n    for source_path_str in paths:\n        try:\n            source_path = Path(source_path_str)\n            if not source_path.is_absolute():\n                source_path = self._temp_dir / source_path\n\n            # Security: ensure path is within temp directory\n            try:\n                source_path.resolve().relative_to(self._temp_dir.resolve())\n            except ValueError:\n                result.failed[source_path_str] = \"Path is outside execution environment directory\"\n                logger.warning(\"Attempted to access path outside execution environment: %s\", source_path_str)\n                continue\n\n            if not source_path.exists():\n                result.failed[source_path_str] = \"File does not exist\"\n                logger.warning(\"Execution environment file does not exist: %s\", source_path_str)\n                continue\n\n            if not source_path.is_file():\n                result.failed[source_path_str] = \"Path is not a file\"\n                logger.warning(\"Execution environment path is not a file: %s\", source_path_str)\n                continue\n\n            file_size = source_path.stat().st_size\n            dest_path = output_dir_path / source_path.name\n\n            # Move file (overwrites if exists)\n            shutil.move(str(source_path), str(dest_path))\n            logger.debug(\"Moved file: %s -&gt; %s\", source_path, dest_path)\n\n            result.saved.append(\n                SavedFile(\n                    source_path=source_path_str,\n                    output_path=dest_path,\n                    size=file_size,\n                ),\n            )\n\n        except Exception as exc:\n            result.failed[source_path_str] = str(exc)\n            logger.exception(\"Failed to move file: %s\", source_path_str)\n\n    return result\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.upload_files","title":"upload_files  <code>async</code>","text":"<pre><code>upload_files(\n    *paths: Path | str,\n    source_env: CodeExecToolProvider | None = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult\n</code></pre> <p>Upload files to the execution environment.</p> <p>When source_env is None (local filesystem), files are COPIED (not moved) - originals remain on the local filesystem. Directories are uploaded recursively, preserving their structure.</p> <p>When source_env is provided (cross-environment transfer), files are copied using the base class implementation via read/write primitives.</p> <p>Parameters:</p> Name Type Description Default <code>*paths</code> <code>Path | str</code> <p>File or directory paths to upload. If source_env is None, these     are local filesystem paths. If source_env is provided, these are     paths within source_env.</p> <code>()</code> <code>source_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, paths are within source_env. If None, paths are         local filesystem paths.</p> <code>None</code> <code>dest_dir</code> <code>str | None</code> <p>Destination subdirectory within the temp directory.       If None, files are placed directly in the temp directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>UploadFilesResult</code> <p>UploadFilesResult containing lists of uploaded files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def upload_files(\n    self,\n    *paths: Path | str,\n    source_env: \"CodeExecToolProvider | None\" = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult:\n    \"\"\"Upload files to the execution environment.\n\n    When source_env is None (local filesystem), files are COPIED (not moved) -\n    originals remain on the local filesystem.\n    Directories are uploaded recursively, preserving their structure.\n\n    When source_env is provided (cross-environment transfer), files are copied\n    using the base class implementation via read/write primitives.\n\n    Args:\n        *paths: File or directory paths to upload. If source_env is None, these\n                are local filesystem paths. If source_env is provided, these are\n                paths within source_env.\n        source_env: If provided, paths are within source_env. If None, paths are\n                    local filesystem paths.\n        dest_dir: Destination subdirectory within the temp directory.\n                  If None, files are placed directly in the temp directory.\n\n    Returns:\n        UploadFilesResult containing lists of uploaded files and any failures.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # If source_env is provided, use the base class implementation (cross-env transfer)\n    if source_env is not None:\n        return await super().upload_files(*paths, source_env=source_env, dest_dir=dest_dir)\n\n    # Local filesystem - use optimized copy operation\n    dest_base = self._temp_dir / dest_dir if dest_dir else self._temp_dir\n    dest_base.mkdir(parents=True, exist_ok=True)\n\n    result = UploadFilesResult()\n\n    for source in paths:\n        source = Path(source).resolve()\n\n        if not source.exists():\n            result.failed[str(source)] = \"File or directory does not exist\"\n            logger.warning(\"Upload source does not exist: %s\", source)\n            continue\n\n        try:\n            if source.is_file():\n                dest = dest_base / source.name\n                shutil.copy2(source, dest)\n                result.uploaded.append(\n                    UploadedFile(\n                        source_path=source,\n                        dest_path=str(dest.relative_to(self._temp_dir)),\n                        size=source.stat().st_size,\n                    ),\n                )\n                logger.debug(\"Uploaded file: %s -&gt; %s\", source, dest)\n\n            elif source.is_dir():\n                # If dest_dir was explicitly provided, copy contents directly to dest_base\n                # Otherwise, create a subdirectory with the source's name\n                if dest_dir:\n                    dest = dest_base\n                    # Copy contents of source directory into dest_base\n                    for item in source.iterdir():\n                        item_dest = dest / item.name\n                        if item.is_file():\n                            shutil.copy2(item, item_dest)\n                        else:\n                            shutil.copytree(item, item_dest, dirs_exist_ok=True)\n                else:\n                    dest = dest_base / source.name\n                    shutil.copytree(source, dest, dirs_exist_ok=True)\n                # Track all individual files uploaded\n                for file_path in source.rglob(\"*\"):\n                    if file_path.is_file():\n                        relative = file_path.relative_to(source)\n                        dest_file = dest / relative\n                        result.uploaded.append(\n                            UploadedFile(\n                                source_path=file_path,\n                                dest_path=str(dest_file.relative_to(self._temp_dir)),\n                                size=file_path.stat().st_size,\n                            ),\n                        )\n                logger.debug(\"Uploaded directory: %s -&gt; %s\", source, dest)\n\n        except Exception as exc:\n            result.failed[str(source)] = str(exc)\n            logger.exception(\"Failed to upload: %s\", source)\n\n    return result\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.LocalCodeExecToolProvider.view_image","title":"view_image  <code>async</code>","text":"<pre><code>view_image(path: str) -&gt; ImageContentBlock\n</code></pre> <p>Read and return an image file from the local execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to image file (relative to temp directory, or absolute within it).</p> required <p>Returns:</p> Type Description <code>ImageContentBlock</code> <p>ImageContentBlock containing the image data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> <code>FileNotFoundError</code> <p>If file does not exist.</p> <code>ValueError</code> <p>If path is outside temp directory, is a directory, or not a valid image.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def view_image(self, path: str) -&gt; ImageContentBlock:\n    \"\"\"Read and return an image file from the local execution environment.\n\n    Args:\n        path: Path to image file (relative to temp directory, or absolute within it).\n\n    Returns:\n        ImageContentBlock containing the image data.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n        FileNotFoundError: If file does not exist.\n        ValueError: If path is outside temp directory, is a directory, or not a valid image.\n\n    \"\"\"\n    file_bytes = await self.read_file_bytes(path)\n    return ImageContentBlock(data=file_bytes)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger","title":"AgentLogger","text":"<pre><code>AgentLogger(\n    *, show_spinner: bool = True, level: int = INFO\n)\n</code></pre> <p>               Bases: <code>AgentLoggerBase</code></p> <p>Rich console logger for agent workflows.</p> <p>Implements AgentLoggerBase with rich formatting, spinners, and visual hierarchy. Each agent (including sub-agents) should have its own logger instance.</p> Usage <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>Initialize the agent logger.</p> <p>Parameters:</p> Name Type Description Default <code>show_spinner</code> <code>bool</code> <p>Whether to show a spinner while agent runs (only for depth=0)</p> <code>True</code> <code>level</code> <code>int</code> <p>Logging level (default: INFO)</p> <code>INFO</code> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __init__(\n    self,\n    *,\n    show_spinner: bool = True,\n    level: int = logging.INFO,\n) -&gt; None:\n    \"\"\"Initialize the agent logger.\n\n    Args:\n        show_spinner: Whether to show a spinner while agent runs (only for depth=0)\n        level: Logging level (default: INFO)\n    \"\"\"\n    # Properties set by Agent before __enter__\n    self.name: str = \"agent\"\n    self.model: str | None = None\n    self.max_turns: int | None = None\n    self.depth: int = 0\n\n    # State set by Agent before __exit__\n    self.finish_params: BaseModel | None = None\n    self.run_metadata: dict[str, list[Any]] | None = None\n    self.output_dir: str | None = None\n\n    # Configuration\n    self._show_spinner = show_spinner\n    self._level = level\n\n    # Spinner state (only used when depth == 0 and show_spinner is True)\n    self._current_step = 0\n    self._tool_calls = 0\n    self._input_tokens = 0\n    self._output_tokens = 0\n    self._live: Live | None = None\n\n    # Configure rich logging on first logger creation\n    self._configure_logging()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--agent-creates-logger-internally-by-default","title":"Agent creates logger internally by default","text":"<p>client = ChatCompletionsClient(model=\"gpt-4\") agent = Agent(client=client, name=\"assistant\")</p>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--or-pass-a-pre-configured-logger","title":"Or pass a pre-configured logger","text":"<p>logger = AgentLogger(show_spinner=False) agent = Agent(client=client, name=\"assistant\", logger=logger)</p>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--agent-sets-these-properties-before-calling-enter","title":"Agent sets these properties before calling enter:","text":""},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--loggername-loggermodel-loggermax_turns-loggerdepth","title":"logger.name, logger.model, logger.max_turns, logger.depth","text":""},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--agent-sets-these-before-calling-exit","title":"Agent sets these before calling exit:","text":""},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger--loggerfinish_params-loggerrun_metadata-loggeroutput_dir","title":"logger.finish_params, logger.run_metadata, logger.output_dir","text":""},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter logging context. Logs agent start and starts spinner if depth=0.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter logging context. Logs agent start and starts spinner if depth=0.\"\"\"\n    # Log agent start (rule + system prompt display)\n    indent_spaces = self.depth * SUBAGENT_INDENT_SPACES\n\n    # Build title with optional model info\n    model_str = f\" ({self.model})\" if self.model else \"\"\n    if self.depth == 0:\n        title = f\"\u25b6 {self.name}{model_str}\"\n        console.rule(f\"[bold cyan]{title}[/]\", style=\"cyan\")\n    else:\n        title = f\"\u25b6 {self.name}: Level {self.depth}{model_str}\"\n        rule = Rule(f\"[bold cyan]{title}[/]\", style=\"cyan\")\n        self._print_indented(rule, indent_spaces)\n    console.print()\n\n    # Start spinner only for top-level agent\n    if self.depth == 0 and self._show_spinner:\n        self._live = Live(self._make_spinner(), console=console, refresh_per_second=10)\n        self._live.start()\n\n    return self\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.__exit__","title":"__exit__","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Exit logging context. Stops spinner and logs completion stats.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Exit logging context. Stops spinner and logs completion stats.\"\"\"\n    # Stop spinner first\n    if self._live:\n        self._live.stop()\n        self._live = None\n\n    error = str(exc_val) if exc_type is not None else None\n    self._log_finish(error=error)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.on_step","title":"on_step","text":"<pre><code>on_step(\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None\n</code></pre> <p>Report step progress and stats during agent execution.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def on_step(\n    self,\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None:\n    \"\"\"Report step progress and stats during agent execution.\"\"\"\n    self._current_step = step\n    self._tool_calls = tool_calls\n    self._input_tokens = input_tokens\n    self._output_tokens = output_tokens\n    if self._live:\n        self._live.update(self._make_spinner())\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.pause_live","title":"pause_live","text":"<pre><code>pause_live() -&gt; None\n</code></pre> <p>Pause the live spinner display.</p> <p>Call this before prompting for user input to prevent the spinner from interfering with the input prompt.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def pause_live(self) -&gt; None:\n    \"\"\"Pause the live spinner display.\n\n    Call this before prompting for user input to prevent the spinner\n    from interfering with the input prompt.\n    \"\"\"\n    if self._live is not None:\n        self._live.stop()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.resume_live","title":"resume_live","text":"<pre><code>resume_live() -&gt; None\n</code></pre> <p>Resume the live spinner display.</p> <p>Call this after user input is complete to restart the spinner.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def resume_live(self) -&gt; None:\n    \"\"\"Resume the live spinner display.\n\n    Call this after user input is complete to restart the spinner.\n    \"\"\"\n    if self._live is not None:\n        self._live.start()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.set_level","title":"set_level","text":"<pre><code>set_level(level: int) -&gt; None\n</code></pre> <p>Set the logging level.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def set_level(self, level: int) -&gt; None:\n    \"\"\"Set the logging level.\"\"\"\n    self._level = level\n    # Also update root logger level\n    logging.getLogger().setLevel(level)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.is_enabled_for","title":"is_enabled_for","text":"<pre><code>is_enabled_for(level: int) -&gt; bool\n</code></pre> <p>Check if a given log level is enabled.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def is_enabled_for(self, level: int) -&gt; bool:\n    \"\"\"Check if a given log level is enabled.\"\"\"\n    return level &gt;= self._level\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.debug","title":"debug","text":"<pre><code>debug(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a debug message (dim style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def debug(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a debug message (dim style).\"\"\"\n    if self._level &lt;= logging.DEBUG:\n        formatted = message % args if args else message\n        console.print(f\"[dim]{formatted}[/]\")\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.info","title":"info","text":"<pre><code>info(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an info message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def info(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an info message.\"\"\"\n    if self._level &lt;= logging.INFO:\n        formatted = message % args if args else message\n        console.print(formatted)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.warning","title":"warning","text":"<pre><code>warning(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a warning message (yellow style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def warning(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a warning message (yellow style).\"\"\"\n    if self._level &lt;= logging.WARNING:\n        formatted = message % args if args else message\n        console.print(f\"[yellow]\u26a0 {formatted}[/]\")\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.error","title":"error","text":"<pre><code>error(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message (red style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def error(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message (red style).\"\"\"\n    if self._level &lt;= logging.ERROR:\n        formatted = message % args if args else message\n        console.print(f\"[red]\u2717 {formatted}[/]\")\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.critical","title":"critical","text":"<pre><code>critical(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a critical message (bold red style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def critical(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a critical message (bold red style).\"\"\"\n    if self._level &lt;= logging.CRITICAL:\n        formatted = message % args if args else message\n        console.print(f\"[bold red]\u2717 CRITICAL: {formatted}[/]\")\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.exception","title":"exception","text":"<pre><code>exception(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message with exception traceback (red style with traceback).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def exception(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message with exception traceback (red style with traceback).\"\"\"\n    if self._level &lt;= logging.ERROR:\n        formatted = message % args if args else message\n        console.print(f\"[red]\u2717 {formatted}[/]\")\n        console.print_exception()\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.assistant_message","title":"assistant_message","text":"<pre><code>assistant_message(\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None\n</code></pre> <p>Log an assistant message with content and tool calls in a panel.</p> <p>Parameters:</p> Name Type Description Default <code>turn</code> <code>int</code> <p>Current turn number (1-indexed)</p> required <code>max_turns</code> <code>int</code> <p>Maximum number of turns</p> required <code>assistant_message</code> <code>AssistantMessage</code> <p>The assistant's response message</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def assistant_message(\n    self,\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None:\n    \"\"\"Log an assistant message with content and tool calls in a panel.\n\n    Args:\n        turn: Current turn number (1-indexed)\n        max_turns: Maximum number of turns\n        assistant_message: The assistant's response message\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Build panel content\n    content = Text()\n\n    # Add assistant content if present\n    if assistant_message.content:\n        text = assistant_message.content\n        if isinstance(text, list):\n            text = \"\\n\".join(str(block) for block in text)\n        # Truncate long content\n        if len(text) &gt; 500:\n            text = text[:500] + \"...\"\n        content.append(text, style=\"white\")\n\n    # Add tool calls if present\n    if assistant_message.tool_calls:\n        if assistant_message.content:\n            content.append(\"\\n\\n\")\n        content.append(\"Tool Calls:\\n\", style=\"bold magenta\")\n        for tc in assistant_message.tool_calls:\n            content.append(f\"  \ud83d\udd27 {tc.name}\", style=\"magenta\")\n            if tc.arguments and tc.arguments.strip():\n                args_parsed = json.loads(tc.arguments)\n                args_formatted = json.dumps(args_parsed, indent=2, ensure_ascii=False)\n                args_preview = args_formatted[:1000] + \"...\" if len(args_formatted) &gt; 1000 else args_formatted\n                content.append(args_preview, style=\"dim\")\n\n    # Create and print panel with agent name in title\n    title = f\"[bold]AssistantMessage[/bold] \u2502 {self.name} \u2502 Turn {turn}/{max_turns}\"\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"yellow\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.user_message","title":"user_message","text":"<pre><code>user_message(user_message: UserMessage) -&gt; None\n</code></pre> <p>Log a user message in a panel.</p> <p>Parameters:</p> Name Type Description Default <code>user_message</code> <code>UserMessage</code> <p>The user's message</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def user_message(self, user_message: UserMessage) -&gt; None:\n    \"\"\"Log a user message in a panel.\n\n    Args:\n        user_message: The user's message\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Build panel content\n    content = Text()\n\n    # Add user content\n    if user_message.content:\n        text = user_message.content\n        if isinstance(text, list):\n            text = \"\\n\".join(str(block) for block in text)\n        # Truncate long content\n        if len(text) &gt; 500:\n            text = text[:500] + \"...\"\n        content.append(text, style=\"white\")\n\n    # Create and print panel with agent name in title\n    title = f\"[bold]UserMessage[/bold] \u2502 {self.name}\"\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"blue\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.task_message","title":"task_message","text":"<pre><code>task_message(task: str | list[Any]) -&gt; None\n</code></pre> <p>Log the initial task/prompt at the start of a run.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def task_message(self, task: str | list[Any]) -&gt; None:\n    \"\"\"Log the initial task/prompt at the start of a run.\"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Convert list content to string\n    if isinstance(task, list):\n        task = \"\\n\".join(str(block) for block in task)\n\n    # Clean up whitespace from multi-line strings\n    # Normalize each line by stripping leading/trailing whitespace and rejoining\n    lines = [line.strip() for line in task.split(\"\\n\")]\n    task = \" \".join(line for line in lines if line)\n\n    # Use \"Sub Agent\" prefix for nested agents\n    prefix = \"Sub Agent\" if self.depth &gt; 0 else \"Agent\"\n\n    if self.depth &gt; 0:\n        indent = \" \" * (self.depth * SUBAGENT_INDENT_SPACES)\n        console.print(f\"{indent}[bold]{prefix} Task:[/bold]\")\n        console.print()\n        for line in task.split(\"\\n\"):\n            console.print(f\"{indent}{line}\")\n    else:\n        console.print(f\"[bold]{prefix} Task:[/bold]\")\n        console.print()\n        console.print(task)\n\n    console.print()  # Add gap after task section\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.warnings_message","title":"warnings_message","text":"<pre><code>warnings_message(warnings: list[str]) -&gt; None\n</code></pre> <p>Display warnings at run start as simple text.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def warnings_message(self, warnings: list[str]) -&gt; None:\n    \"\"\"Display warnings at run start as simple text.\"\"\"\n    if self._level &gt; logging.INFO or not warnings:\n        return\n\n    console.print(\"[bold orange1]Warnings[/bold orange1]\")\n    console.print()\n    for warning in warnings:\n        console.print(f\"[orange1]\u26a0 {warning}[/orange1]\")\n        console.print()  # Add gap between warnings\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.tool_result","title":"tool_result","text":"<pre><code>tool_result(tool_message: ToolMessage) -&gt; None\n</code></pre> <p>Log a single tool execution result in a panel with XML syntax highlighting.</p> <p>Parameters:</p> Name Type Description Default <code>tool_message</code> <code>ToolMessage</code> <p>The tool execution result</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def tool_result(self, tool_message: ToolMessage) -&gt; None:\n    \"\"\"Log a single tool execution result in a panel with XML syntax highlighting.\n\n    Args:\n        tool_message: The tool execution result\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    tool_name = tool_message.name or \"unknown\"\n\n    # Get result content\n    result_text = tool_message.content\n    if isinstance(result_text, list):\n        result_text = \"\\n\".join(str(block) for block in result_text)\n\n    # Unescape HTML entities (e.g., &amp;lt; -&gt; &lt;, &amp;gt; -&gt; &gt;, &amp;amp; -&gt; &amp;)\n    result_text = html.unescape(result_text)\n\n    # Truncate long results (keeps start and end, removes middle)\n    result_text = truncate_msg(result_text, 1000)\n\n    # Format as XML with syntax highlighting\n    content = Syntax(result_text, \"xml\", theme=\"monokai\", word_wrap=True)\n\n    # Status indicator in title with agent name\n    status = \"\u2713\" if tool_message.args_was_valid else \"\u2717\"\n    status_style = \"green\" if tool_message.args_was_valid else \"red\"\n    title = f\"[{status_style}]{status}[/{status_style}] [bold]ToolResult[/bold] \u2502 {self.name} \u2502 [green]{tool_name}[/green]\"\n\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"green\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.context_summarization_start","title":"context_summarization_start","text":"<pre><code>context_summarization_start(\n    pct_used: float, cutoff: float\n) -&gt; None\n</code></pre> <p>Log context window summarization starting in an orange panel.</p> <p>Parameters:</p> Name Type Description Default <code>pct_used</code> <code>float</code> <p>Percentage of context window currently used (0.0-1.0)</p> required <code>cutoff</code> <code>float</code> <p>The threshold that triggered summarization (0.0-1.0)</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def context_summarization_start(self, pct_used: float, cutoff: float) -&gt; None:\n    \"\"\"Log context window summarization starting in an orange panel.\n\n    Args:\n        pct_used: Percentage of context window currently used (0.0-1.0)\n        cutoff: The threshold that triggered summarization (0.0-1.0)\n    \"\"\"\n    # Build panel content\n    content = Text()\n    content.append(\"Context window limit reached\\n\\n\", style=\"bold\")\n    content.append(\"Used: \", style=\"dim\")\n    content.append(f\"{pct_used:.1%}\", style=\"bold orange1\")\n    content.append(\"  \u2502  \", style=\"dim\")\n    content.append(\"Threshold: \", style=\"dim\")\n    content.append(f\"{cutoff:.1%}\", style=\"bold\")\n    content.append(\"\\n\\n\", style=\"dim\")\n    content.append(\"Summarizing conversation history...\", style=\"italic\")\n\n    panel = Panel(\n        content,\n        title=\"[bold orange1]\ud83d\udcdd Context Summarization[/]\",\n        title_align=\"left\",\n        border_style=\"orange1\",\n        padding=(0, 1),\n    )\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLogger.context_summarization_complete","title":"context_summarization_complete","text":"<pre><code>context_summarization_complete(\n    summary: str, bridge: str\n) -&gt; None\n</code></pre> <p>Log the completed context summarization with summary content.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>str</code> <p>The generated summary of the conversation</p> required <code>bridge</code> <code>str</code> <p>The bridge message that will be used to continue the conversation</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def context_summarization_complete(self, summary: str, bridge: str) -&gt; None:\n    \"\"\"Log the completed context summarization with summary content.\n\n    Args:\n        summary: The generated summary of the conversation\n        bridge: The bridge message that will be used to continue the conversation\n    \"\"\"\n    # Truncate long summaries for display\n    summary_display = summary\n    if len(summary_display) &gt; 800:\n        summary_display = summary_display[:800] + \"...\"\n\n    # Build panel content\n    content = Text()\n    content.append(\"Summary:\\n\", style=\"bold\")\n    content.append(summary_display, style=\"white\")\n\n    if self._level &gt; logging.INFO:\n        bridge_display = bridge\n        if len(bridge_display) &gt; 200:\n            bridge_display = bridge_display[:200] + \"...\"\n        content.append(\"\\n\\n\")\n        content.append(\"Bridge Message:\\n\", style=\"bold dim\")\n        content.append(bridge_display, style=\"dim italic\")\n\n    panel = Panel(\n        content,\n        title=\"[bold green]\u2713 Summary Generated[/]\",\n        title_align=\"left\",\n        border_style=\"green\",\n        padding=(0, 1),\n    )\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase","title":"AgentLoggerBase","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for agent loggers.</p> <p>Defines the interface that Agent uses for logging. Implement this to create custom loggers (e.g., for testing, file output, or monitoring services).</p> <p>Properties are set by Agent after construction: - name, model, max_turns, depth: Agent configuration - finish_params, run_metadata, output_dir: Set before exit for final stats</p>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.__enter__","title":"__enter__  <code>abstractmethod</code>","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter logging context. Called when agent session starts.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef __enter__(self) -&gt; Self:\n    \"\"\"Enter logging context. Called when agent session starts.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.__exit__","title":"__exit__  <code>abstractmethod</code>","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Exit logging context. Called when agent session ends.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Exit logging context. Called when agent session ends.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.on_step","title":"on_step  <code>abstractmethod</code>","text":"<pre><code>on_step(\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None\n</code></pre> <p>Report step progress and stats during agent execution.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef on_step(\n    self,\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None:\n    \"\"\"Report step progress and stats during agent execution.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.assistant_message","title":"assistant_message  <code>abstractmethod</code>","text":"<pre><code>assistant_message(\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None\n</code></pre> <p>Log an assistant message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef assistant_message(\n    self,\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None:\n    \"\"\"Log an assistant message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.user_message","title":"user_message  <code>abstractmethod</code>","text":"<pre><code>user_message(user_message: UserMessage) -&gt; None\n</code></pre> <p>Log a user message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef user_message(self, user_message: UserMessage) -&gt; None:\n    \"\"\"Log a user message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.task_message","title":"task_message  <code>abstractmethod</code>","text":"<pre><code>task_message(task: str | list[Any]) -&gt; None\n</code></pre> <p>Log the initial task/prompt at the start of a run.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef task_message(self, task: str | list[Any]) -&gt; None:\n    \"\"\"Log the initial task/prompt at the start of a run.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.tool_result","title":"tool_result  <code>abstractmethod</code>","text":"<pre><code>tool_result(tool_message: ToolMessage) -&gt; None\n</code></pre> <p>Log a tool execution result.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef tool_result(self, tool_message: ToolMessage) -&gt; None:\n    \"\"\"Log a tool execution result.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.context_summarization_start","title":"context_summarization_start  <code>abstractmethod</code>","text":"<pre><code>context_summarization_start(\n    pct_used: float, cutoff: float\n) -&gt; None\n</code></pre> <p>Log that context summarization is starting.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef context_summarization_start(self, pct_used: float, cutoff: float) -&gt; None:\n    \"\"\"Log that context summarization is starting.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.context_summarization_complete","title":"context_summarization_complete  <code>abstractmethod</code>","text":"<pre><code>context_summarization_complete(\n    summary: str, bridge: str\n) -&gt; None\n</code></pre> <p>Log completed context summarization.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef context_summarization_complete(self, summary: str, bridge: str) -&gt; None:\n    \"\"\"Log completed context summarization.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.debug","title":"debug  <code>abstractmethod</code>","text":"<pre><code>debug(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a debug message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef debug(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a debug message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.info","title":"info  <code>abstractmethod</code>","text":"<pre><code>info(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an info message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef info(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an info message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.warning","title":"warning  <code>abstractmethod</code>","text":"<pre><code>warning(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a warning message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef warning(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a warning message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.error","title":"error  <code>abstractmethod</code>","text":"<pre><code>error(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef error(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.pause_live","title":"pause_live","text":"<pre><code>pause_live() -&gt; None\n</code></pre> <p>Pause live display (e.g., spinner) before user interaction.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def pause_live(self) -&gt; None:  # noqa: B027\n    \"\"\"Pause live display (e.g., spinner) before user interaction.\"\"\"\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.AgentLoggerBase.resume_live","title":"resume_live","text":"<pre><code>resume_live() -&gt; None\n</code></pre> <p>Resume live display after user interaction.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def resume_live(self) -&gt; None:  # noqa: B027\n    \"\"\"Resume live display after user interaction.\"\"\"\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.SessionState","title":"SessionState  <code>dataclass</code>","text":"<pre><code>SessionState(\n    exit_stack: AsyncExitStack,\n    exec_env: CodeExecToolProvider | None = None,\n    output_dir: str | None = None,\n    parent_exec_env: CodeExecToolProvider | None = None,\n    depth: int = 0,\n    exec_env_owned: bool = True,\n    uploaded_file_paths: list[str] = list(),\n    skills_metadata: list[SkillMetadata] = list(),\n    logger: AgentLoggerBase | None = None,\n)\n</code></pre> <p>Per-session state for resource lifecycle management.</p> <p>Kept minimal - only contains resources that need async lifecycle management (exit_stack, exec_env) and session-specific configuration (output_dir).</p> <p>Tool availability is managed via Agent._active_tools (instance-scoped), and run results are stored on the agent instance temporarily.</p> <p>For subagent file transfer: - parent_exec_env: Reference to the parent's exec env (for cross-env transfers) - depth: Agent depth (0 = root, &gt;0 = subagent) - output_dir: For root agent, this is a local filesystem path. For subagents,   this is a path within the parent's exec env. - exec_env_owned: Whether this session owns the exec_env and should clean it up.   When share_parent_exec_env=True, the subagent borrows the parent's exec_env   and exec_env_owned=False to prevent cleanup on subagent exit.</p>"},{"location":"api/core/agent/#stirrup.core.agent.SubAgentParams","title":"SubAgentParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for sub-agent tool invocation.</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent","title":"Agent","text":"<pre><code>Agent(\n    client: LLMClient,\n    name: str,\n    *,\n    max_turns: int = AGENT_MAX_TURNS,\n    system_prompt: str | None = None,\n    tools: list[Tool | ToolProvider] | None = None,\n    finish_tool: Tool[FinishParams, FinishMeta]\n    | None = None,\n    context_summarization_cutoff: float = CONTEXT_SUMMARIZATION_CUTOFF,\n    turns_remaining_warning_threshold: int = TURNS_REMAINING_WARNING_THRESHOLD,\n    run_sync_in_thread: bool = True,\n    text_only_tool_responses: bool = True,\n    block_successive_assistant_messages: bool = True,\n    share_parent_exec_env: bool = False,\n    logger: AgentLoggerBase | None = None,\n)\n</code></pre> <p>Agent that executes tool-using loops with automatic context management.</p> <p>Runs up to max_turns iterations of: LLM generation \u2192 tool execution \u2192 message accumulation. When conversation history exceeds context window limits, older messages are automatically condensed into a summary to preserve working memory.</p> <p>The Agent can be used as an async context manager via .session() for automatic tool lifecycle management, logging, and file saving:</p> <pre><code>from stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n# Create client and agent\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(client=client, name=\"assistant\")\n\nasync with agent.session(output_dir=\"./output\") as session:\n    finish_params, history, metadata = await session.run(\"Your task here\")\n</code></pre> <p>Initialize the agent with an LLM client and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>LLMClient</code> <p>LLM client for generating responses. Use ChatCompletionsClient for     OpenAI/OpenAI-compatible APIs, or LiteLLMClient for other providers.</p> required <code>name</code> <code>str</code> <p>Name of the agent (used for logging purposes)</p> required <code>max_turns</code> <code>int</code> <p>Maximum number of turns before stopping</p> <code>AGENT_MAX_TURNS</code> <code>system_prompt</code> <code>str | None</code> <p>System prompt to prepend to all runs (when using string prompts)</p> <code>None</code> <code>tools</code> <code>list[Tool | ToolProvider] | None</code> <p>List of Tools and/or ToolProviders available to the agent.    If None, uses DEFAULT_TOOLS. ToolProviders are automatically    set up and torn down by Agent.session().    Use [*DEFAULT_TOOLS, extra_tool] to extend defaults.</p> <code>None</code> <code>finish_tool</code> <code>Tool[FinishParams, FinishMeta] | None</code> <p>Tool used to signal task completion. Defaults to SIMPLE_FINISH_TOOL.</p> <code>None</code> <code>context_summarization_cutoff</code> <code>float</code> <p>Fraction of context window (0-1) at which to trigger summarization</p> <code>CONTEXT_SUMMARIZATION_CUTOFF</code> <code>run_sync_in_thread</code> <code>bool</code> <p>Execute synchronous tool executors in a separate thread</p> <code>True</code> <code>text_only_tool_responses</code> <code>bool</code> <p>Extract images from tool responses as separate user messages</p> <code>True</code> <code>block_successive_assistant_messages</code> <code>bool</code> <p>If True (default), automatically inject a continue                                message when assistant responds without tool calls to                                prevent successive assistant messages.</p> <code>True</code> <code>share_parent_exec_env</code> <code>bool</code> <p>When True and used as a subagent, share the parent's code                    execution environment instead of creating a new one. This                    provides better performance (no file copying) and allows                    the subagent to see all files in the parent's environment.                    Only effective when the agent is used as a subagent via to_tool().</p> <code>False</code> <code>logger</code> <code>AgentLoggerBase | None</code> <p>Optional logger instance. If None, creates AgentLogger() internally.</p> <code>None</code> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def __init__(\n    self,\n    client: LLMClient,\n    name: str,\n    *,\n    max_turns: int = AGENT_MAX_TURNS,\n    system_prompt: str | None = None,\n    tools: list[Tool | ToolProvider] | None = None,\n    finish_tool: Tool[FinishParams, FinishMeta] | None = None,\n    # Agent options\n    context_summarization_cutoff: float = CONTEXT_SUMMARIZATION_CUTOFF,\n    turns_remaining_warning_threshold: int = TURNS_REMAINING_WARNING_THRESHOLD,\n    run_sync_in_thread: bool = True,\n    text_only_tool_responses: bool = True,\n    block_successive_assistant_messages: bool = True,\n    # Subagent options\n    share_parent_exec_env: bool = False,\n    # Logging\n    logger: AgentLoggerBase | None = None,\n) -&gt; None:\n    \"\"\"Initialize the agent with an LLM client and configuration.\n\n    Args:\n        client: LLM client for generating responses. Use ChatCompletionsClient for\n                OpenAI/OpenAI-compatible APIs, or LiteLLMClient for other providers.\n        name: Name of the agent (used for logging purposes)\n        max_turns: Maximum number of turns before stopping\n        system_prompt: System prompt to prepend to all runs (when using string prompts)\n        tools: List of Tools and/or ToolProviders available to the agent.\n               If None, uses DEFAULT_TOOLS. ToolProviders are automatically\n               set up and torn down by Agent.session().\n               Use [*DEFAULT_TOOLS, extra_tool] to extend defaults.\n        finish_tool: Tool used to signal task completion. Defaults to SIMPLE_FINISH_TOOL.\n        context_summarization_cutoff: Fraction of context window (0-1) at which to trigger summarization\n        run_sync_in_thread: Execute synchronous tool executors in a separate thread\n        text_only_tool_responses: Extract images from tool responses as separate user messages\n        block_successive_assistant_messages: If True (default), automatically inject a continue\n                                           message when assistant responds without tool calls to\n                                           prevent successive assistant messages.\n        share_parent_exec_env: When True and used as a subagent, share the parent's code\n                               execution environment instead of creating a new one. This\n                               provides better performance (no file copying) and allows\n                               the subagent to see all files in the parent's environment.\n                               Only effective when the agent is used as a subagent via to_tool().\n        logger: Optional logger instance. If None, creates AgentLogger() internally.\n\n    \"\"\"\n    # Validate agent name\n    if not AGENT_NAME_PATTERN.match(name):\n        raise ValueError(\n            f\"Invalid agent name '{name}'. \"\n            \"Agent names must match pattern '^[a-zA-Z0-9_-]{1,128}$' \"\n            \"(alphanumeric, underscores, hyphens only, 1-128 characters).\"\n        )\n\n    self._client: LLMClient = client\n    self._name = name\n    self._max_turns = max_turns\n    self._system_prompt = system_prompt\n    self._tools = tools if tools is not None else DEFAULT_TOOLS\n    self._finish_tool: Tool = finish_tool if finish_tool is not None else SIMPLE_FINISH_TOOL\n    self._context_summarization_cutoff = context_summarization_cutoff\n    self._turns_remaining_warning_threshold = turns_remaining_warning_threshold\n    self._run_sync_in_thread = run_sync_in_thread\n    self._text_only_tool_responses = text_only_tool_responses\n    self._block_successive_assistant_messages = block_successive_assistant_messages\n    self._share_parent_exec_env = share_parent_exec_env\n\n    # Logger (can be passed in or created here)\n    self._logger: AgentLoggerBase = logger if logger is not None else AgentLogger()\n\n    # Session configuration (set during session(), used in __aenter__)\n    self._pending_output_dir: Path | None = None\n    self._pending_input_files: str | Path | list[str | Path] | None = None\n    self._pending_skills_dir: Path | None = None\n    self._resume: bool = False\n    self._clear_cache_on_success: bool = True\n    self._cache_on_interrupt: bool = True\n\n    # Instance-scoped state (populated during __aenter__, isolated per agent instance)\n    self._active_tools: dict[str, Tool] = {}\n    self._last_finish_params: Any = None  # FinishParams type parameter\n    self._last_run_metadata: dict[str, list[Any]] = {}\n    self._transferred_paths: list[str] = []  # Paths transferred to parent (for subagents)\n\n    # Cache state for resumption (set during run(), used in __aexit__ for caching on interrupt)\n    self._current_task_hash: str | None = None\n    self._current_run_state: CacheState | None = None\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of this agent.</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.client","title":"client  <code>property</code>","text":"<pre><code>client: LLMClient\n</code></pre> <p>The LLM client used by this agent.</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: dict[str, Tool]\n</code></pre> <p>Currently active tools (available after entering session context).</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.finish_tool","title":"finish_tool  <code>property</code>","text":"<pre><code>finish_tool: Tool\n</code></pre> <p>The finish tool used to signal task completion.</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.logger","title":"logger  <code>property</code>","text":"<pre><code>logger: AgentLoggerBase\n</code></pre> <p>The logger instance used by this agent.</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.session","title":"session","text":"<pre><code>session(\n    output_dir: Path | str | None = None,\n    input_files: str\n    | Path\n    | list[str | Path]\n    | None = None,\n    skills_dir: Path | str | None = None,\n    resume: bool = False,\n    clear_cache_on_success: bool = True,\n    cache_on_interrupt: bool = True,\n) -&gt; Self\n</code></pre> <p>Configure a session and return self for use as async context manager.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str | None</code> <p>Directory to save output files from finish_params.paths</p> <code>None</code> <code>input_files</code> <code>str | Path | list[str | Path] | None</code> <p>Files to upload to the execution environment at session start.         Accepts a single path or list of paths. Supports:         - File paths (str or Path)         - Directory paths (uploaded recursively)         - Glob patterns (e.g., \"data/.csv\", \"/.py\")         Raises ValueError if no CodeExecToolProvider is configured         or if a glob pattern matches no files.</p> <code>None</code> <code>skills_dir</code> <code>Path | str | None</code> <p>Directory containing skill definitions to load and make available        to the agent. Skills are uploaded to the execution environment        and their metadata is included in the system prompt.</p> <code>None</code> <code>resume</code> <code>bool</code> <p>If True, attempt to resume from cached state if available.    The cache is identified by hashing the init_msgs passed to run().    Cached state includes message history, current turn, and execution    environment files from a previous interrupted run.</p> <code>False</code> <code>clear_cache_on_success</code> <code>bool</code> <p>If True (default), automatically clear the cache                    when the agent completes successfully. Set to False                    to preserve caches for inspection or debugging.</p> <code>True</code> <code>cache_on_interrupt</code> <code>bool</code> <p>If True (default), set up a SIGINT handler to cache                state on Ctrl+C. Set to False when running agents in                threads or subprocesses where signal handlers cannot                be registered from non-main threads.</p> <code>True</code> <p>Returns:</p> Type Description <code>Self</code> <p>Self, for use with <code>async with agent.session(...) as session:</code></p> Example <p>async with agent.session(output_dir=\"./output\", input_files=\"data/*.csv\") as session:     result = await session.run(\"Analyze the CSV files\")</p> Note <p>Multiple concurrent sessions from the same Agent instance are supported. Each session maintains isolated state via ContextVar.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def session(\n    self,\n    output_dir: Path | str | None = None,\n    input_files: str | Path | list[str | Path] | None = None,\n    skills_dir: Path | str | None = None,\n    resume: bool = False,\n    clear_cache_on_success: bool = True,\n    cache_on_interrupt: bool = True,\n) -&gt; Self:\n    \"\"\"Configure a session and return self for use as async context manager.\n\n    Args:\n        output_dir: Directory to save output files from finish_params.paths\n        input_files: Files to upload to the execution environment at session start.\n                    Accepts a single path or list of paths. Supports:\n                    - File paths (str or Path)\n                    - Directory paths (uploaded recursively)\n                    - Glob patterns (e.g., \"data/*.csv\", \"**/*.py\")\n                    Raises ValueError if no CodeExecToolProvider is configured\n                    or if a glob pattern matches no files.\n        skills_dir: Directory containing skill definitions to load and make available\n                   to the agent. Skills are uploaded to the execution environment\n                   and their metadata is included in the system prompt.\n        resume: If True, attempt to resume from cached state if available.\n               The cache is identified by hashing the init_msgs passed to run().\n               Cached state includes message history, current turn, and execution\n               environment files from a previous interrupted run.\n        clear_cache_on_success: If True (default), automatically clear the cache\n                               when the agent completes successfully. Set to False\n                               to preserve caches for inspection or debugging.\n        cache_on_interrupt: If True (default), set up a SIGINT handler to cache\n                           state on Ctrl+C. Set to False when running agents in\n                           threads or subprocesses where signal handlers cannot\n                           be registered from non-main threads.\n\n    Returns:\n        Self, for use with `async with agent.session(...) as session:`\n\n    Example:\n        async with agent.session(output_dir=\"./output\", input_files=\"data/*.csv\") as session:\n            result = await session.run(\"Analyze the CSV files\")\n\n    Note:\n        Multiple concurrent sessions from the same Agent instance are supported.\n        Each session maintains isolated state via ContextVar.\n\n    \"\"\"\n    self._pending_output_dir = Path(output_dir) if output_dir else None\n    self._pending_input_files = input_files\n    self._pending_skills_dir = Path(skills_dir) if skills_dir else None\n    self._resume = resume\n    self._clear_cache_on_success = clear_cache_on_success\n    self._cache_on_interrupt = cache_on_interrupt\n    return self\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Self\n</code></pre> <p>Enter session context: set up tools, logging, and resources.</p> <p>Creates a new SessionState and stores it in the _SESSION_STATE ContextVar, allowing concurrent sessions from the same Agent instance.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def __aenter__(self) -&gt; Self:\n    \"\"\"Enter session context: set up tools, logging, and resources.\n\n    Creates a new SessionState and stores it in the _SESSION_STATE ContextVar,\n    allowing concurrent sessions from the same Agent instance.\n    \"\"\"\n    exit_stack = AsyncExitStack()\n    await exit_stack.__aenter__()\n\n    # Get parent state if exists (for subagent file transfer)\n    parent_state = _SESSION_STATE.get(None)\n\n    current_depth = _PARENT_DEPTH.get()\n\n    # Create session state and store in ContextVar\n    state = SessionState(\n        exit_stack=exit_stack,\n        output_dir=str(self._pending_output_dir) if self._pending_output_dir else None,\n        parent_exec_env=parent_state.exec_env if parent_state else None,\n        depth=current_depth,\n        logger=self._logger,\n    )\n    _SESSION_STATE.set(state)\n\n    try:\n        # === TWO-PASS TOOL INITIALIZATION ===\n        # First pass initializes CodeExecToolProvider so that dependent tools\n        # (like ViewImageToolProvider) can access state.exec_env in second pass.\n        active_tools: list[Tool] = []\n\n        # Check if we should share parent's exec_env (subagent with share_parent_exec_env=True)\n        should_share_exec_env = (\n            self._share_parent_exec_env\n            and current_depth &gt; 0\n            and parent_state is not None\n            and parent_state.exec_env is not None\n        )\n\n        if should_share_exec_env:\n            # SHARED EXEC ENV: Use parent's exec_env directly, don't create new one\n            state.exec_env = parent_state.exec_env  # type: ignore[union-attr]\n            state.exec_env_owned = False\n            logger.debug(\n                \"[%s __aenter__] Sharing parent's exec_env: %s (temp_dir=%s)\",\n                self._name,\n                type(state.exec_env).__name__,\n                getattr(state.exec_env, \"_temp_dir\", \"N/A\"),\n            )\n            # Skip CodeExecToolProvider initialization but still need to add code exec tool\n            # Create the tool from the shared exec_env using get_code_exec_tool()\n            # (the exec_env is already entered by parent, so we just create the tool wrapper)\n            if state.exec_env is None:\n                raise RuntimeError(\"Expected shared exec_env to be set, but it is None\")\n            code_exec_tool = state.exec_env.get_code_exec_tool()\n            active_tools.append(code_exec_tool)\n        else:\n            # OWNED EXEC ENV: Initialize our own CodeExecToolProvider (at most one allowed)\n            code_exec_providers = [t for t in self._tools if isinstance(t, CodeExecToolProvider)]\n            if len(code_exec_providers) &gt; 1:\n                raise ValueError(\n                    f\"Agent can only have one CodeExecToolProvider, found {len(code_exec_providers)}: \"\n                    f\"{[type(p).__name__ for p in code_exec_providers]}\"\n                )\n\n            if code_exec_providers:\n                provider = code_exec_providers[0]\n                result = await exit_stack.enter_async_context(provider)\n                if isinstance(result, list):\n                    active_tools.extend(result)\n                else:\n                    active_tools.append(result)\n                state.exec_env = provider\n                state.exec_env_owned = True\n\n        # Second pass: Initialize remaining ToolProviders and static Tools\n        for tool in self._tools:\n            if isinstance(tool, CodeExecToolProvider):\n                continue  # Already processed in first pass\n\n            if isinstance(tool, ToolProvider):\n                # ToolProvider: enter context and get returned tool(s)\n                result = await exit_stack.enter_async_context(tool)\n                # Handle both single Tool and list[Tool] returns (e.g., MCPToolProvider)\n                if isinstance(result, list):\n                    active_tools.extend(result)\n                else:\n                    active_tools.append(result)\n            else:\n                # Static Tool, use directly\n                active_tools.append(tool)\n\n        # Build active tools dict with finish tool (stored on instance, not session)\n        self._active_tools = {FINISH_TOOL_NAME: self._finish_tool}\n        self._active_tools.update({t.name: t for t in active_tools})\n\n        # Validate subagent code exec requirements (only at root level)\n        if current_depth == 0:\n            self._validate_subagent_code_exec_requirements()\n\n        # Upload input files to exec_env if specified\n        if self._pending_input_files:\n            if not state.exec_env:\n                raise ValueError(\"input_files specified but no CodeExecToolProvider configured\")\n\n            logger.debug(\n                \"[%s __aenter__] Uploading input files: %s, depth=%d, parent_exec_env=%s, parent_exec_env._temp_dir=%s, exec_env_owned=%s\",\n                self._name,\n                self._pending_input_files,\n                state.depth,\n                type(state.parent_exec_env).__name__ if state.parent_exec_env else None,\n                getattr(state.parent_exec_env, \"_temp_dir\", \"N/A\") if state.parent_exec_env else None,\n                state.exec_env_owned,\n            )\n\n            if state.depth &gt; 0 and state.parent_exec_env:\n                if not state.exec_env_owned:\n                    # SHARED EXEC ENV: Files already accessible - no transfer needed\n                    # Just record the paths as \"uploaded\" for system prompt\n                    if isinstance(self._pending_input_files, (str, Path)):\n                        state.uploaded_file_paths = [str(self._pending_input_files)]\n                    else:\n                        state.uploaded_file_paths = [str(p) for p in self._pending_input_files]\n                    logger.debug(\n                        \"[%s __aenter__] Shared exec_env - files already accessible: %s\",\n                        self._name,\n                        state.uploaded_file_paths,\n                    )\n                else:\n                    # SEPARATE EXEC ENV: Read files from parent's exec env, write to subagent's exec env\n                    # input_files are paths within the parent's environment\n                    result = await state.exec_env.upload_files(\n                        *self._pending_input_files,\n                        source_env=state.parent_exec_env,\n                    )\n                    logger.debug(\n                        \"[%s __aenter__] Upload result: uploaded=%s, failed=%s\",\n                        self._name,\n                        result.uploaded,\n                        result.failed,\n                    )\n                    state.uploaded_file_paths = [uf.dest_path for uf in result.uploaded]\n                    if result.failed:\n                        raise RuntimeError(f\"Failed to upload files: {result.failed}\")\n            else:\n                # ROOT AGENT: Read files from local filesystem\n                resolved = self._resolve_input_files(self._pending_input_files)\n                result = await state.exec_env.upload_files(*resolved)\n                logger.debug(\n                    \"[%s __aenter__] Upload result: uploaded=%s, failed=%s\",\n                    self._name,\n                    result.uploaded,\n                    result.failed,\n                )\n                state.uploaded_file_paths = [uf.dest_path for uf in result.uploaded]\n                if result.failed:\n                    raise RuntimeError(f\"Failed to upload files: {result.failed}\")\n        self._pending_input_files = None  # Clear pending state\n\n        # Upload skills directory if it exists and load metadata\n        if self._pending_skills_dir:\n            skills_path = self._pending_skills_dir\n            if skills_path.exists() and skills_path.is_dir():\n                if state.exec_env:\n                    logger.debug(\"[%s __aenter__] Uploading skills directory: %s\", self._name, skills_path)\n                    await state.exec_env.upload_files(skills_path, dest_dir=\"skills\")\n                # Load skills metadata (even if no exec_env, for system prompt)\n                state.skills_metadata = load_skills_metadata(skills_path)\n                logger.debug(\"[%s __aenter__] Loaded %d skills\", self._name, len(state.skills_metadata))\n            self._pending_skills_dir = None  # Clear pending state\n        elif parent_state and parent_state.skills_metadata:\n            # Sub-agent: inherit skills from parent\n            state.skills_metadata = parent_state.skills_metadata\n            logger.debug(\"[%s __aenter__] Inherited %d skills from parent\", self._name, len(state.skills_metadata))\n            # Transfer skills directory from parent's exec_env to sub-agent's exec_env\n            # (only if we have a separate exec_env)\n            if state.exec_env and parent_state.exec_env and state.exec_env_owned:\n                await state.exec_env.upload_files(\"skills\", source_env=parent_state.exec_env)\n\n        # Configure and enter logger context\n        self._logger.name = self._name\n        self._logger.model = self._client.model_slug\n        self._logger.max_turns = self._max_turns\n        # depth is already set (0 for main agent, passed in for sub-agents)\n        self._logger.__enter__()\n\n        # Set up signal handler for graceful caching on interrupt (root agent only)\n        if current_depth == 0 and self._cache_on_interrupt:\n            self._original_sigint = signal.getsignal(signal.SIGINT)\n            signal.signal(signal.SIGINT, self._handle_interrupt)\n\n        return self\n\n    except Exception:\n        await exit_stack.__aexit__(None, None, None)\n        raise\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit session context: save files, cleanup resources.</p> <p>File handling is depth-aware: - Root agent (depth=0): Saves files to local filesystem output_dir - Subagent (depth&gt;0): Transfers files to parent's exec env at output_dir path</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit session context: save files, cleanup resources.\n\n    File handling is depth-aware:\n    - Root agent (depth=0): Saves files to local filesystem output_dir\n    - Subagent (depth&gt;0): Transfers files to parent's exec env at output_dir path\n    \"\"\"\n    state = _SESSION_STATE.get()\n\n    try:\n        # Cache state on non-success exit (only at root level)\n        should_cache = (\n            state.depth == 0\n            and (exc_type is not None or self._last_finish_params is None)\n            and self._current_task_hash is not None\n            and self._current_run_state is not None\n        )\n\n        logger.debug(\n            \"[%s __aexit__] Cache decision: should_cache=%s, depth=%d, exc_type=%s, \"\n            \"finish_params=%s, task_hash=%s, run_state=%s\",\n            self._name,\n            should_cache,\n            state.depth,\n            exc_type,\n            self._last_finish_params is not None,\n            self._current_task_hash,\n            self._current_run_state is not None,\n        )\n\n        if should_cache:\n            cache_manager = CacheManager(clear_on_success=self._clear_cache_on_success)\n\n            exec_env_dir = state.exec_env.temp_dir if state.exec_env else None\n\n            # Explicit checks to keep type checker happy - should_cache condition guarantees these\n            if self._current_task_hash is None or self._current_run_state is None:\n                raise ValueError(\"Cache state is unexpectedly None after should_cache check\")\n\n            # Temporarily block SIGINT during cache save to prevent interruption\n            original_handler = signal.getsignal(signal.SIGINT)\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n            try:\n                cache_manager.save_state(\n                    self._current_task_hash,\n                    self._current_run_state,\n                    exec_env_dir,\n                )\n            finally:\n                signal.signal(signal.SIGINT, original_handler)\n            self._logger.info(f\"Cached state for task {self._current_task_hash}\")\n        # Save files from finish_params.paths based on depth\n        if state.output_dir and self._last_finish_params and state.exec_env:\n            paths = getattr(self._last_finish_params, \"paths\", None)\n            if paths:\n                if state.depth == 0:\n                    # ROOT AGENT: Save to local filesystem\n                    output_path = Path(state.output_dir)\n                    output_path.mkdir(parents=True, exist_ok=True)\n                    logger.debug(\n                        \"[%s] ROOT AGENT (depth=0): Saving %d file(s) to local filesystem: %s -&gt; %s\",\n                        self._name,\n                        len(paths),\n                        paths,\n                        output_path,\n                    )\n                    result = await state.exec_env.save_output_files(paths, output_path, dest_env=None)\n                    logger.debug(\n                        \"[%s] ROOT AGENT: Saved %d file(s), failed %d\",\n                        self._name,\n                        len(result.saved),\n                        len(result.failed),\n                    )\n                else:\n                    # SUBAGENT: Handle file transfer based on exec_env ownership\n                    if not state.exec_env_owned:\n                        # SHARED EXEC ENV: Files already in parent's env - no transfer needed\n                        # Just record the paths for reporting to parent\n                        self._transferred_paths = list(paths)\n                        logger.debug(\n                            \"[%s] SUBAGENT (depth=%d, shared_exec_env): Files already in parent env: %s\",\n                            self._name,\n                            state.depth,\n                            self._transferred_paths,\n                        )\n                    elif state.parent_exec_env:\n                        # SEPARATE EXEC ENV: Transfer to parent's exec env\n                        logger.debug(\n                            \"[%s] SUBAGENT (depth=%d): Transferring %d file(s) to parent exec env: %s -&gt; %s\",\n                            self._name,\n                            state.depth,\n                            len(paths),\n                            paths,\n                            state.output_dir,\n                        )\n                        result = await state.exec_env.save_output_files(\n                            paths, state.output_dir, dest_env=state.parent_exec_env\n                        )\n                        # Store transferred paths for returning to parent\n                        self._transferred_paths = [str(sf.output_path) for sf in result.saved]\n                        logger.debug(\n                            \"[%s] SUBAGENT: Transferred %d file(s) to parent, failed %d. Paths: %s\",\n                            self._name,\n                            len(result.saved),\n                            len(result.failed),\n                            self._transferred_paths,\n                        )\n                        if result.failed:\n                            logger.warning(\"Failed to transfer some files to parent env: %s\", result.failed)\n                    else:\n                        logger.warning(\n                            \"Subagent at depth %d has exec_env but no parent_exec_env. \"\n                            \"Files will not be transferred.\",\n                            state.depth,\n                        )\n    finally:\n        # Restore original signal handler (root agent only)\n        if hasattr(self, \"_original_sigint\"):\n            signal.signal(signal.SIGINT, self._original_sigint)\n            del self._original_sigint\n\n        # Exit logger context\n        self._logger.finish_params = self._last_finish_params\n        self._logger.run_metadata = self._last_run_metadata\n        self._logger.output_dir = str(state.output_dir) if state.output_dir else None\n        self._logger.__exit__(exc_type, exc_val, exc_tb)\n\n        # Cleanup all async resources\n        await state.exit_stack.__aexit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.run_tool","title":"run_tool  <code>async</code>","text":"<pre><code>run_tool(\n    tool_call: ToolCall, run_metadata: dict[str, list[Any]]\n) -&gt; ToolMessage\n</code></pre> <p>Execute a single tool call with error handling for invalid JSON/arguments.</p> <p>Returns a ToolMessage containing either the tool output or an error description. Metadata from the tool result is stored in the provided run_metadata dict.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def run_tool(self, tool_call: ToolCall, run_metadata: dict[str, list[Any]]) -&gt; ToolMessage:\n    \"\"\"Execute a single tool call with error handling for invalid JSON/arguments.\n\n    Returns a ToolMessage containing either the tool output or an error description.\n    Metadata from the tool result is stored in the provided run_metadata dict.\n    \"\"\"\n    tool = self._active_tools.get(tool_call.name)\n    result: ToolResult\n    args_valid = True\n\n    # Ensure tool is tracked in metadata dict (even if no metadata returned)\n    if tool_call.name not in run_metadata:\n        run_metadata[tool_call.name] = []\n\n    tool_start_time = perf_counter()\n\n    if tool:\n        try:\n            # Normalize empty arguments to valid empty JSON object\n            args = tool_call.arguments if tool_call.arguments and tool_call.arguments.strip() else \"{}\"\n            params = tool.parameters.model_validate_json(args)\n\n            # Set parent depth for sub-agent tools to read\n            prev_depth = _PARENT_DEPTH.set(self._logger.depth)\n            try:\n                if inspect.iscoroutinefunction(tool.executor):\n                    result = await tool.executor(params)  # ty: ignore[invalid-await]\n                elif self._run_sync_in_thread:\n                    # ty: ignore - type checker doesn't understand iscoroutinefunction narrowing\n                    result = await anyio.to_thread.run_sync(tool.executor, params)  # ty: ignore[unresolved-attribute]\n                else:\n                    # ty: ignore - iscoroutinefunction check above ensures this is sync\n                    result = tool.executor(params)  # ty: ignore[invalid-assignment]\n            finally:\n                _PARENT_DEPTH.reset(prev_depth)\n\n            # Store metadata if present\n            if result.metadata is not None:\n                run_metadata[tool_call.name].append(result.metadata)\n        except ValidationError:\n            LOGGER.debug(\n                \"LLMClient tried to use the tool %s but the tool arguments are not valid: %r\",\n                tool_call.name,\n                tool_call.arguments,\n            )\n            result = ToolResult(content=\"Tool arguments are not valid\", success=False)\n            args_valid = False\n    else:\n        LOGGER.debug(f\"LLMClient tried to use the tool {tool_call.name} which is not in the tools list\")\n        result = ToolResult(content=f\"{tool_call.name} is not a valid tool\", success=False)\n\n    tool_end_time = perf_counter()\n\n    return ToolMessage(\n        content=result.content,\n        tool_call_id=tool_call.tool_call_id,\n        name=tool_call.name,\n        args_was_valid=args_valid,\n        success=result.success,\n        tool_start_time=tool_start_time,\n        tool_end_time=tool_end_time,\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.step","title":"step  <code>async</code>","text":"<pre><code>step(\n    messages: list[ChatMessage],\n    run_metadata: dict[str, list[Any]],\n    turn: int = 0,\n    max_turns: int = 0,\n) -&gt; tuple[\n    AssistantMessage, list[ToolMessage], FinishParams | None\n]\n</code></pre> <p>Execute one agent step: generate assistant message and run any requested tool calls.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ChatMessage]</code> <p>Current conversation messages</p> required <code>run_metadata</code> <code>dict[str, list[Any]]</code> <p>Metadata storage for tool results</p> required <code>turn</code> <code>int</code> <p>Current turn number (1-indexed) for logging</p> <code>0</code> <code>max_turns</code> <code>int</code> <p>Maximum turns for logging</p> <code>0</code> <p>Returns the assistant message, tool execution results, and finish tool call (if present).</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def step(\n    self,\n    messages: list[ChatMessage],\n    run_metadata: dict[str, list[Any]],\n    turn: int = 0,\n    max_turns: int = 0,\n) -&gt; tuple[AssistantMessage, list[ToolMessage], FinishParams | None]:\n    \"\"\"Execute one agent step: generate assistant message and run any requested tool calls.\n\n    Args:\n        messages: Current conversation messages\n        run_metadata: Metadata storage for tool results\n        turn: Current turn number (1-indexed) for logging\n        max_turns: Maximum turns for logging\n\n    Returns the assistant message, tool execution results, and finish tool call (if present).\n\n    \"\"\"\n    assistant_message = await self._client.generate(messages, self._active_tools)\n\n    # Log assistant message immediately\n    if turn &gt; 0:\n        self._logger.assistant_message(turn, max_turns, assistant_message)\n\n    finish_params: FinishParams | None = None\n    tool_messages: list[ToolMessage] = []\n    if assistant_message.tool_calls:\n        tool_messages = []\n        for tool_call in assistant_message.tool_calls:\n            tool_message = await self.run_tool(tool_call, run_metadata)\n            tool_messages.append(tool_message)\n\n            if tool_message.success and tool_message.name == FINISH_TOOL_NAME:\n                finish_params = self._finish_tool.parameters.model_validate_json(tool_call.arguments)\n\n            # Log tool result immediately\n            self._logger.tool_result(tool_message)\n\n    return assistant_message, tool_messages, finish_params\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.summarize_messages","title":"summarize_messages  <code>async</code>","text":"<pre><code>summarize_messages(\n    messages: list[ChatMessage],\n) -&gt; list[ChatMessage]\n</code></pre> <p>Condense message history using LLM to stay within context window.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def summarize_messages(self, messages: list[ChatMessage]) -&gt; list[ChatMessage]:\n    \"\"\"Condense message history using LLM to stay within context window.\"\"\"\n    task_context: list[ChatMessage] = list(takewhile(lambda m: not isinstance(m, AssistantMessage), messages))\n\n    summary_prompt = [*messages, UserMessage(content=MESSAGE_SUMMARIZER)]\n\n    # We need to pass the tools to the client so that it has context of tools used in the conversation\n    summary = await self._client.generate(summary_prompt, self._active_tools)\n\n    summary_bridge_prompt = MESSAGE_SUMMARIZER_BRIDGE_TEMPLATE.format(summary=summary.content)\n    summary_bridge = UserMessage(content=summary_bridge_prompt)\n    acknowledgement_msg = UserMessage(content=\"Got it, thanks!\")\n\n    # Log the completed summary\n    summary_content = summary.content if isinstance(summary.content, str) else str(summary.content)\n    self._logger.context_summarization_complete(summary_content, summary_bridge_prompt)\n\n    return [*task_context, summary_bridge, acknowledgement_msg]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.run","title":"run  <code>async</code>","text":"<pre><code>run(\n    init_msgs: str | list[ChatMessage],\n    *,\n    depth: int | None = None,\n) -&gt; tuple[\n    FinishParams | None,\n    list[list[ChatMessage]],\n    dict[str, Any],\n]\n</code></pre> <p>Execute the agent loop until finish tool is called or max_turns reached.</p> <p>A base system prompt is automatically prepended to all runs, including: - Agent purpose and max_turns info - List of input files (if provided via session()) - User's custom system_prompt (if configured in init)</p> <p>Parameters:</p> Name Type Description Default <code>init_msgs</code> <code>str | list[ChatMessage]</code> <p>Either a string prompt (converted to UserMessage) or a list of       ChatMessage to extend the conversation after the system prompt.</p> required <code>depth</code> <code>int | None</code> <p>Logging depth for sub-agent runs. If provided, updates logger.depth for this run.</p> <code>None</code> <p>Returns:</p> Type Description <code>FinishParams | None</code> <p>Tuple of (finish params, message history, run metadata).</p> <code>list[list[ChatMessage]]</code> <p>finish params is None if max_turns reached.</p> <code>dict[str, Any]</code> <p>run metadata maps tool/agent names to lists of metadata returned by each call.</p> Example Source code in <code>src/stirrup/core/agent.py</code> <pre><code>async def run(\n    self,\n    init_msgs: str | list[ChatMessage],\n    *,\n    depth: int | None = None,\n) -&gt; tuple[FinishParams | None, list[list[ChatMessage]], dict[str, Any]]:\n    \"\"\"Execute the agent loop until finish tool is called or max_turns reached.\n\n    A base system prompt is automatically prepended to all runs, including:\n    - Agent purpose and max_turns info\n    - List of input files (if provided via session())\n    - User's custom system_prompt (if configured in __init__)\n\n    Args:\n        init_msgs: Either a string prompt (converted to UserMessage) or a list of\n                  ChatMessage to extend the conversation after the system prompt.\n        depth: Logging depth for sub-agent runs. If provided, updates logger.depth for this run.\n\n    Returns:\n        Tuple of (finish params, message history, run metadata).\n        finish params is None if max_turns reached.\n        run metadata maps tool/agent names to lists of metadata returned by each call.\n\n    Example:\n        # Simple string prompt\n        await agent.run(\"Analyze this data and create a report\")\n\n        # Multiple messages\n        await agent.run([\n            UserMessage(content=\"First, read the data\"),\n            AssistantMessage(content=\"I've read the data file...\"),\n            UserMessage(content=\"Now analyze it\"),\n        ])\n\n    \"\"\"\n\n    # Compute task hash for caching/resume\n    task_hash = compute_task_hash(init_msgs)\n    self._current_task_hash = task_hash\n\n    # Initialize cache manager\n    cache_manager = CacheManager(clear_on_success=self._clear_cache_on_success)\n    start_turn = 0\n    resumed = False\n\n    # Try to resume from cache if requested\n    if self._resume:\n        state = _SESSION_STATE.get()\n        cached = cache_manager.load_state(task_hash)\n        if cached:\n            # Restore files to exec env\n            if state.exec_env and state.exec_env.temp_dir:\n                cache_manager.restore_files(task_hash, state.exec_env.temp_dir)\n\n            # Restore state\n            msgs = cached.msgs\n            full_msg_history = cached.full_msg_history\n            run_metadata = cached.run_metadata\n            start_turn = cached.turn\n            resumed = True\n            self._logger.info(f\"Resuming from cached state at turn {start_turn}\")\n        else:\n            self._logger.info(f\"No cache found for task {task_hash}, starting fresh\")\n\n    if not resumed:\n        msgs: list[ChatMessage] = []\n\n        # Build the complete system prompt (base + input files + user instructions)\n        full_system_prompt = self._build_system_prompt()\n        msgs.append(SystemMessage(content=full_system_prompt))\n\n        if isinstance(init_msgs, str):\n            msgs.append(UserMessage(content=init_msgs))\n        else:\n            msgs.extend(init_msgs)\n\n        # Local metadata storage - isolated per run() invocation for thread safety\n        run_metadata: dict[str, list[Any]] = {}\n\n        full_msg_history: list[list[ChatMessage]] = []\n\n    # Set logger depth if provided (for sub-agent runs)\n    if depth is not None:\n        self._logger.depth = depth\n\n    # Log the task at run start (only if not resuming)\n    if not resumed:\n        self._logger.task_message(msgs[-1].content)\n\n    # Show warnings (top-level only, if logger supports it)\n    if self._logger.depth == 0 and isinstance(self._logger, AgentLogger):\n        run_warnings = self._collect_warnings()\n        if run_warnings:\n            self._logger.warnings_message(run_warnings)\n\n    # Use logger callback if available and not overridden\n    step_callback = self._logger.on_step\n\n    full_msg_history: list[list[ChatMessage]] = []\n\n    # Cumulative stats for spinner\n    total_tool_calls = 0\n    total_input_tokens = 0\n    total_output_tokens = 0\n\n    for i in range(start_turn, self._max_turns):\n        # Capture current state for potential caching (before any async work)\n        self._current_run_state = CacheState(\n            msgs=list(msgs),\n            full_msg_history=[list(group) for group in full_msg_history],\n            turn=i,\n            run_metadata=dict(run_metadata),\n            task_hash=task_hash,\n            agent_name=self._name,\n        )\n        if self._max_turns - i &lt;= self._turns_remaining_warning_threshold and i != 0:\n            num_turns_remaining_msg = _num_turns_remaining_msg(self._max_turns - i)\n            msgs.append(num_turns_remaining_msg)\n            self._logger.user_message(num_turns_remaining_msg)\n\n        # Pass turn info to step() for real-time logging\n        assistant_message, tool_messages, finish_params = await self.step(\n            msgs,\n            run_metadata,\n            turn=i + 1,\n            max_turns=self._max_turns,\n        )\n\n        # Update cumulative stats\n        total_tool_calls += len(tool_messages)\n        total_input_tokens += assistant_message.token_usage.input\n        total_output_tokens += assistant_message.token_usage.output\n\n        # Call progress callback after step completes\n        if step_callback:\n            step_callback(i + 1, total_tool_calls, total_input_tokens, total_output_tokens)\n\n        user_messages: list[UserMessage] = []\n        if self._text_only_tool_responses:\n            tool_messages, user_messages = _handle_text_only_tool_responses(tool_messages)\n\n        # Log user messages (e.g., image content extracted from tool responses)\n        for user_msg in user_messages:\n            self._logger.user_message(user_msg)\n\n        msgs.extend([assistant_message, *tool_messages, *user_messages])\n\n        if finish_params:\n            break\n\n        pct_context_used = assistant_message.token_usage.total / self._client.max_tokens\n        if pct_context_used &gt;= self._context_summarization_cutoff and i + 1 != self._max_turns:\n            self._logger.context_summarization_start(pct_context_used, self._context_summarization_cutoff)\n            full_msg_history.append(msgs)\n            msgs = await self.summarize_messages(msgs)\n\n        # Avoid successive assistant messages (only if next turn won't show turns remaining)\n        next_turn_will_show_warning = self._max_turns - (i + 1) &lt;= self._turns_remaining_warning_threshold\n        if (\n            self._block_successive_assistant_messages\n            and not tool_messages\n            and not user_messages\n            and not next_turn_will_show_warning\n        ):\n            msgs.extend([UserMessage(content=\"Please continue the task\")])\n    else:\n        LOGGER.error(\n            f\"Maximum number of turns reached: {self._max_turns}. The agent was not able to finish the task. Consider increasing the max_turns parameter.\",\n        )\n\n    full_msg_history.append(msgs)\n\n    # Add agent's own token usage, tool durations, and model speed to run_metadata\n    run_metadata[\"token_usage\"] = _get_total_token_usage(full_msg_history)\n    run_metadata[\"_tool_durations\"] = _get_tool_durations(full_msg_history)  # type: ignore[assignment]\n    run_metadata[\"_model_speed\"] = _get_model_speed_stats(full_msg_history, self._client.model_slug)  # type: ignore[assignment]\n    # Store for __aexit__ to access (on instance for this agent)\n    self._last_finish_params = finish_params\n    self._last_run_metadata = run_metadata\n\n    # Clear cache on successful completion (finish_params is set)\n    if finish_params is not None and cache_manager.clear_on_success:\n        cache_manager.clear_cache(task_hash)\n        self._current_task_hash = None\n        self._current_run_state = None\n\n    return finish_params, full_msg_history, run_metadata\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.run--simple-string-prompt","title":"Simple string prompt","text":"<p>await agent.run(\"Analyze this data and create a report\")</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.run--multiple-messages","title":"Multiple messages","text":"<p>await agent.run([     UserMessage(content=\"First, read the data\"),     AssistantMessage(content=\"I've read the data file...\"),     UserMessage(content=\"Now analyze it\"), ])</p>"},{"location":"api/core/agent/#stirrup.core.agent.Agent.to_tool","title":"to_tool","text":"<pre><code>to_tool(\n    *,\n    description: str = DEFAULT_SUB_AGENT_DESCRIPTION,\n    system_prompt: str | None = None,\n) -&gt; Tool[SubAgentParams, SubAgentMetadata]\n</code></pre> <p>Convert this Agent to a Tool for use as a sub-agent.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Tool description shown to the parent agent</p> <code>DEFAULT_SUB_AGENT_DESCRIPTION</code> <code>system_prompt</code> <code>str | None</code> <p>Optional system prompt to prepend when running</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool[SubAgentParams, SubAgentMetadata]</code> <p>Tool that executes this agent when called, returning SubAgentMetadata</p> <code>Tool[SubAgentParams, SubAgentMetadata]</code> <p>containing token usage, message history, and any metadata from tools</p> <code>Tool[SubAgentParams, SubAgentMetadata]</code> <p>the sub-agent used.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def to_tool(\n    self,\n    *,\n    description: str = DEFAULT_SUB_AGENT_DESCRIPTION,\n    system_prompt: str | None = None,\n) -&gt; Tool[SubAgentParams, SubAgentMetadata]:\n    \"\"\"Convert this Agent to a Tool for use as a sub-agent.\n\n    Args:\n        description: Tool description shown to the parent agent\n        system_prompt: Optional system prompt to prepend when running\n\n    Returns:\n        Tool that executes this agent when called, returning SubAgentMetadata\n        containing token usage, message history, and any metadata from tools\n        the sub-agent used.\n\n    \"\"\"\n    agent = self  # Capture self for closure\n\n    async def sub_agent_executor(params: SubAgentParams) -&gt; ToolResult[SubAgentMetadata]:\n        \"\"\"Execute the sub-agent with the given task.\n\n        Sub-agents enter their own full session to ensure:\n        1. Tool isolation - each agent only sees its own tools (fixes recursive sub-agent bug)\n        2. Proper ToolProvider lifecycle - sub-agent's ToolProviders are initialized\n        3. Correct logging - logger context is entered for proper output formatting\n        \"\"\"\n        # Get parent's depth and calculate subagent depth\n        parent_depth = _PARENT_DEPTH.get()\n        sub_agent_depth = parent_depth + 1\n\n        # Save parent's session state so we can restore it after subagent completes\n        # This ensures sibling subagents see the parent's state, not a previous sibling's stale state\n        parent_session_state = _SESSION_STATE.get(None)\n        logger.debug(\n            \"[%s] PRE-SESSION: _SESSION_STATE=%s, exec_env=%s, exec_env._temp_dir=%s\",\n            agent.name,\n            id(parent_session_state) if parent_session_state else None,\n            type(parent_session_state.exec_env).__name__\n            if parent_session_state and parent_session_state.exec_env\n            else None,\n            getattr(parent_session_state.exec_env, \"_temp_dir\", \"N/A\")\n            if parent_session_state and parent_session_state.exec_env\n            else None,\n        )\n\n        # Set _PARENT_DEPTH to subagent's depth BEFORE entering session\n        # so that __aenter__ reads the correct depth for SessionState.depth\n        prev_depth = _PARENT_DEPTH.set(sub_agent_depth)\n        try:\n            init_msgs: list[ChatMessage] = []\n            if system_prompt:\n                init_msgs.append(SystemMessage(content=system_prompt))\n            init_msgs.append(UserMessage(content=params.task))\n\n            # Sub-agent enters its own full session for tool isolation and proper lifecycle\n            # output_dir is a path within the parent's exec env (not local filesystem)\n            # Files are transferred to parent's env at __aexit__ via save_output_files(dest_env=parent)\n            async with agent.session(\n                output_dir=\".\",  # Path in parent's exec env\n                input_files=list(params.input_files) if params.input_files else None,  # ty: ignore[invalid-argument-type]\n            ) as agent_session:\n                # Override logger depth for proper indentation in console output\n                agent_session._logger.depth = sub_agent_depth  # noqa: SLF001\n\n                finish_params, msg_history, run_metadata = await agent_session.run(init_msgs)\n\n                # Extract the last assistant message with actual content (not just tool calls)\n                last_assistant_msg: AssistantMessage | None = None\n                for msg_group in reversed(msg_history):\n                    for msg in reversed(msg_group):\n                        if isinstance(msg, AssistantMessage) and msg.content:\n                            last_assistant_msg = msg\n                            break\n                    if last_assistant_msg:\n                        break\n\n                # Build content from the assistant message and/or finish params\n                content_parts: list[str] = []\n\n                if last_assistant_msg and last_assistant_msg.content:\n                    content = last_assistant_msg.content\n                    if isinstance(content, list):\n                        content = \"\\n\".join(str(block) for block in content)\n                    content_parts.append(content)\n\n                # Include finish params if available (they often contain the actual result)\n                if finish_params is not None:\n                    finish_dict = finish_params.model_dump()\n                    if finish_dict:\n                        content_parts.append(f\"Finish params: {finish_dict}\")\n\n                # Report files transferred to parent's exec env (set in __aexit__)\n                transferred_paths = agent_session._transferred_paths  # noqa: SLF001\n                if transferred_paths:\n                    content_parts.append(f\"Files available in your environment: {transferred_paths}\")\n\n                if not content_parts:\n                    result_content = \"&lt;sub_agent_result&gt;\\n&lt;error&gt;No assistant message or finish params found&lt;/error&gt;\\n&lt;/sub_agent_result&gt;\"\n                else:\n                    content = \"\\n\".join(content_parts)\n                    result_content = (\n                        f\"&lt;sub_agent_result&gt;\"\n                        f\"\\n&lt;response&gt;{content}&lt;/response&gt;\"\n                        f\"\\n&lt;finished&gt;{finish_params is not None}&lt;/finished&gt;\"\n                        f\"\\n&lt;/sub_agent_result&gt;\"\n                    )\n\n                # Create subagent metadata with token usage, message history, and run metadata\n                sub_metadata = SubAgentMetadata(\n                    message_history=msg_history,\n                    run_metadata=run_metadata,\n                )\n\n                return ToolResult(content=result_content, metadata=sub_metadata)\n\n        except Exception as e:\n            # On error, return empty metadata\n            error_metadata = SubAgentMetadata(\n                message_history=[],\n                run_metadata={},\n            )\n            return ToolResult(\n                content=f\"&lt;sub_agent_result&gt;\\n&lt;error&gt;{e!s}&lt;/error&gt;\\n&lt;/sub_agent_result&gt;\",\n                success=False,\n                metadata=error_metadata,\n            )\n        finally:\n            # DEBUG: Log SESSION_STATE after subagent session\n            post_session_state = _SESSION_STATE.get(None)\n            logger.debug(\n                \"[%s] POST-SESSION: _SESSION_STATE=%s, exec_env=%s, exec_env._temp_dir=%s\",\n                agent.name,\n                id(post_session_state) if post_session_state else None,\n                type(post_session_state.exec_env).__name__\n                if post_session_state and post_session_state.exec_env\n                else None,\n                getattr(post_session_state.exec_env, \"_temp_dir\", \"N/A\")\n                if post_session_state and post_session_state.exec_env\n                else None,\n            )\n\n            # Restore parent's depth\n            _PARENT_DEPTH.reset(prev_depth)\n            # Restore parent's session state so next sibling subagent sees it\n            if parent_session_state is not None:\n                _SESSION_STATE.set(parent_session_state)\n\n    return Tool[SubAgentParams, SubAgentMetadata](\n        name=self._name,\n        description=description,\n        parameters=SubAgentParams,\n        executor=sub_agent_executor,  # ty: ignore[invalid-argument-type]\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.compute_task_hash","title":"compute_task_hash","text":"<pre><code>compute_task_hash(\n    init_msgs: str | list[ChatMessage],\n) -&gt; str\n</code></pre> <p>Compute deterministic hash from initial messages for cache identification.</p> <p>Parameters:</p> Name Type Description Default <code>init_msgs</code> <code>str | list[ChatMessage]</code> <p>Either a string prompt or list of ChatMessage objects.</p> required <p>Returns:</p> Type Description <code>str</code> <p>First 12 characters of SHA256 hash (hex) for readability.</p> Source code in <code>src/stirrup/core/cache.py</code> <pre><code>def compute_task_hash(init_msgs: str | list[ChatMessage]) -&gt; str:\n    \"\"\"Compute deterministic hash from initial messages for cache identification.\n\n    Args:\n        init_msgs: Either a string prompt or list of ChatMessage objects.\n\n    Returns:\n        First 12 characters of SHA256 hash (hex) for readability.\n    \"\"\"\n    if isinstance(init_msgs, str):\n        content = init_msgs\n    else:\n        # Serialize messages to JSON for hashing\n        content = json.dumps(\n            [serialize_message(msg) for msg in init_msgs],\n            sort_keys=True,\n            ensure_ascii=True,\n        )\n\n    hash_bytes = hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n    return hash_bytes[:12]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.format_skills_section","title":"format_skills_section","text":"<pre><code>format_skills_section(skills: list[SkillMetadata]) -&gt; str\n</code></pre> <p>Format skills metadata as a system prompt section.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>list[SkillMetadata]</code> <p>List of skill metadata to include</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string for inclusion in system prompt.</p> <code>str</code> <p>Returns empty string if no skills provided.</p> Source code in <code>src/stirrup/skills/skills.py</code> <pre><code>def format_skills_section(skills: list[SkillMetadata]) -&gt; str:\n    \"\"\"Format skills metadata as a system prompt section.\n\n    Args:\n        skills: List of skill metadata to include\n\n    Returns:\n        Formatted string for inclusion in system prompt.\n        Returns empty string if no skills provided.\n\n    \"\"\"\n    if not skills:\n        return \"\"\n\n    lines = [\n        \"## Available Skills\",\n        \"\",\n        \"You have access to the following skills located in the `skills/` directory. \"\n        \"Each skill contains a SKILL.md file with detailed instructions and potentially bundled scripts.\",\n        \"\",\n        \"To use a skill:\",\n        \"1. Read the full instructions: `cat &lt;skill_path&gt;/SKILL.md`\",\n        \"2. Follow the instructions and use any bundled resources as described\",\n        \"\",\n    ]\n    lines.extend([f\"- **{skill.name}**: {skill.description} (`{skill.path}/SKILL.md`)\" for skill in skills])\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent.load_skills_metadata","title":"load_skills_metadata","text":"<pre><code>load_skills_metadata(\n    skills_dir: Path,\n) -&gt; list[SkillMetadata]\n</code></pre> <p>Scan skills directory for SKILL.md files and extract metadata.</p> <p>Parameters:</p> Name Type Description Default <code>skills_dir</code> <code>Path</code> <p>Path to the skills directory</p> required <p>Returns:</p> Type Description <code>list[SkillMetadata]</code> <p>List of SkillMetadata for each valid skill found.</p> <code>list[SkillMetadata]</code> <p>Returns empty list if skills_dir doesn't exist or has no skills.</p> Source code in <code>src/stirrup/skills/skills.py</code> <pre><code>def load_skills_metadata(skills_dir: Path) -&gt; list[SkillMetadata]:\n    \"\"\"Scan skills directory for SKILL.md files and extract metadata.\n\n    Args:\n        skills_dir: Path to the skills directory\n\n    Returns:\n        List of SkillMetadata for each valid skill found.\n        Returns empty list if skills_dir doesn't exist or has no skills.\n\n    \"\"\"\n    if not skills_dir.exists():\n        logger.debug(\"Skills directory does not exist: %s\", skills_dir)\n        return []\n\n    if not skills_dir.is_dir():\n        logger.warning(\"Skills path is not a directory: %s\", skills_dir)\n        return []\n\n    skills: list[SkillMetadata] = []\n\n    for skill_path in skills_dir.iterdir():\n        if not skill_path.is_dir():\n            continue\n\n        skill_md = skill_path / \"SKILL.md\"\n        if not skill_md.exists():\n            logger.debug(\"Skill directory missing SKILL.md: %s\", skill_path)\n            continue\n\n        try:\n            content = skill_md.read_text(encoding=\"utf-8\")\n            metadata = parse_frontmatter(content)\n\n            name = metadata.get(\"name\")\n            description = metadata.get(\"description\")\n\n            if not name or not description:\n                logger.warning(\n                    \"Skill %s missing required frontmatter (name, description)\",\n                    skill_path.name,\n                )\n                continue\n\n            skills.append(\n                SkillMetadata(\n                    name=name,\n                    description=description,\n                    path=f\"skills/{skill_path.name}\",\n                )\n            )\n            logger.debug(\"Loaded skill: %s\", name)\n\n        except Exception as e:\n            logger.warning(\"Failed to load skill %s: %s\", skill_path.name, e)\n\n    return skills\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._num_turns_remaining_msg","title":"_num_turns_remaining_msg","text":"<pre><code>_num_turns_remaining_msg(\n    number_of_turns_remaining: int,\n) -&gt; UserMessage\n</code></pre> <p>Create a user message warning the agent about remaining turns before max_turns is reached.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def _num_turns_remaining_msg(number_of_turns_remaining: int) -&gt; UserMessage:\n    \"\"\"Create a user message warning the agent about remaining turns before max_turns is reached.\"\"\"\n    if number_of_turns_remaining == 1:\n        return UserMessage(content=\"This is the last turn. Please finish the task by calling the finish tool.\")\n    return UserMessage(\n        content=f\"You have {number_of_turns_remaining} turns remaining to complete the task. Please continue. Remember you will need a separate turn to finish the task.\",\n    )\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._handle_text_only_tool_responses","title":"_handle_text_only_tool_responses","text":"<pre><code>_handle_text_only_tool_responses(\n    tool_messages: list[ToolMessage],\n) -&gt; tuple[list[ToolMessage], list[UserMessage]]\n</code></pre> <p>Extract image blocks from tool messages and convert them to user messages for text-only models.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def _handle_text_only_tool_responses(tool_messages: list[ToolMessage]) -&gt; tuple[list[ToolMessage], list[UserMessage]]:\n    \"\"\"Extract image blocks from tool messages and convert them to user messages for text-only models.\"\"\"\n    user_messages: list[UserMessage] = []\n    for tm in tool_messages:\n        if isinstance(tm.content, list):\n            for idx, block in enumerate(tm.content):\n                if isinstance(block, ImageContentBlock):\n                    user_messages.append(\n                        UserMessage(content=[f\"Here is the image for tool call {tm.tool_call_id}\", block]),\n                    )\n                    tm.content[idx] = f\"Done! The User will provide the image for tool call {tm.tool_call_id}\"\n                elif isinstance(block, str):\n                    continue\n                else:\n                    raise NotImplementedError(f\"Unsupported content block: {type(block)}\")\n\n    return tool_messages, user_messages\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._get_total_token_usage","title":"_get_total_token_usage","text":"<pre><code>_get_total_token_usage(\n    messages: list[list[ChatMessage]],\n) -&gt; list[TokenUsage]\n</code></pre> <p>Returns a list of TokenUsage objects aggregated from all AssistantMessage instances across the provided grouped message history.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[list[ChatMessage]]</code> <p>A list where each item is a list of ChatMessage objects representing a segment       or turn group of the conversation history.</p> required <p>Returns:</p> Type Description <code>list[TokenUsage]</code> <p>List of TokenUsage corresponding to each AssistantMessage in the flattened conversation history.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def _get_total_token_usage(messages: list[list[ChatMessage]]) -&gt; list[TokenUsage]:\n    \"\"\"\n    Returns a list of TokenUsage objects aggregated from all AssistantMessage\n    instances across the provided grouped message history.\n\n    Args:\n        messages: A list where each item is a list of ChatMessage objects representing a segment\n                  or turn group of the conversation history.\n\n    Returns:\n        List of TokenUsage corresponding to each AssistantMessage in the flattened conversation history.\n    \"\"\"\n    return [msg.token_usage for msg in chain.from_iterable(messages) if isinstance(msg, AssistantMessage)]\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._get_tool_durations","title":"_get_tool_durations","text":"<pre><code>_get_tool_durations(\n    messages: list[list[ChatMessage]],\n) -&gt; dict[str, list[float]]\n</code></pre> <p>Collect tool execution durations grouped by tool name from message history.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def _get_tool_durations(messages: list[list[ChatMessage]]) -&gt; dict[str, list[float]]:\n    \"\"\"Collect tool execution durations grouped by tool name from message history.\"\"\"\n    durations: dict[str, list[float]] = {}\n    for msg in chain.from_iterable(messages):\n        if isinstance(msg, ToolMessage) and msg.name and msg.tool_duration is not None:\n            durations.setdefault(msg.name, []).append(msg.tool_duration)\n    return durations\n</code></pre>"},{"location":"api/core/agent/#stirrup.core.agent._get_model_speed_stats","title":"_get_model_speed_stats","text":"<pre><code>_get_model_speed_stats(\n    messages: list[list[ChatMessage]], model_slug: str\n) -&gt; dict[str, float | int | str]\n</code></pre> <p>Compute speed stats for this agent's model from AssistantMessages.</p> <p>Returns a flat dict with model_slug, num_calls, output_tokens, duration, e2e_otps. Returns empty dict if no timed messages found.</p> Source code in <code>src/stirrup/core/agent.py</code> <pre><code>def _get_model_speed_stats(messages: list[list[ChatMessage]], model_slug: str) -&gt; dict[str, float | int | str]:\n    \"\"\"Compute speed stats for this agent's model from AssistantMessages.\n\n    Returns a flat dict with model_slug, num_calls, output_tokens, duration, e2e_otps.\n    Returns empty dict if no timed messages found.\n    \"\"\"\n    num_calls = 0\n    output_tokens = 0\n    duration = 0.0\n    for msg in chain.from_iterable(messages):\n        if not isinstance(msg, AssistantMessage):\n            continue\n        if msg.request_start_time is None or msg.request_end_time is None:\n            continue\n        msg_duration = msg.request_end_time - msg.request_start_time\n        if msg_duration &lt;= 0:\n            continue\n        num_calls += 1\n        output_tokens += msg.token_usage.output\n        duration += msg_duration\n    if num_calls == 0:\n        return {}\n    return {\n        \"model_slug\": model_slug,\n        \"num_calls\": num_calls,\n        \"output_tokens\": output_tokens,\n        \"duration\": duration,\n        \"e2e_otps\": output_tokens / duration if duration &gt; 0 else 0.0,\n    }\n</code></pre>"},{"location":"api/core/exceptions/","title":"Exceptions","text":""},{"location":"api/core/exceptions/#stirrup.core.exceptions","title":"stirrup.core.exceptions","text":"<p>Custom exceptions for agent framework.</p>"},{"location":"api/core/exceptions/#stirrup.core.exceptions.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['ContextOverflowError']\n</code></pre>"},{"location":"api/core/exceptions/#stirrup.core.exceptions.ContextOverflowError","title":"ContextOverflowError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when LLM context window is exceeded (max_tokens or length finish_reason).</p>"},{"location":"api/core/models/","title":"Models","text":""},{"location":"api/core/models/#stirrup.core.models","title":"stirrup.core.models","text":""},{"location":"api/core/models/#stirrup.core.models.RESOLUTION_1MP","title":"RESOLUTION_1MP  <code>module-attribute</code>","text":"<pre><code>RESOLUTION_1MP = 1000000\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.RESOLUTION_480P","title":"RESOLUTION_480P  <code>module-attribute</code>","text":"<pre><code>RESOLUTION_480P = 640 * 480\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = [\n    \"Addable\",\n    \"AssistantMessage\",\n    \"AudioContentBlock\",\n    \"BinaryContentBlock\",\n    \"ChatMessage\",\n    \"Content\",\n    \"ContentBlock\",\n    \"EmptyParams\",\n    \"ImageContentBlock\",\n    \"LLMClient\",\n    \"SubAgentMetadata\",\n    \"SystemMessage\",\n    \"TokenUsage\",\n    \"Tool\",\n    \"ToolCall\",\n    \"ToolMessage\",\n    \"ToolProvider\",\n    \"ToolResult\",\n    \"ToolUseCountMetadata\",\n    \"UserMessage\",\n    \"VideoContentBlock\",\n    \"aggregate_metadata\",\n]\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.Base64Bytes","title":"Base64Bytes  <code>module-attribute</code>","text":"<pre><code>Base64Bytes = Annotated[\n    bytes,\n    PlainValidator(_b64_to_bytes),\n    PlainSerializer(_bytes_to_b64, when_used=\"json\"),\n]\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.ContentBlock","title":"ContentBlock","text":"<pre><code>ContentBlock = (\n    ImageContentBlock\n    | VideoContentBlock\n    | AudioContentBlock\n    | str\n)\n</code></pre> <p>Union of all content block types (image, video, audio, or text).</p>"},{"location":"api/core/models/#stirrup.core.models.Content","title":"Content","text":"<pre><code>Content = list[ContentBlock] | str\n</code></pre> <p>Message content: either a plain string or list of mixed content blocks.</p>"},{"location":"api/core/models/#stirrup.core.models.ChatMessage","title":"ChatMessage","text":"<pre><code>ChatMessage = Annotated[\n    SystemMessage\n    | UserMessage\n    | AssistantMessage\n    | ToolMessage,\n    Field(discriminator=role),\n]\n</code></pre> <p>Discriminated union of all message types, automatically parsed based on role field.</p>"},{"location":"api/core/models/#stirrup.core.models.BinaryContentBlock","title":"BinaryContentBlock","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for binary content (images, video, audio) with MIME type validation.</p>"},{"location":"api/core/models/#stirrup.core.models.BinaryContentBlock.mime_type","title":"mime_type  <code>property</code>","text":"<pre><code>mime_type: str\n</code></pre> <p>MIME type for data based on headers.</p>"},{"location":"api/core/models/#stirrup.core.models.BinaryContentBlock.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: str\n</code></pre> <p>File extension for the content (e.g., 'png', 'mp4', 'mp3') without leading dot.</p>"},{"location":"api/core/models/#stirrup.core.models.ImageContentBlock","title":"ImageContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>Image content supporting PNG, JPEG, WebP, PSD formats with automatic downscaling.</p>"},{"location":"api/core/models/#stirrup.core.models.ImageContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(\n    max_pixels: int | None = RESOLUTION_1MP,\n) -&gt; str\n</code></pre> <p>Convert image to base64 data URL, optionally resizing to max pixel count.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, max_pixels: int | None = RESOLUTION_1MP) -&gt; str:\n    \"\"\"Convert image to base64 data URL, optionally resizing to max pixel count.\"\"\"\n    img: Image.Image = Image.open(BytesIO(self.data))\n    if max_pixels is not None and img.width * img.height &gt; max_pixels:\n        tw, th = downscale_image(img.width, img.height, max_pixels)\n        img.thumbnail((tw, th), Image.Resampling.LANCZOS)\n    if img.mode != \"RGB\":\n        img = img.convert(\"RGB\")\n    buf = BytesIO()\n    img.save(buf, format=\"PNG\")\n    return f\"data:image/png;base64,{b64encode(buf.getvalue()).decode()}\"\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.VideoContentBlock","title":"VideoContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>MP4 video content with automatic transcoding and resolution downscaling.</p>"},{"location":"api/core/models/#stirrup.core.models.VideoContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(\n    max_pixels: int | None = RESOLUTION_480P,\n    fps: int | None = None,\n) -&gt; str\n</code></pre> <p>Transcode to MP4 and return base64 data URL.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, max_pixels: int | None = RESOLUTION_480P, fps: int | None = None) -&gt; str:\n    \"\"\"Transcode to MP4 and return base64 data URL.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"moviepy.*\")\n        with NamedTemporaryFile(suffix=\".mp4\") as fin, NamedTemporaryFile(suffix=\".mp4\") as fout:\n            fin.write(self.data)\n            fin.flush()\n            clip = VideoFileClip(fin.name)\n            tw, th = downscale_image(int(clip.w), int(clip.h), max_pixels)\n            clip = clip.with_effects([Resize(new_size=(tw, th))])\n\n            clip.write_videofile(\n                fout.name,\n                codec=\"libx264\",\n                fps=fps,\n                audio=clip.audio is not None,\n                audio_codec=\"aac\",\n                preset=\"veryfast\",\n                logger=None,\n            )\n            clip.close()\n            return f\"data:video/mp4;base64,{b64encode(fout.read()).decode()}\"\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.AudioContentBlock","title":"AudioContentBlock","text":"<p>               Bases: <code>BinaryContentBlock</code></p> <p>Audio content supporting MPEG, WAV, AAC, and other common audio formats.</p>"},{"location":"api/core/models/#stirrup.core.models.AudioContentBlock.to_base64_url","title":"to_base64_url","text":"<pre><code>to_base64_url(bitrate: str = '192k') -&gt; str\n</code></pre> <p>Transcode to MP3 and return base64 data URL.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_base64_url(self, bitrate: str = \"192k\") -&gt; str:\n    \"\"\"Transcode to MP3 and return base64 data URL.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"moviepy.*\")\n        with NamedTemporaryFile(suffix=\".bin\") as fin, NamedTemporaryFile(suffix=\".mp3\") as fout:\n            fin.write(self.data)\n            fin.flush()\n            clip = AudioFileClip(fin.name)\n            clip.write_audiofile(fout.name, codec=\"libmp3lame\", bitrate=bitrate, logger=None)\n            clip.close()\n            return f\"data:audio/mpeg;base64,{b64encode(fout.read()).decode()}\"\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.Addable","title":"Addable","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for types that support aggregation via add.</p>"},{"location":"api/core/models/#stirrup.core.models.TokenUsage","title":"TokenUsage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Token counts for LLM usage.</p> <p>Token terminology: output = reasoning + answer.</p>"},{"location":"api/core/models/#stirrup.core.models.TokenUsage.output","title":"output  <code>property</code>","text":"<pre><code>output: int\n</code></pre> <p>Total output tokens (reasoning + answer).</p>"},{"location":"api/core/models/#stirrup.core.models.TokenUsage.total","title":"total  <code>property</code>","text":"<pre><code>total: int\n</code></pre> <p>Total token count across input, answer, and reasoning.</p>"},{"location":"api/core/models/#stirrup.core.models.TokenUsage.__add__","title":"__add__","text":"<pre><code>__add__(other: TokenUsage) -&gt; TokenUsage\n</code></pre> <p>Add two TokenUsage objects together, summing each field independently.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"TokenUsage\") -&gt; \"TokenUsage\":\n    \"\"\"Add two TokenUsage objects together, summing each field independently.\"\"\"\n    return TokenUsage(\n        input=self.input + other.input,\n        answer=self.answer + other.answer,\n        reasoning=self.reasoning + other.reasoning,\n    )\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.ToolUseCountMetadata","title":"ToolUseCountMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic metadata tracking tool usage count.</p> <p>Implements Addable protocol for aggregation. Use this for tools that only need to track how many times they were called.</p> <p>Subclasses can override add with their own type thanks to Self typing.</p>"},{"location":"api/core/models/#stirrup.core.models.ToolResult","title":"ToolResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result from a tool executor with optional metadata.</p> <p>Generic over metadata type M. M should implement Addable protocol for aggregation support, but this is not enforced at the class level due to Pydantic schema generation limitations.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Content</code> <p>The result content (string, list of content blocks, or images)</p> <code>success</code> <code>bool</code> <p>Whether the tool call was successful. For finish tools, controls if agent terminates.</p> <code>metadata</code> <code>M | None</code> <p>Optional metadata (e.g., usage stats) that implements Addable for aggregation</p>"},{"location":"api/core/models/#stirrup.core.models.EmptyParams","title":"EmptyParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Empty parameter model for tools that don't require parameters.</p>"},{"location":"api/core/models/#stirrup.core.models.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool definition with name, description, parameter schema, and executor function.</p> Generic over <p>P: Parameter model type (Pydantic BaseModel subclass, or EmptyParams for parameterless tools) M: Metadata type (should implement Addable for aggregation; use None for tools without metadata)</p> <p>Tools are simple, stateless callables. For tools requiring lifecycle management (setup/teardown, resource pooling), use a ToolProvider instead.</p> Example with parameters <pre><code>class CalcParams(BaseModel):\n    expression: str\n\ncalc_tool = Tool[CalcParams, None](\n    name=\"calc\",\n    description=\"Evaluate math\",\n    parameters=CalcParams,\n    executor=lambda p: ToolResult(content=str(eval(p.expression))),\n)\n</code></pre> <p>Example without parameters (uses EmptyParams by default):     <pre><code>time_tool = Tool[EmptyParams, None](\n    name=\"time\",\n    description=\"Get current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre></p>"},{"location":"api/core/models/#stirrup.core.models.ToolProvider","title":"ToolProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tool providers with lifecycle management.</p> <p>ToolProviders manage resources (HTTP clients, sandboxes, server connections) and return Tool instances when entering their async context. They implement the async context manager protocol.</p> <p>Use ToolProvider for: - Tools requiring setup/teardown (connections, temp directories) - Tools that return multiple Tool instances (e.g., MCP servers) - Tools with shared state across calls (e.g., HTTP client pooling)</p> Example <p>class MyToolProvider(ToolProvider):     async def aenter(self) -&gt; Tool | list[Tool]:         # Setup resources and return tool(s)         return self._create_tool()</p> <pre><code># __aexit__ is optional - default is no-op\n</code></pre> <p>Agent automatically manages ToolProvider lifecycle via its session() context.</p>"},{"location":"api/core/models/#stirrup.core.models.ToolProvider.__aenter__","title":"__aenter__  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool | list[Tool]\n</code></pre> <p>Enter async context: setup resources and return tool(s).</p> <p>Returns:</p> Type Description <code>Tool | list[Tool]</code> <p>A single Tool instance, or a list of Tool instances for providers</p> <code>Tool | list[Tool]</code> <p>that expose multiple tools (e.g., MCP servers).</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>@abstractmethod\nasync def __aenter__(self) -&gt; \"Tool | list[Tool]\":\n    \"\"\"Enter async context: setup resources and return tool(s).\n\n    Returns:\n        A single Tool instance, or a list of Tool instances for providers\n        that expose multiple tools (e.g., MCP servers).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.ToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit async context: cleanup resources. Default: no-op.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>async def __aexit__(  # noqa: B027\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit async context: cleanup resources. Default: no-op.\"\"\"\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.LLMClient","title":"LLMClient","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the interface for LLM client implementations.</p> <p>Any LLM client must implement this protocol to work with the Agent class. Provides text generation with tool support and model capability inspection.</p>"},{"location":"api/core/models/#stirrup.core.models.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a tool invocation request from the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the tool to invoke</p> <code>arguments</code> <code>str</code> <p>JSON string containing tool parameters</p> <code>tool_call_id</code> <code>str | None</code> <p>Unique identifier for tracking this tool call and its result</p>"},{"location":"api/core/models/#stirrup.core.models.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>System-level instructions and context for the LLM.</p>"},{"location":"api/core/models/#stirrup.core.models.UserMessage","title":"UserMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>User input message to the LLM.</p>"},{"location":"api/core/models/#stirrup.core.models.Reasoning","title":"Reasoning","text":"<p>               Bases: <code>BaseModel</code></p> <p>Extended thinking/reasoning content from models that support chain-of-thought reasoning.</p>"},{"location":"api/core/models/#stirrup.core.models.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/core/models/#stirrup.core.models.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/core/models/#stirrup.core.models.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool execution result returned to the LLM.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['tool']</code> <p>Always \"tool\"</p> <code>content</code> <code>Content</code> <p>The tool result content</p> <code>tool_call_id</code> <code>str | None</code> <p>ID linking this result to the corresponding tool call</p> <code>name</code> <code>str | None</code> <p>Name of the tool that was called</p> <code>args_was_valid</code> <code>bool</code> <p>Whether the tool arguments were valid</p> <code>success</code> <code>bool</code> <p>Whether the tool executed successfully (used by finish tool to control termination)</p>"},{"location":"api/core/models/#stirrup.core.models.ToolMessage.tool_duration","title":"tool_duration  <code>property</code>","text":"<pre><code>tool_duration: float | None\n</code></pre> <p>Tool execution duration in seconds.</p>"},{"location":"api/core/models/#stirrup.core.models.SubAgentMetadata","title":"SubAgentMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata from sub-agent execution including token usage, message history, and child run metadata.</p> <p>Implements Addable protocol to support aggregation across multiple subagent calls.</p>"},{"location":"api/core/models/#stirrup.core.models.SubAgentMetadata.__add__","title":"__add__","text":"<pre><code>__add__(other: SubAgentMetadata) -&gt; SubAgentMetadata\n</code></pre> <p>Combine metadata from multiple subagent calls.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def __add__(self, other: \"SubAgentMetadata\") -&gt; \"SubAgentMetadata\":\n    \"\"\"Combine metadata from multiple subagent calls.\"\"\"\n    # Concatenate message histories\n    combined_history = self.message_history + other.message_history\n    # Merge run metadata (concatenate lists per key, keep last for non-list internal keys)\n    combined_meta: dict[str, Any] = dict(self.run_metadata)\n    for key, value in other.run_metadata.items():\n        if key in combined_meta and isinstance(combined_meta[key], list) and isinstance(value, list):\n            combined_meta[key] = combined_meta[key] + value\n        else:\n            combined_meta[key] = value\n    return SubAgentMetadata(\n        message_history=combined_history,\n        run_metadata=combined_meta,\n    )\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models._bytes_to_b64","title":"_bytes_to_b64","text":"<pre><code>_bytes_to_b64(v: bytes) -&gt; str\n</code></pre> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def _bytes_to_b64(v: bytes) -&gt; str:\n    return base64.b64encode(v).decode(\"ascii\")\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models._b64_to_bytes","title":"_b64_to_bytes","text":"<pre><code>_b64_to_bytes(v: bytes | str) -&gt; bytes\n</code></pre> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def _b64_to_bytes(v: bytes | str) -&gt; bytes:\n    if isinstance(v, bytes):\n        return v\n    if isinstance(v, str):\n        return base64.b64decode(v.encode(\"ascii\"))\n    raise TypeError(\"Invalid bytes value\")\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.downscale_image","title":"downscale_image","text":"<pre><code>downscale_image(\n    w: int, h: int, max_pixels: int | None = 1000000\n) -&gt; tuple[int, int]\n</code></pre> <p>Downscale image dimensions to fit within max pixel count while maintaining aspect ratio.</p> <p>Returns even dimensions with minimum 2x2 size.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def downscale_image(w: int, h: int, max_pixels: int | None = 1_000_000) -&gt; tuple[int, int]:\n    \"\"\"Downscale image dimensions to fit within max pixel count while maintaining aspect ratio.\n\n    Returns even dimensions with minimum 2x2 size.\n    \"\"\"\n    s = 1.0 if max_pixels is None or w * h &lt;= max_pixels else sqrt(max_pixels / (w * h))\n    nw, nh = int(w * s) // 2 * 2, int(h * s) // 2 * 2\n    return max(nw, 2), max(nh, 2)\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models._aggregate_list","title":"_aggregate_list","text":"<pre><code>_aggregate_list(metadata_list: list[T]) -&gt; T | None\n</code></pre> <p>Aggregate a list of metadata using add.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def _aggregate_list[T: Addable](metadata_list: list[T]) -&gt; T | None:\n    \"\"\"Aggregate a list of metadata using __add__.\"\"\"\n    if not metadata_list:\n        return None\n    aggregated = metadata_list[0]\n    for m in metadata_list[1:]:\n        aggregated = aggregated + m\n    return aggregated\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.to_json_serializable","title":"to_json_serializable","text":"<pre><code>to_json_serializable(value: object) -&gt; object\n</code></pre> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def to_json_serializable(value: object) -&gt; object:\n    # None and JSON primitives\n    if value is None or isinstance(value, str | int | bool):\n        return value\n\n    # Floats need special handling for nan/inf\n    if isinstance(value, float):\n        if isnan(value) or isinf(value):\n            raise ValueError(f\"Cannot serialize {value} to JSON\")\n        return value\n\n    # Pydantic models\n    if isinstance(value, BaseModel):\n        return value.model_dump(mode=\"json\")\n\n    # Common non-serializable types\n    if isinstance(value, datetime | date | time):\n        return value.isoformat()\n\n    if isinstance(value, timedelta):\n        return value.total_seconds()\n\n    if isinstance(value, Decimal):\n        return float(value)\n\n    if isinstance(value, dict):\n        return {k: to_json_serializable(v) for k, v in value.items()}\n\n    if isinstance(value, list | tuple | set | frozenset):\n        return [to_json_serializable(v) for v in value]\n\n    # We have not implemented other cases (e.g. Bytes, Enum, etc.)\n    raise TypeError(f\"Cannot serialize {type(value).__name__} to JSON: {value!r}\")\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models._collect_all_token_usage","title":"_collect_all_token_usage","text":"<pre><code>_collect_all_token_usage(result: dict) -&gt; TokenUsage\n</code></pre> <p>Recursively collect all token_usage from a flattened aggregate_metadata result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>The flattened dict from aggregate_metadata (before JSON serialization)</p> required <p>Returns:</p> Type Description <code>TokenUsage</code> <p>Combined TokenUsage from all entries (direct and nested sub-agents)</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def _collect_all_token_usage(result: dict) -&gt; \"TokenUsage\":\n    \"\"\"Recursively collect all token_usage from a flattened aggregate_metadata result.\n\n    Args:\n        result: The flattened dict from aggregate_metadata (before JSON serialization)\n\n    Returns:\n        Combined TokenUsage from all entries (direct and nested sub-agents)\n    \"\"\"\n    total = TokenUsage()\n\n    for key, value in result.items():\n        if key == \"token_usage\" and isinstance(value, TokenUsage):\n            # Direct token_usage at this level\n            total = total + value\n        elif isinstance(value, dict):\n            # This could be a sub-agent's tool dict - check for nested token_usage\n            nested_token_usage = value.get(\"token_usage\")\n            if isinstance(nested_token_usage, TokenUsage):\n                total = total + nested_token_usage\n\n    return total\n</code></pre>"},{"location":"api/core/models/#stirrup.core.models.aggregate_metadata","title":"aggregate_metadata","text":"<pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: Literal[True] = True,\n) -&gt; object\n</code></pre><pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: Literal[False] = ...,\n) -&gt; dict\n</code></pre> <pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: bool = True,\n) -&gt; dict | object\n</code></pre> <p>Aggregate metadata lists and flatten sub-agents into a single-level dict with hierarchical keys.</p> <p>For entries with nested run_metadata (e.g., SubAgentMetadata), flattens sub-agents using dot notation. Each sub-agent's value is a dict mapping its direct tool names to their aggregated metadata (excluding nested sub-agent data, which gets its own top-level key).</p> <p>At the root level, token_usage is rolled up to include all sub-agent token usage.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_dict</code> <code>dict[str, list[Any]]</code> <p>Dict mapping names (tools or agents) to lists of metadata instances</p> required <code>prefix</code> <code>str</code> <p>Key prefix for nested calls (used internally for recursion)</p> <code>''</code> <p>Returns:</p> Name Type Description <code>dict | object</code> <p>Flat dict with dot-notation keys for sub-agents.</p> <code>Example</code> <code>dict | object</code> <p>{ \"token_usage\": , \"web_browsing_sub_agent\": {\"web_search\": , \"token_usage\": }, \"web_browsing_sub_agent.web_fetch_sub_agent\": {\"fetch_web_page\": , \"token_usage\": } <code>dict | object</code> <p>}</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def aggregate_metadata(\n    metadata_dict: dict[str, list[Any]], prefix: str = \"\", return_json_serializable: bool = True\n) -&gt; dict | object:\n    \"\"\"Aggregate metadata lists and flatten sub-agents into a single-level dict with hierarchical keys.\n\n    For entries with nested run_metadata (e.g., SubAgentMetadata), flattens sub-agents using dot notation.\n    Each sub-agent's value is a dict mapping its direct tool names to their aggregated metadata\n    (excluding nested sub-agent data, which gets its own top-level key).\n\n    At the root level, token_usage is rolled up to include all sub-agent token usage.\n\n    Args:\n        metadata_dict: Dict mapping names (tools or agents) to lists of metadata instances\n        prefix: Key prefix for nested calls (used internally for recursion)\n\n    Returns:\n        Flat dict with dot-notation keys for sub-agents.\n        Example: {\n            \"token_usage\": &lt;combined from all agents&gt;,\n            \"web_browsing_sub_agent\": {\"web_search\": &lt;aggregated&gt;, \"token_usage\": &lt;aggregated&gt;},\n            \"web_browsing_sub_agent.web_fetch_sub_agent\": {\"fetch_web_page\": &lt;aggregated&gt;, \"token_usage\": &lt;aggregated&gt;}\n        }\n    \"\"\"\n    result: dict = {}\n\n    # First pass: aggregate all entries in this level (skip internal keys prefixed with _)\n    aggregated_level: dict = {}\n    for name, metadata_list in metadata_dict.items():\n        if name.startswith(\"_\") or not metadata_list:\n            continue\n        aggregated_level[name] = _aggregate_list(metadata_list)\n\n    # Second pass: separate nested sub-agents from direct tools, and recurse\n    direct_tools: dict = {}\n    for name, aggregated in aggregated_level.items():\n        if hasattr(aggregated, \"run_metadata\") and isinstance(aggregated.run_metadata, dict):\n            # This is a sub-agent - recurse into it\n            full_key = f\"{prefix}.{name}\" if prefix else name\n            nested = aggregate_metadata(aggregated.run_metadata, prefix=full_key, return_json_serializable=False)\n            result.update(nested)\n        else:\n            # This is a direct tool/metadata - keep it at this level\n            direct_tools[name] = aggregated\n\n    # Store direct tools under the current prefix\n    if prefix:\n        result[prefix] = direct_tools\n    else:\n        # At root level, merge direct tools into result\n        result.update(direct_tools)\n\n    # At root level, roll up all token_usage from sub-agents\n    if not prefix:\n        total_token_usage = _collect_all_token_usage(result)\n        if total_token_usage.total &gt; 0:\n            result[\"token_usage\"] = [total_token_usage]\n\n    if return_json_serializable:\n        # Convert all Pydantic models to JSON-serializable dicts\n        return to_json_serializable(result)\n    return result\n</code></pre>"},{"location":"api/tools/","title":"Tools Module","text":"<p>The <code>stirrup.tools</code> module provides tools, tool providers, and the default tool set.</p>"},{"location":"api/tools/#default_tools","title":"DEFAULT_TOOLS","text":"<p>The standard set of tool providers included with every agent (unless overridden):</p> <pre><code>from stirrup.tools import DEFAULT_TOOLS\n\n# DEFAULT_TOOLS contains:\n# - LocalCodeExecToolProvider() \u2192 provides \"code_exec\" tool\n# - WebToolProvider() \u2192 provides \"web_fetch\" and \"web_search\" tools\n</code></pre>"},{"location":"api/tools/#built-in-tools","title":"Built-in Tools","text":"<p>Simple tools that don't require lifecycle management:</p>"},{"location":"api/tools/#stirrup.tools.calculator.CALCULATOR_TOOL","title":"stirrup.tools.calculator.CALCULATOR_TOOL  <code>module-attribute</code>","text":"<pre><code>CALCULATOR_TOOL: Tool[\n    CalculatorParams, ToolUseCountMetadata\n] = Tool[CalculatorParams, ToolUseCountMetadata](\n    name=\"calculator\",\n    description=\"Evaluate mathematical expressions. Supports basic arithmetic operations (+, -, *, /, **, %, //).\",\n    parameters=CalculatorParams,\n    executor=calculator_executor,\n)\n</code></pre>"},{"location":"api/tools/#stirrup.tools.finish.SIMPLE_FINISH_TOOL","title":"stirrup.tools.finish.SIMPLE_FINISH_TOOL  <code>module-attribute</code>","text":"<pre><code>SIMPLE_FINISH_TOOL: Tool[\n    FinishParams, ToolUseCountMetadata\n] = Tool[FinishParams, ToolUseCountMetadata](\n    name=FINISH_TOOL_NAME,\n    description=\"Signal task completion with a reason. Use when the task is finished or cannot proceed further. Note that you will need a separate turn to finish.\",\n    parameters=FinishParams,\n    executor=_validating_finish_executor,\n)\n</code></pre>"},{"location":"api/tools/#stirrup.tools.finish.FinishParams","title":"stirrup.tools.finish.FinishParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Explanation for why the task is complete or cannot proceed.</p>"},{"location":"api/tools/#stirrup.tools.finish.FinishParams.reason","title":"reason  <code>instance-attribute</code>","text":"<pre><code>reason: Annotated[\n    str, Field(description=\"Reason for finishing.\")\n]\n</code></pre>"},{"location":"api/tools/#stirrup.tools.finish.FinishParams.paths","title":"paths  <code>instance-attribute</code>","text":"<pre><code>paths: Annotated[\n    list[str],\n    Field(\n        description=\"List of file paths created or modified. Do not include directories, only files.\"\n    ),\n]\n</code></pre>"},{"location":"api/tools/#tool-providers","title":"Tool Providers","text":"<p>Tool providers for lifecycle-managed tools:</p> <ul> <li>Web Tools - <code>WebToolProvider</code> for web fetch and search</li> <li>Code Execution - <code>LocalCodeExecToolProvider</code>, <code>DockerCodeExecToolProvider</code>, <code>E2BCodeExecToolProvider</code></li> <li>Browser Use - <code>BrowserUseToolProvider</code> for browser automation</li> <li>MCP - <code>MCPToolProvider</code> for MCP server integration</li> <li>View Image - <code>ViewImageToolProvider</code> for viewing images</li> </ul>"},{"location":"api/tools/browser-use/","title":"Browser Use Tool Provider","text":"<p>The <code>BrowserUseToolProvider</code> provides browser automation capabilities using the browser-use library.</p>"},{"location":"api/tools/browser-use/#installation","title":"Installation","text":"<pre><code>pip install 'stirrup[browser]'  # or: uv add 'stirrup[browser]'\n</code></pre>"},{"location":"api/tools/browser-use/#prerequisites","title":"Prerequisites","text":"<p>Local browser (default):</p> <pre><code># Install Chromium browser\nuvx browser-use install\n</code></pre> <p>Cloud browser (optional):</p> <p>For cloud-hosted browser sessions, set the <code>BROWSER_USE_API_KEY</code> environment variable:</p> <pre><code>export BROWSER_USE_API_KEY=your-api-key-here\n</code></pre> <p>Get your API key from the Browser Use Cloud dashboard.</p>"},{"location":"api/tools/browser-use/#quick-start","title":"Quick Start","text":"<pre><code>import asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import DEFAULT_TOOLS\nfrom stirrup.tools.browser_use import BrowserUseToolProvider\n\n\nasync def main() -&gt; None:\n    \"\"\"Run browser automation example.\"\"\"\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    browser_provider = BrowserUseToolProvider(\n        headless=False,  # Set to True for headless mode\n    )\n\n    agent = Agent(\n        client=client,\n        name=\"browser_agent\",\n        tools=[*DEFAULT_TOOLS, browser_provider],\n        system_prompt=(\n            \"You are a web automation assistant. Use the browser tools to complete tasks. \"\n            \"Always start by taking a snapshot to see the current page state and element indices. \"\n            \"Use the indices from the snapshot when clicking or typing.\"\n        ),\n    )\n\n    async with agent.session(output_dir=\"output/browser_use_example\") as session:\n        _finish_params, _history, _metadata = await session.run(\n            \"Go to artificial analysis and select GPT-5 (high) on the AA Index score\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/tools/browser-use/#available-tools","title":"Available Tools","text":"<p>When you add <code>BrowserUseToolProvider</code> to your agent, it exposes the following tools (all prefixed with <code>browser_</code> by default):</p>"},{"location":"api/tools/browser-use/#navigation","title":"Navigation","text":"Tool Description <code>browser_search</code> Search using Google, DuckDuckGo, or Bing <code>browser_navigate</code> Navigate to a URL (optionally in new tab) <code>browser_go_back</code> Go back in browser history <code>browser_wait</code> Wait for specified seconds (1-30)"},{"location":"api/tools/browser-use/#page-interaction","title":"Page Interaction","text":"Tool Description <code>browser_click</code> Click an element by index <code>browser_input_text</code> Type text into a form field <code>browser_scroll</code> Scroll up or down <code>browser_find_text</code> Find and scroll to specific text <code>browser_send_keys</code> Send keyboard keys (Enter, Tab, etc.)"},{"location":"api/tools/browser-use/#javascript","title":"JavaScript","text":"Tool Description <code>browser_evaluate_js</code> Execute custom JavaScript code"},{"location":"api/tools/browser-use/#tab-management","title":"Tab Management","text":"Tool Description <code>browser_switch_tab</code> Switch to a different tab by index"},{"location":"api/tools/browser-use/#content-extraction","title":"Content Extraction","text":"Tool Description <code>browser_snapshot</code> Get accessibility tree with element indices <code>browser_screenshot</code> Take a screenshot of the page <code>browser_get_url</code> Get current page URL"},{"location":"api/tools/browser-use/#workflow","title":"Workflow","text":"<p>The typical workflow for browser automation:</p> <ol> <li>Navigate to a page using <code>browser_navigate</code> or <code>browser_search</code></li> <li>Snapshot the page using <code>browser_snapshot</code> to see elements and their indices</li> <li>Interact with elements using <code>browser_click</code>, <code>browser_input_text</code>, etc.</li> <li>Repeat snapshot and interaction as needed</li> </ol> <p>The snapshot returns an accessibility tree showing interactive elements with numerical indices that you reference in other tools.</p>"},{"location":"api/tools/browser-use/#configuration","title":"Configuration","text":""},{"location":"api/tools/browser-use/#browserusetoolprovider","title":"BrowserUseToolProvider","text":""},{"location":"api/tools/browser-use/#stirrup.tools.browser_use.BrowserUseToolProvider","title":"stirrup.tools.browser_use.BrowserUseToolProvider","text":"<pre><code>BrowserUseToolProvider(\n    *,\n    headless: bool = True,\n    disable_security: bool = False,\n    executable_path: str | None = None,\n    cdp_url: str | None = None,\n    use_cloud: bool = False,\n    tool_prefix: str = \"browser\",\n    extra_args: list[str] | None = None,\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code></p> <p>Browser automation tool provider using browser-use library.</p> <p>Provides tools for: - Navigation: search, navigate, go_back, wait - Page Interaction: click, input_text, scroll, find_text, send_keys - JavaScript: evaluate_js - Tab Management: switch_tab - Content Extraction: snapshot, screenshot, get_url</p> Example <p>from stirrup.tools.browser_use import BrowserUseToolProvider</p> <p>agent = Agent(     client=client,     name=\"browser_agent\",     tools=[BrowserUseToolProvider(headless=False)], )</p> <p>async with agent.session() as session:     await session.run(\"Navigate to example.com and click the first link\")</p> <p>Initialize BrowserUseToolProvider.</p> <p>Parameters:</p> Name Type Description Default <code>headless</code> <code>bool</code> <p>Run browser in headless mode (default: True)</p> <code>True</code> <code>disable_security</code> <code>bool</code> <p>Disable browser security features (default: False)</p> <code>False</code> <code>executable_path</code> <code>str | None</code> <p>Path to Chrome/Chromium executable</p> <code>None</code> <code>cdp_url</code> <code>str | None</code> <p>Chrome DevTools Protocol URL for remote connection</p> <code>None</code> <code>use_cloud</code> <code>bool</code> <p>Use Browser Use cloud browser (requires BROWSER_USE_API_KEY env var)</p> <code>False</code> <code>tool_prefix</code> <code>str</code> <p>Prefix for tool names (default: \"browser\")</p> <code>'browser'</code> <code>extra_args</code> <code>list[str] | None</code> <p>Additional Chromium command line arguments</p> <code>None</code> Source code in <code>src/stirrup/tools/browser_use.py</code> <pre><code>def __init__(\n    self,\n    *,\n    headless: bool = True,\n    disable_security: bool = False,\n    executable_path: str | None = None,\n    cdp_url: str | None = None,\n    use_cloud: bool = False,\n    tool_prefix: str = \"browser\",\n    extra_args: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize BrowserUseToolProvider.\n\n    Args:\n        headless: Run browser in headless mode (default: True)\n        disable_security: Disable browser security features (default: False)\n        executable_path: Path to Chrome/Chromium executable\n        cdp_url: Chrome DevTools Protocol URL for remote connection\n        use_cloud: Use Browser Use cloud browser (requires BROWSER_USE_API_KEY env var)\n        tool_prefix: Prefix for tool names (default: \"browser\")\n        extra_args: Additional Chromium command line arguments\n\n    \"\"\"\n    self._headless = headless\n    self._disable_security = disable_security\n    self._executable_path = executable_path\n    self._cdp_url = cdp_url\n    self._use_cloud = use_cloud\n    self._tool_prefix = tool_prefix\n    self._extra_args = extra_args\n\n    self._session: BrowserSession | None = None\n</code></pre>"},{"location":"api/tools/browser-use/#stirrup.tools.browser_use.BrowserUseToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; list[Tool[Any, Any]]\n</code></pre> <p>Enter async context: start browser and return tools.</p> Source code in <code>src/stirrup/tools/browser_use.py</code> <pre><code>async def __aenter__(self) -&gt; list[Tool[Any, Any]]:\n    \"\"\"Enter async context: start browser and return tools.\"\"\"\n    if self._use_cloud and not os.environ.get(\"BROWSER_USE_API_KEY\"):\n        raise ValueError(\n            \"BROWSER_USE_API_KEY environment variable is required when use_cloud=True. \"\n            \"Get your API key from https://cloud.browser-use.com\"\n        )\n    self._session = BrowserSession(  # type: ignore[call-overload]\n        headless=self._headless,\n        disable_security=self._disable_security,\n        executable_path=self._executable_path,\n        cdp_url=self._cdp_url,\n        use_cloud=self._use_cloud,\n        args=self._extra_args,\n    )\n    await self._session.start()\n    return self._build_tools()\n</code></pre>"},{"location":"api/tools/browser-use/#stirrup.tools.browser_use.BrowserUseToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit async context: close browser.</p> Source code in <code>src/stirrup/tools/browser_use.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit async context: close browser.\"\"\"\n    if self._session:\n        await self._session.stop()\n        self._session = None\n</code></pre>"},{"location":"api/tools/browser-use/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>headless</code> <code>bool</code> <code>True</code> Run browser without visible window <code>disable_security</code> <code>bool</code> <code>False</code> Disable browser security features <code>executable_path</code> <code>str \\| None</code> <code>None</code> Path to Chrome/Chromium <code>cdp_url</code> <code>str \\| None</code> <code>None</code> CDP URL for remote browser <code>tool_prefix</code> <code>str</code> <code>\"browser\"</code> Prefix for tool names <code>extra_args</code> <code>list[str] \\| None</code> <code>None</code> Extra Chromium args"},{"location":"api/tools/browser-use/#cloud-browser","title":"Cloud Browser","text":"<p>To use a cloud-hosted browser:</p> <pre><code>import os\n\nbrowser_provider = BrowserUseToolProvider(\n    use_cloud=True,  # Requires BROWSER_USE_API_KEY env var\n)\n</code></pre> <p>This requires setting the <code>BROWSER_USE_API_KEY</code> environment variable.</p>"},{"location":"api/tools/browser-use/#example","title":"Example","text":"<p>See the full example at <code>examples/browser_use_example.py</code>.</p>"},{"location":"api/tools/code_backends/","title":"Code Execution Backends","text":"<p>The <code>stirrup.tools.code_backends</code> module provides code execution backends.</p>"},{"location":"api/tools/code_backends/#codeexectoolprovider-base-class","title":"CodeExecToolProvider (Base Class)","text":""},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider","title":"stirrup.tools.code_backends.CodeExecToolProvider","text":"<pre><code>CodeExecToolProvider(\n    *, allowed_commands: list[str] | None = None\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code>, <code>ABC</code></p> <p>Abstract base class for code execution tool providers.</p> <p>CodeExecToolProvider is a ToolProvider that manages code execution environments (sandboxes, containers, local temp directories) and returns a code_exec Tool.</p> <p>Subclasses must implement: - aenter(): Initialize environment and return the code_exec tool - aexit(): Cleanup the execution environment - run_command(): Execute a command and return raw result - read_file_bytes(): Read file content as bytes from the environment - write_file_bytes(): Write bytes to a file in the environment</p> <p>Default implementations are provided for: - save_output_files(): Save files to local dir or another exec env (uses primitives) - upload_files(): Upload files from local or another exec env (uses primitives)</p> <p>All code execution providers support an optional allowlist of command patterns. If provided, only commands matching at least one pattern are allowed. If None, all commands are allowed.</p> Usage with Agent <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>client = ChatCompletionsClient(model=\"gpt-5\") agent = Agent(     client=client,     name=\"assistant\",     tools=[LocalCodeExecToolProvider(), CALCULATOR_TOOL], )</p> <p>Initialize execution environment with optional command allowlist.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_commands</code> <code>list[str] | None</code> <p>Optional list of regex patterns. If provided, only              commands matching at least one pattern are allowed.              If None, all commands are allowed.</p> <code>None</code> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def __init__(self, *, allowed_commands: list[str] | None = None) -&gt; None:\n    \"\"\"Initialize execution environment with optional command allowlist.\n\n    Args:\n        allowed_commands: Optional list of regex patterns. If provided, only\n                         commands matching at least one pattern are allowed.\n                         If None, all commands are allowed.\n\n    \"\"\"\n    self._allowed_commands = allowed_commands\n    self._compiled_allowed: list[re.Pattern[str]] | None = None\n    if allowed_commands is not None:\n        self._compiled_allowed = [re.compile(p) for p in allowed_commands]\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.run_command","title":"run_command  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>run_command(\n    cmd: str, *, timeout: int = SHELL_TIMEOUT\n) -&gt; CommandResult\n</code></pre> <p>Execute a shell command and return raw CommandResult.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def run_command(self, cmd: str, *, timeout: int = SHELL_TIMEOUT) -&gt; CommandResult:\n    \"\"\"Execute a shell command and return raw CommandResult.\"\"\"\n    ...\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.read_file_bytes","title":"read_file_bytes  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>read_file_bytes(path: str) -&gt; bytes\n</code></pre> <p>Read file content as bytes from this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path within this execution environment (relative or absolute   within the env's working directory).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>File contents as bytes.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file does not exist.</p> <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def read_file_bytes(self, path: str) -&gt; bytes:\n    \"\"\"Read file content as bytes from this execution environment.\n\n    Args:\n        path: File path within this execution environment (relative or absolute\n              within the env's working directory).\n\n    Returns:\n        File contents as bytes.\n\n    Raises:\n        FileNotFoundError: If file does not exist.\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.write_file_bytes","title":"write_file_bytes  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>write_file_bytes(path: str, content: bytes) -&gt; None\n</code></pre> <p>Write bytes to a file in this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Destination path within this execution environment.</p> required <code>content</code> <code>bytes</code> <p>File contents to write.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>@abstractmethod\nasync def write_file_bytes(self, path: str, content: bytes) -&gt; None:\n    \"\"\"Write bytes to a file in this execution environment.\n\n    Args:\n        path: Destination path within this execution environment.\n        content: File contents to write.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.save_output_files","title":"save_output_files  <code>async</code>","text":"<pre><code>save_output_files(\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: CodeExecToolProvider | None = None,\n) -&gt; SaveOutputFilesResult\n</code></pre> <p>Save files from this execution environment to a destination.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths in this execution environment to save.</p> required <code>output_dir</code> <code>Path | str</code> <p>Directory path to save files to.</p> required <code>dest_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, output_dir is interpreted as a path within dest_env       (cross-environment transfer). If None, output_dir is a local       filesystem path.</p> <code>None</code> <p>Returns:</p> Type Description <code>SaveOutputFilesResult</code> <p>SaveOutputFilesResult containing lists of saved files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>async def save_output_files(\n    self,\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: \"CodeExecToolProvider | None\" = None,\n) -&gt; SaveOutputFilesResult:\n    \"\"\"Save files from this execution environment to a destination.\n\n    Args:\n        paths: List of file paths in this execution environment to save.\n        output_dir: Directory path to save files to.\n        dest_env: If provided, output_dir is interpreted as a path within dest_env\n                  (cross-environment transfer). If None, output_dir is a local\n                  filesystem path.\n\n    Returns:\n        SaveOutputFilesResult containing lists of saved files and any failures.\n\n    \"\"\"\n    result = SaveOutputFilesResult()\n    output_dir_str = str(output_dir)\n\n    for source_path in paths:\n        try:\n            content = await self.read_file_bytes(source_path)\n            filename = Path(source_path).name\n            dest_path = f\"{output_dir_str}/{filename}\"\n\n            if dest_env:\n                # Transfer to another exec env (cross-environment)\n                logger.debug(\n                    \"CROSS-ENV TRANSFER: %s (%d bytes) -&gt; %s (dest_env: %s)\",\n                    source_path,\n                    len(content),\n                    dest_path,\n                    type(dest_env).__name__,\n                )\n                await dest_env.write_file_bytes(dest_path, content)\n                result.saved.append(SavedFile(source_path, Path(dest_path), len(content)))\n            else:\n                # Save to local filesystem\n                output_path = Path(output_dir) / filename\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n                logger.debug(\n                    \"SAVE TO LOCAL: %s (%d bytes) -&gt; %s\",\n                    source_path,\n                    len(content),\n                    output_path,\n                )\n                output_path.write_bytes(content)\n                result.saved.append(SavedFile(source_path, output_path, len(content)))\n        except Exception as e:\n            logger.debug(\"TRANSFER FAILED: %s -&gt; %s: %s\", source_path, output_dir_str, e)\n            result.failed[source_path] = str(e)\n\n    return result\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.upload_files","title":"upload_files  <code>async</code>","text":"<pre><code>upload_files(\n    *paths: Path | str,\n    source_env: CodeExecToolProvider | None = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult\n</code></pre> <p>Upload files to this execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>*paths</code> <code>Path | str</code> <p>File or directory paths to upload. If source_env is None, these     are local filesystem paths. If source_env is provided, these are     paths within source_env (cross-environment transfer).</p> <code>()</code> <code>source_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, paths are within source_env. If None, paths are         local filesystem paths.</p> <code>None</code> <code>dest_dir</code> <code>str | None</code> <p>Destination directory in this environment.       If None, uses the environment's working directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>UploadFilesResult</code> <p>UploadFilesResult containing lists of uploaded files and any failures.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>async def upload_files(\n    self,\n    *paths: Path | str,\n    source_env: \"CodeExecToolProvider | None\" = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult:\n    \"\"\"Upload files to this execution environment.\n\n    Args:\n        *paths: File or directory paths to upload. If source_env is None, these\n                are local filesystem paths. If source_env is provided, these are\n                paths within source_env (cross-environment transfer).\n        source_env: If provided, paths are within source_env. If None, paths are\n                    local filesystem paths.\n        dest_dir: Destination directory in this environment.\n                  If None, uses the environment's working directory.\n\n    Returns:\n        UploadFilesResult containing lists of uploaded files and any failures.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n\n    \"\"\"\n    result = UploadFilesResult()\n    dest_dir_str = dest_dir or \"\"\n\n    for path in paths:\n        path_str = str(path)\n        try:\n            if source_env:\n                # Cross-environment transfer: read from source_env\n                # Check if it's a directory first\n                if await source_env.is_directory(path_str):\n                    # Handle directory recursively\n                    # Preserve directory name when dest_dir not specified\n                    dir_name = Path(path_str).name\n                    files = await source_env.list_files(path_str)\n                    for rel_file_path in files:\n                        src_file_path = f\"{path_str}/{rel_file_path}\"\n                        # If dest_dir specified, put files directly there\n                        # Otherwise, preserve the source directory name\n                        if dest_dir_str:\n                            dest_path = f\"{dest_dir_str}/{rel_file_path}\"\n                        else:\n                            dest_path = f\"{dir_name}/{rel_file_path}\"\n                        content = await source_env.read_file_bytes(src_file_path)\n                        logger.debug(\n                            \"UPLOAD CROSS-ENV (dir): %s (%d bytes) from %s -&gt; %s\",\n                            src_file_path,\n                            len(content),\n                            type(source_env).__name__,\n                            dest_path,\n                        )\n                        await self.write_file_bytes(dest_path, content)\n                        result.uploaded.append(UploadedFile(Path(src_file_path), dest_path, len(content)))\n                else:\n                    # Single file transfer\n                    content = await source_env.read_file_bytes(path_str)\n                    filename = Path(path_str).name\n                    dest_path = f\"{dest_dir_str}/{filename}\" if dest_dir_str else filename\n                    logger.debug(\n                        \"UPLOAD CROSS-ENV: %s (%d bytes) from %s -&gt; %s\",\n                        path_str,\n                        len(content),\n                        type(source_env).__name__,\n                        dest_path,\n                    )\n                    await self.write_file_bytes(dest_path, content)\n                    result.uploaded.append(UploadedFile(Path(path_str), dest_path, len(content)))\n            else:\n                # Local filesystem upload - must be handled by subclass\n                # This is a fallback that reads from local fs and writes to env\n                local_path = Path(path)\n                if local_path.is_dir():\n                    # Handle directory recursively\n                    for file_path in local_path.rglob(\"*\"):\n                        if file_path.is_file():\n                            rel_path = file_path.relative_to(local_path)\n                            dest_path = f\"{dest_dir_str}/{rel_path}\" if dest_dir_str else str(rel_path)\n                            content = file_path.read_bytes()\n                            logger.debug(\n                                \"UPLOAD FROM LOCAL: %s (%d bytes) -&gt; %s\",\n                                file_path,\n                                len(content),\n                                dest_path,\n                            )\n                            await self.write_file_bytes(dest_path, content)\n                            result.uploaded.append(UploadedFile(file_path, dest_path, len(content)))\n                else:\n                    filename = local_path.name\n                    dest_path = f\"{dest_dir_str}/{filename}\" if dest_dir_str else filename\n                    content = local_path.read_bytes()\n                    logger.debug(\n                        \"UPLOAD FROM LOCAL: %s (%d bytes) -&gt; %s\",\n                        local_path,\n                        len(content),\n                        dest_path,\n                    )\n                    await self.write_file_bytes(dest_path, content)\n                    result.uploaded.append(UploadedFile(local_path, dest_path, len(content)))\n        except Exception as e:\n            logger.debug(\"UPLOAD FAILED: %s -&gt; %s: %s\", path_str, dest_dir_str, e)\n            result.failed[path_str] = str(e)\n\n    return result\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.get_code_exec_tool","title":"get_code_exec_tool","text":"<pre><code>get_code_exec_tool(\n    *,\n    name: str = \"code_exec\",\n    description: str | None = None,\n) -&gt; Tool[CodeExecutionParams, ToolUseCountMetadata]\n</code></pre> <p>Create a code execution tool for this environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name</p> <code>'code_exec'</code> <code>description</code> <code>str | None</code> <p>Tool description</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool[CodeExecutionParams, ToolUseCountMetadata]</code> <p>Tool[CodeExecutionParams] that executes commands in this environment</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def get_code_exec_tool(\n    self,\n    *,\n    name: str = \"code_exec\",\n    description: str | None = None,\n) -&gt; Tool[CodeExecutionParams, ToolUseCountMetadata]:\n    \"\"\"Create a code execution tool for this environment.\n\n    Args:\n        name: Tool name\n        description: Tool description\n\n    Returns:\n        Tool[CodeExecutionParams] that executes commands in this environment\n\n    \"\"\"\n    env = self\n\n    async def executor(params: CodeExecutionParams) -&gt; ToolResult[ToolUseCountMetadata]:\n        result = await env.run_command(params.cmd)\n        return format_result(result)\n\n    return Tool[CodeExecutionParams, ToolUseCountMetadata](\n        name=name,\n        description=description\n        or \"Execute a shell command in the execution environment. Returns exit code, stdout, and stderr as XML.\",\n        parameters=CodeExecutionParams,\n        executor=executor,  # ty: ignore[invalid-argument-type]\n    )\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecToolProvider.get_view_image_tool","title":"get_view_image_tool","text":"<pre><code>get_view_image_tool(\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n) -&gt; Tool[ViewImageParams, ToolUseCountMetadata]\n</code></pre> <p>Create a view_image tool for this environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name</p> <code>'view_image'</code> <code>description</code> <code>str | None</code> <p>Tool description</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool[ViewImageParams, ToolUseCountMetadata]</code> <p>Tool[ViewImageParams, ToolUseCountMetadata] that views images in this environment</p> Source code in <code>src/stirrup/tools/code_backends/base.py</code> <pre><code>def get_view_image_tool(\n    self,\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n) -&gt; Tool[ViewImageParams, ToolUseCountMetadata]:\n    \"\"\"Create a view_image tool for this environment.\n\n    Args:\n        name: Tool name\n        description: Tool description\n\n    Returns:\n        Tool[ViewImageParams, ToolUseCountMetadata] that views images in this environment\n\n    \"\"\"\n    env = self\n\n    async def executor(params: ViewImageParams) -&gt; ToolResult[ToolUseCountMetadata]:\n        try:\n            image = await env.view_image(params.path)\n            return ToolResult(\n                content=[\"Viewing image at path: \" + params.path, image],\n                metadata=ToolUseCountMetadata(),\n            )\n        except FileNotFoundError:\n            return ToolResult(\n                content=f\"Image `{params.path}` not found.\",\n                success=False,\n                metadata=ToolUseCountMetadata(),\n            )\n        except ValueError as e:\n            return ToolResult(\n                content=str(e),\n                success=False,\n                metadata=ToolUseCountMetadata(),\n            )\n\n    return Tool[ViewImageParams, ToolUseCountMetadata](\n        name=name,\n        description=description or \"View an image file from the execution environment's filesystem.\",\n        parameters=ViewImageParams,\n        executor=executor,  # ty: ignore[invalid-argument-type]\n    )\n</code></pre>"},{"location":"api/tools/code_backends/#localcodeexectoolprovider","title":"LocalCodeExecToolProvider","text":"<p>Executes code in an isolated temporary directory on the host machine.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider","title":"stirrup.tools.code_backends.LocalCodeExecToolProvider","text":"<pre><code>LocalCodeExecToolProvider(\n    *,\n    allowed_commands: list[str] | None = None,\n    temp_base_dir: Path | str | None = None,\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>CodeExecToolProvider</code></p> <p>Local code execution tool provider using an isolated temp directory.</p> <p>Commands are executed with the temp directory as the working directory. An optional allowlist can restrict which commands are permitted.</p> Usage with Agent <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>client = ChatCompletionsClient(model=\"gpt-5\") agent = Agent(     client=client,     name=\"assistant\",     tools=[LocalCodeExecToolProvider(), CALCULATOR_TOOL], )</p> <p>async with agent.session(output_dir=\"./output\") as session:     await session.run(\"Run some Python code\")</p> Standalone usage <p>provider = LocalCodeExecToolProvider()</p> <p>async with provider as tool:     # tool is a Tool instance for code execution     result = await provider.run_command(\"python script.py\")     await provider.save_output_files([\"output.txt\"], \"/path/to/output\")</p> <p>Initialize LocalCodeExecToolProvider configuration.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_commands</code> <code>list[str] | None</code> <p>Optional list of regex patterns. If provided, only              commands matching at least one pattern are allowed.              If None, all commands are allowed.</p> <code>None</code> <code>temp_base_dir</code> <code>Path | str | None</code> <p>Optional base directory for creating the execution environment           temp directory. If None, uses the system default temp directory.</p> <code>None</code> <code>description</code> <code>str | None</code> <p>Optional description of the tool. If None, uses the default description.</p> <code>None</code> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>def __init__(\n    self,\n    *,\n    allowed_commands: list[str] | None = None,\n    temp_base_dir: Path | str | None = None,\n    description: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize LocalCodeExecToolProvider configuration.\n\n    Args:\n        allowed_commands: Optional list of regex patterns. If provided, only\n                         commands matching at least one pattern are allowed.\n                         If None, all commands are allowed.\n        temp_base_dir: Optional base directory for creating the execution environment\n                      temp directory. If None, uses the system default temp directory.\n        description: Optional description of the tool. If None, uses the default description.\n\n    \"\"\"\n    super().__init__(allowed_commands=allowed_commands)\n    self._temp_dir: Path | None = None\n    self._temp_base_dir: Path | None = Path(temp_base_dir) if temp_base_dir else None\n    self._description = (\n        description\n        or \"Execute a shell command in the execution environment. Returns exit code, stdout, and stderr as XML. Use `uv` to manage packages.\"\n    )\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider._temp_dir","title":"_temp_dir  <code>instance-attribute</code>","text":"<pre><code>_temp_dir: Path | None = None\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider._temp_base_dir","title":"_temp_base_dir  <code>instance-attribute</code>","text":"<pre><code>_temp_base_dir: Path | None = (\n    Path(temp_base_dir) if temp_base_dir else None\n)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider._description","title":"_description  <code>instance-attribute</code>","text":"<pre><code>_description = (\n    description\n    or \"Execute a shell command in the execution environment. Returns exit code, stdout, and stderr as XML. Use `uv` to manage packages.\"\n)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.temp_dir","title":"temp_dir  <code>property</code>","text":"<pre><code>temp_dir: Path | None\n</code></pre> <p>Return the temp directory path, or None if not started.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider._check_absolute_paths","title":"_check_absolute_paths","text":"<pre><code>_check_absolute_paths(cmd: str) -&gt; CommandResult | None\n</code></pre> <p>Check if command contains absolute paths that could escape the temp directory.</p> <p>Returns:</p> Type Description <code>CommandResult | None</code> <p>CommandResult with error if absolute paths detected, None otherwise.</p> Note <p>This check is specific to LocalCodeExecToolProvider since Docker and E2B providers are already sandboxed and absolute paths are safe within them.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>def _check_absolute_paths(self, cmd: str) -&gt; CommandResult | None:\n    \"\"\"Check if command contains absolute paths that could escape the temp directory.\n\n    Returns:\n        CommandResult with error if absolute paths detected, None otherwise.\n\n    Note:\n        This check is specific to LocalCodeExecToolProvider since Docker and E2B\n        providers are already sandboxed and absolute paths are safe within them.\n    \"\"\"\n    absolute_patterns = [\n        r\"~/\",  # ~/path - home directory shortcut\n        r\"/(?:home|Users|tmp|var|etc)/\",  # /home/, /Users/, /tmp/, etc.\n        r\"\\$HOME/\",  # $HOME/path\n        r\"\\$\\{HOME\\}/\",  # ${HOME}/path\n    ]\n    for pattern in absolute_patterns:\n        if re.search(pattern, cmd):\n            return CommandResult(\n                exit_code=1,\n                stdout=\"\",\n                stderr=(\n                    \"Command appears to use absolute paths which could write outside \"\n                    \"the execution environment. Use relative paths instead.\"\n                ),\n                error_kind=\"absolute_path_detected\",\n                advice=(\n                    \"Use relative paths (e.g., './output.txt' instead of '~/output.txt'). \"\n                    \"For full filesystem access, use DockerCodeExecToolProvider or E2BCodeExecToolProvider.\"\n                ),\n            )\n    return None\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool[\n    CodeExecutionParams, ToolUseCountMetadata\n]\n</code></pre> <p>Create temp directory and return the code_exec tool.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def __aenter__(self) -&gt; \"Tool[CodeExecutionParams, ToolUseCountMetadata]\":\n    \"\"\"Create temp directory and return the code_exec tool.\"\"\"\n    if self._temp_base_dir:\n        self._temp_base_dir.mkdir(parents=True, exist_ok=True)\n    self._temp_dir = Path(tempfile.mkdtemp(prefix=\"local_exec_env_\", dir=self._temp_base_dir))\n    logger.debug(\"Created local execution environment temp directory: %s\", self._temp_dir)\n    return self.get_code_exec_tool(description=self._description)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Cleanup the local execution environment.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Cleanup the local execution environment.\"\"\"\n    if self._temp_dir and self._temp_dir.exists():\n        try:\n            shutil.rmtree(self._temp_dir)\n        except Exception as exc:\n            logger.warning(\"Failed to cleanup temp directory %s: %s\", self._temp_dir, exc)\n    self._temp_dir = None\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider._resolve_and_validate_path","title":"_resolve_and_validate_path","text":"<pre><code>_resolve_and_validate_path(path: str) -&gt; Path\n</code></pre> <p>Resolve a path and validate it's within the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Resolved absolute Path.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> <code>FileNotFoundError</code> <p>If path does not exist (for reads).</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>def _resolve_and_validate_path(self, path: str) -&gt; Path:\n    \"\"\"Resolve a path and validate it's within the temp directory.\n\n    Args:\n        path: File path (relative or absolute within the temp dir).\n\n    Returns:\n        Resolved absolute Path.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n        FileNotFoundError: If path does not exist (for reads).\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\"ExecutionEnvironment not started.\")\n\n    resolved = Path(path)\n    if not resolved.is_absolute():\n        resolved = self._temp_dir / resolved\n\n    # Security: ensure path is within temp directory\n    try:\n        resolved.resolve().relative_to(self._temp_dir.resolve())\n    except ValueError as e:\n        raise ValueError(f\"Path is outside execution environment: {path}\") from e\n\n    return resolved\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.read_file_bytes","title":"read_file_bytes  <code>async</code>","text":"<pre><code>read_file_bytes(path: str) -&gt; bytes\n</code></pre> <p>Read file content as bytes from the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>File contents as bytes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> <code>FileNotFoundError</code> <p>If file does not exist.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def read_file_bytes(self, path: str) -&gt; bytes:\n    \"\"\"Read file content as bytes from the temp directory.\n\n    Args:\n        path: File path (relative or absolute within the temp dir).\n\n    Returns:\n        File contents as bytes.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n        FileNotFoundError: If file does not exist.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    if not resolved.exists():\n        raise FileNotFoundError(f\"File not found: {path}\")\n    return resolved.read_bytes()\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.write_file_bytes","title":"write_file_bytes  <code>async</code>","text":"<pre><code>write_file_bytes(path: str, content: bytes) -&gt; None\n</code></pre> <p>Write bytes to a file in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Destination path (relative or absolute within the temp dir).</p> required <code>content</code> <code>bytes</code> <p>File contents to write.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def write_file_bytes(self, path: str, content: bytes) -&gt; None:\n    \"\"\"Write bytes to a file in the temp directory.\n\n    Args:\n        path: Destination path (relative or absolute within the temp dir).\n        content: File contents to write.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    resolved.parent.mkdir(parents=True, exist_ok=True)\n    resolved.write_bytes(content)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.file_exists","title":"file_exists  <code>async</code>","text":"<pre><code>file_exists(path: str) -&gt; bool\n</code></pre> <p>Check if a file exists in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def file_exists(self, path: str) -&gt; bool:\n    \"\"\"Check if a file exists in the temp directory.\n\n    Args:\n        path: File path (relative or absolute within the temp dir).\n\n    Returns:\n        True if the file exists, False otherwise.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    return resolved.exists() and resolved.is_file()\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.is_directory","title":"is_directory  <code>async</code>","text":"<pre><code>is_directory(path: str) -&gt; bool\n</code></pre> <p>Check if a path is a directory in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the path exists and is a directory, False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def is_directory(self, path: str) -&gt; bool:\n    \"\"\"Check if a path is a directory in the temp directory.\n\n    Args:\n        path: Path (relative or absolute within the temp dir).\n\n    Returns:\n        True if the path exists and is a directory, False otherwise.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    return resolved.exists() and resolved.is_dir()\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.list_files","title":"list_files  <code>async</code>","text":"<pre><code>list_files(path: str) -&gt; list[str]\n</code></pre> <p>List all files recursively in a directory within the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path (relative or absolute within the temp dir).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of file paths (relative to the given path) for all files in the directory.</p> <code>list[str]</code> <p>Returns an empty list if the path is a file or doesn't exist.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If environment not started.</p> <code>ValueError</code> <p>If path is outside temp directory.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def list_files(self, path: str) -&gt; list[str]:\n    \"\"\"List all files recursively in a directory within the temp directory.\n\n    Args:\n        path: Directory path (relative or absolute within the temp dir).\n\n    Returns:\n        List of file paths (relative to the given path) for all files in the directory.\n        Returns an empty list if the path is a file or doesn't exist.\n\n    Raises:\n        RuntimeError: If environment not started.\n        ValueError: If path is outside temp directory.\n\n    \"\"\"\n    resolved = self._resolve_and_validate_path(path)\n    if not resolved.exists() or not resolved.is_dir():\n        return []\n\n    files = []\n    for file_path in resolved.rglob(\"*\"):\n        if file_path.is_file():\n            rel_path = file_path.relative_to(resolved)\n            files.append(str(rel_path))\n    return files\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.run_command","title":"run_command  <code>async</code>","text":"<pre><code>run_command(\n    cmd: str, *, timeout: int = SHELL_TIMEOUT\n) -&gt; CommandResult\n</code></pre> <p>Execute command in the temp directory.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Shell command to execute (bash syntax).</p> required <code>timeout</code> <code>int</code> <p>Maximum time in seconds to wait for command completion.</p> <code>SHELL_TIMEOUT</code> <p>Returns:</p> Type Description <code>CommandResult</code> <p>CommandResult with exit_code, stdout, stderr, and optional error info.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def run_command(self, cmd: str, *, timeout: int = SHELL_TIMEOUT) -&gt; CommandResult:\n    \"\"\"Execute command in the temp directory.\n\n    Args:\n        cmd: Shell command to execute (bash syntax).\n        timeout: Maximum time in seconds to wait for command completion.\n\n    Returns:\n        CommandResult with exit_code, stdout, stderr, and optional error info.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # Check allowlist\n    if not self._check_allowed(cmd):\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=f\"Command not allowed: '{cmd}' does not match any allowed patterns\",\n            error_kind=\"command_not_allowed\",\n            advice=\"Only commands matching the allowlist patterns are permitted.\",\n        )\n\n    # Check for absolute paths (local environment is not sandboxed)\n    absolute_path_error = self._check_absolute_paths(cmd)\n    if absolute_path_error:\n        return absolute_path_error\n\n    process = None\n    try:\n        with anyio.fail_after(timeout):\n            # Use shell=True by wrapping in a shell command\n            process = await anyio.open_process(\n                [\"bash\", \"-c\", cmd],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                cwd=self._temp_dir,\n            )\n\n            # Read all output from streams concurrently\n            stdout_chunks: list[bytes] = []\n            stderr_chunks: list[bytes] = []\n\n            async def read_stdout() -&gt; None:\n                if process.stdout:\n                    stdout_chunks.extend([chunk async for chunk in process.stdout])\n\n            async def read_stderr() -&gt; None:\n                if process.stderr:\n                    stderr_chunks.extend([chunk async for chunk in process.stderr])\n\n            async with anyio.create_task_group() as tg:\n                tg.start_soon(read_stdout)\n                tg.start_soon(read_stderr)\n\n            await process.wait()\n\n            return CommandResult(\n                exit_code=process.returncode or 0,\n                stdout=b\"\".join(stdout_chunks).decode(\"utf-8\", errors=\"replace\"),\n                stderr=b\"\".join(stderr_chunks).decode(\"utf-8\", errors=\"replace\"),\n            )\n\n    except TimeoutError:\n        if process:\n            process.kill()\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=f\"Command timed out after {timeout} seconds\",\n            error_kind=\"timeout\",\n        )\n    except Exception as exc:\n        return CommandResult(\n            exit_code=1,\n            stdout=\"\",\n            stderr=str(exc),\n            error_kind=\"execution_error\",\n        )\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.save_output_files","title":"save_output_files  <code>async</code>","text":"<pre><code>save_output_files(\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: CodeExecToolProvider | None = None,\n) -&gt; SaveOutputFilesResult\n</code></pre> <p>Move files from the temp directory to a destination.</p> <p>When dest_env is None (local filesystem), files are MOVED (not copied) - originals are deleted from the execution environment. Existing files in output_dir are silently overwritten.</p> <p>When dest_env is provided (cross-environment transfer), files are copied using the base class implementation via read/write primitives.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>List of file paths in the execution environment (relative or absolute).    Relative paths are resolved against the execution environment temp directory.</p> required <code>output_dir</code> <code>Path | str</code> <p>Directory path to save files to.</p> required <code>dest_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, output_dir is interpreted as a path within dest_env       (cross-environment transfer). If None, output_dir is a local       filesystem path.</p> <code>None</code> <p>Returns:</p> Type Description <code>SaveOutputFilesResult</code> <p>SaveOutputFilesResult containing lists of saved files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def save_output_files(\n    self,\n    paths: list[str],\n    output_dir: Path | str,\n    dest_env: \"CodeExecToolProvider | None\" = None,\n) -&gt; SaveOutputFilesResult:\n    \"\"\"Move files from the temp directory to a destination.\n\n    When dest_env is None (local filesystem), files are MOVED (not copied) -\n    originals are deleted from the execution environment.\n    Existing files in output_dir are silently overwritten.\n\n    When dest_env is provided (cross-environment transfer), files are copied\n    using the base class implementation via read/write primitives.\n\n    Args:\n        paths: List of file paths in the execution environment (relative or absolute).\n               Relative paths are resolved against the execution environment temp directory.\n        output_dir: Directory path to save files to.\n        dest_env: If provided, output_dir is interpreted as a path within dest_env\n                  (cross-environment transfer). If None, output_dir is a local\n                  filesystem path.\n\n    Returns:\n        SaveOutputFilesResult containing lists of saved files and any failures.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # If dest_env is provided, use the base class implementation (cross-env transfer)\n    if dest_env is not None:\n        return await super().save_output_files(paths, output_dir, dest_env)\n\n    # Local filesystem - use optimized move operation\n    output_dir_path = Path(output_dir)\n    output_dir_path.mkdir(parents=True, exist_ok=True)\n\n    result = SaveOutputFilesResult()\n\n    for source_path_str in paths:\n        try:\n            source_path = Path(source_path_str)\n            if not source_path.is_absolute():\n                source_path = self._temp_dir / source_path\n\n            # Security: ensure path is within temp directory\n            try:\n                source_path.resolve().relative_to(self._temp_dir.resolve())\n            except ValueError:\n                result.failed[source_path_str] = \"Path is outside execution environment directory\"\n                logger.warning(\"Attempted to access path outside execution environment: %s\", source_path_str)\n                continue\n\n            if not source_path.exists():\n                result.failed[source_path_str] = \"File does not exist\"\n                logger.warning(\"Execution environment file does not exist: %s\", source_path_str)\n                continue\n\n            if not source_path.is_file():\n                result.failed[source_path_str] = \"Path is not a file\"\n                logger.warning(\"Execution environment path is not a file: %s\", source_path_str)\n                continue\n\n            file_size = source_path.stat().st_size\n            dest_path = output_dir_path / source_path.name\n\n            # Move file (overwrites if exists)\n            shutil.move(str(source_path), str(dest_path))\n            logger.debug(\"Moved file: %s -&gt; %s\", source_path, dest_path)\n\n            result.saved.append(\n                SavedFile(\n                    source_path=source_path_str,\n                    output_path=dest_path,\n                    size=file_size,\n                ),\n            )\n\n        except Exception as exc:\n            result.failed[source_path_str] = str(exc)\n            logger.exception(\"Failed to move file: %s\", source_path_str)\n\n    return result\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.upload_files","title":"upload_files  <code>async</code>","text":"<pre><code>upload_files(\n    *paths: Path | str,\n    source_env: CodeExecToolProvider | None = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult\n</code></pre> <p>Upload files to the execution environment.</p> <p>When source_env is None (local filesystem), files are COPIED (not moved) - originals remain on the local filesystem. Directories are uploaded recursively, preserving their structure.</p> <p>When source_env is provided (cross-environment transfer), files are copied using the base class implementation via read/write primitives.</p> <p>Parameters:</p> Name Type Description Default <code>*paths</code> <code>Path | str</code> <p>File or directory paths to upload. If source_env is None, these     are local filesystem paths. If source_env is provided, these are     paths within source_env.</p> <code>()</code> <code>source_env</code> <code>CodeExecToolProvider | None</code> <p>If provided, paths are within source_env. If None, paths are         local filesystem paths.</p> <code>None</code> <code>dest_dir</code> <code>str | None</code> <p>Destination subdirectory within the temp directory.       If None, files are placed directly in the temp directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>UploadFilesResult</code> <p>UploadFilesResult containing lists of uploaded files and any failures.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def upload_files(\n    self,\n    *paths: Path | str,\n    source_env: \"CodeExecToolProvider | None\" = None,\n    dest_dir: str | None = None,\n) -&gt; UploadFilesResult:\n    \"\"\"Upload files to the execution environment.\n\n    When source_env is None (local filesystem), files are COPIED (not moved) -\n    originals remain on the local filesystem.\n    Directories are uploaded recursively, preserving their structure.\n\n    When source_env is provided (cross-environment transfer), files are copied\n    using the base class implementation via read/write primitives.\n\n    Args:\n        *paths: File or directory paths to upload. If source_env is None, these\n                are local filesystem paths. If source_env is provided, these are\n                paths within source_env.\n        source_env: If provided, paths are within source_env. If None, paths are\n                    local filesystem paths.\n        dest_dir: Destination subdirectory within the temp directory.\n                  If None, files are placed directly in the temp directory.\n\n    Returns:\n        UploadFilesResult containing lists of uploaded files and any failures.\n\n    \"\"\"\n    if self._temp_dir is None:\n        raise RuntimeError(\n            \"ExecutionEnvironment not started. Ensure current Agent is equipped with a CodeExecToolProvider.\"\n        )\n\n    # If source_env is provided, use the base class implementation (cross-env transfer)\n    if source_env is not None:\n        return await super().upload_files(*paths, source_env=source_env, dest_dir=dest_dir)\n\n    # Local filesystem - use optimized copy operation\n    dest_base = self._temp_dir / dest_dir if dest_dir else self._temp_dir\n    dest_base.mkdir(parents=True, exist_ok=True)\n\n    result = UploadFilesResult()\n\n    for source in paths:\n        source = Path(source).resolve()\n\n        if not source.exists():\n            result.failed[str(source)] = \"File or directory does not exist\"\n            logger.warning(\"Upload source does not exist: %s\", source)\n            continue\n\n        try:\n            if source.is_file():\n                dest = dest_base / source.name\n                shutil.copy2(source, dest)\n                result.uploaded.append(\n                    UploadedFile(\n                        source_path=source,\n                        dest_path=str(dest.relative_to(self._temp_dir)),\n                        size=source.stat().st_size,\n                    ),\n                )\n                logger.debug(\"Uploaded file: %s -&gt; %s\", source, dest)\n\n            elif source.is_dir():\n                # If dest_dir was explicitly provided, copy contents directly to dest_base\n                # Otherwise, create a subdirectory with the source's name\n                if dest_dir:\n                    dest = dest_base\n                    # Copy contents of source directory into dest_base\n                    for item in source.iterdir():\n                        item_dest = dest / item.name\n                        if item.is_file():\n                            shutil.copy2(item, item_dest)\n                        else:\n                            shutil.copytree(item, item_dest, dirs_exist_ok=True)\n                else:\n                    dest = dest_base / source.name\n                    shutil.copytree(source, dest, dirs_exist_ok=True)\n                # Track all individual files uploaded\n                for file_path in source.rglob(\"*\"):\n                    if file_path.is_file():\n                        relative = file_path.relative_to(source)\n                        dest_file = dest / relative\n                        result.uploaded.append(\n                            UploadedFile(\n                                source_path=file_path,\n                                dest_path=str(dest_file.relative_to(self._temp_dir)),\n                                size=file_path.stat().st_size,\n                            ),\n                        )\n                logger.debug(\"Uploaded directory: %s -&gt; %s\", source, dest)\n\n        except Exception as exc:\n            result.failed[str(source)] = str(exc)\n            logger.exception(\"Failed to upload: %s\", source)\n\n    return result\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.LocalCodeExecToolProvider.view_image","title":"view_image  <code>async</code>","text":"<pre><code>view_image(path: str) -&gt; ImageContentBlock\n</code></pre> <p>Read and return an image file from the local execution environment.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to image file (relative to temp directory, or absolute within it).</p> required <p>Returns:</p> Type Description <code>ImageContentBlock</code> <p>ImageContentBlock containing the image data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If execution environment not started.</p> <code>FileNotFoundError</code> <p>If file does not exist.</p> <code>ValueError</code> <p>If path is outside temp directory, is a directory, or not a valid image.</p> Source code in <code>src/stirrup/tools/code_backends/local.py</code> <pre><code>async def view_image(self, path: str) -&gt; ImageContentBlock:\n    \"\"\"Read and return an image file from the local execution environment.\n\n    Args:\n        path: Path to image file (relative to temp directory, or absolute within it).\n\n    Returns:\n        ImageContentBlock containing the image data.\n\n    Raises:\n        RuntimeError: If execution environment not started.\n        FileNotFoundError: If file does not exist.\n        ValueError: If path is outside temp directory, is a directory, or not a valid image.\n\n    \"\"\"\n    file_bytes = await self.read_file_bytes(path)\n    return ImageContentBlock(data=file_bytes)\n</code></pre>"},{"location":"api/tools/code_backends/#dockercodeexectoolprovider","title":"DockerCodeExecToolProvider","text":"<p>Executes code in a Docker container.</p> <p>Note</p> <p>Requires <code>pip install stirrup[docker]</code> (or: <code>uv add stirrup[docker]</code>)</p> <pre><code>from stirrup.tools.code_backends.docker import DockerCodeExecToolProvider\n\nprovider = DockerCodeExecToolProvider.from_image(\"python:3.12-slim\")\n</code></pre>"},{"location":"api/tools/code_backends/#e2bcodeexectoolprovider","title":"E2BCodeExecToolProvider","text":"<p>Executes code in an E2B cloud sandbox.</p> <p>Note</p> <p>Requires <code>pip install stirrup[e2b]</code> (or: <code>uv add stirrup[e2b]</code>) and <code>E2B_API_KEY</code> environment variable.</p> <pre><code>from stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\n\nprovider = E2BCodeExecToolProvider()\n</code></pre>"},{"location":"api/tools/code_backends/#data-types","title":"Data Types","text":""},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecutionParams","title":"stirrup.tools.code_backends.CodeExecutionParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Shell command to execute in the execution environment.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CodeExecutionParams.cmd","title":"cmd  <code>instance-attribute</code>","text":"<pre><code>cmd: Annotated[\n    str,\n    Field(\n        description=\"Shell command to execute (bash syntax). IMPORTANT: Use only relative paths. Do not use absolute paths (starting with / or ~) or reference directories outside the working directory.\"\n    ),\n]\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult","title":"stirrup.tools.code_backends.CommandResult  <code>dataclass</code>","text":"<pre><code>CommandResult(\n    exit_code: int,\n    stdout: str,\n    stderr: str,\n    error_kind: str | None = None,\n    advice: str | None = None,\n)\n</code></pre> <p>Raw result from command execution (before formatting).</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult.exit_code","title":"exit_code  <code>instance-attribute</code>","text":"<pre><code>exit_code: int\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult.stdout","title":"stdout  <code>instance-attribute</code>","text":"<pre><code>stdout: str\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult.stderr","title":"stderr  <code>instance-attribute</code>","text":"<pre><code>stderr: str\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult.error_kind","title":"error_kind  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>error_kind: str | None = None\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.CommandResult.advice","title":"advice  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>advice: str | None = None\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SavedFile","title":"stirrup.tools.code_backends.SavedFile  <code>dataclass</code>","text":"<pre><code>SavedFile(source_path: str, output_path: Path, size: int)\n</code></pre> <p>Information about a file saved from the execution environment.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SavedFile.source_path","title":"source_path  <code>instance-attribute</code>","text":"<pre><code>source_path: str\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SavedFile.output_path","title":"output_path  <code>instance-attribute</code>","text":"<pre><code>output_path: Path\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SavedFile.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SaveOutputFilesResult","title":"stirrup.tools.code_backends.SaveOutputFilesResult  <code>dataclass</code>","text":"<pre><code>SaveOutputFilesResult(\n    saved: list[SavedFile] = list(),\n    failed: dict[str, str] = dict(),\n)\n</code></pre> <p>Result of saving output files from the execution environment.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SaveOutputFilesResult.saved","title":"saved  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>saved: list[SavedFile] = field(default_factory=list)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.SaveOutputFilesResult.failed","title":"failed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>failed: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadedFile","title":"stirrup.tools.code_backends.UploadedFile  <code>dataclass</code>","text":"<pre><code>UploadedFile(source_path: Path, dest_path: str, size: int)\n</code></pre> <p>Information about a file uploaded to the execution environment.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadedFile.source_path","title":"source_path  <code>instance-attribute</code>","text":"<pre><code>source_path: Path\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadedFile.dest_path","title":"dest_path  <code>instance-attribute</code>","text":"<pre><code>dest_path: str\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadedFile.size","title":"size  <code>instance-attribute</code>","text":"<pre><code>size: int\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadFilesResult","title":"stirrup.tools.code_backends.UploadFilesResult  <code>dataclass</code>","text":"<pre><code>UploadFilesResult(\n    uploaded: list[UploadedFile] = list(),\n    failed: dict[str, str] = dict(),\n)\n</code></pre> <p>Result of uploading files to the execution environment.</p>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadFilesResult.uploaded","title":"uploaded  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uploaded: list[UploadedFile] = field(default_factory=list)\n</code></pre>"},{"location":"api/tools/code_backends/#stirrup.tools.code_backends.UploadFilesResult.failed","title":"failed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>failed: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"api/tools/mcp/","title":"MCP Tool Provider","text":"<p>The <code>stirrup.tools.mcp</code> module provides MCP (Model Context Protocol) integration.</p> <p>Note</p> <p>Requires <code>pip install stirrup[mcp]</code> (or: <code>uv add stirrup[mcp]</code>)</p>"},{"location":"api/tools/mcp/#mcptoolprovider","title":"MCPToolProvider","text":""},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPToolProvider","title":"stirrup.tools.mcp.MCPToolProvider","text":"<pre><code>MCPToolProvider(\n    config: MCPConfig, server_names: list[str] | None = None\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code></p> <p>MCP tool provider that manages connections to multiple MCP servers.</p> <p>MCPToolProvider connects to MCP servers and exposes each server's tools as individual Tool objects.</p> <p>Usage with Agent (preferred):     from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <pre><code>client = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"assistant\",\n    tools=[*DEFAULT_TOOLS, MCPToolProvider.from_config(\"mcp.json\")],\n)\n\nasync with agent.session(output_dir=\"./output\") as session:\n    await session.run(\"Use MCP tools\")\n</code></pre> <p>Standalone usage with connect() context manager:     provider = MCPToolProvider.from_config(Path(\"mcp.json\"))     async with provider.connect() as provider:         tools = provider.get_all_tools()         # Use tools...</p> <p>Initialize the MCP manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MCPConfig</code> <p>MCPConfig instance.</p> required <code>server_names</code> <code>list[str] | None</code> <p>Which servers to connect to. If None, connects to all servers in config.</p> <code>None</code> Source code in <code>src/stirrup/tools/mcp.py</code> <pre><code>def __init__(\n    self,\n    config: MCPConfig,\n    server_names: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the MCP manager.\n\n    Args:\n        config: MCPConfig instance.\n        server_names: Which servers to connect to. If None, connects to all servers in config.\n    \"\"\"\n    self._config = config\n    self._server_names = server_names\n    self._servers: dict[str, ClientSession] = {}\n    self._tools: dict[str, list[dict[str, Any]]] = {}\n    self._exit_stack: AsyncExitStack | None = None\n</code></pre>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPToolProvider.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config_path: Path | str,\n    server_names: list[str] | None = None,\n) -&gt; Self\n</code></pre> <p>Create an MCPToolProvider from a config file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Path | str</code> <p>Path to the MCP config file.</p> required <code>server_names</code> <code>list[str] | None</code> <p>Which servers to connect to. If None, connects to all servers in config.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>MCPToolProvider instance.</p> Source code in <code>src/stirrup/tools/mcp.py</code> <pre><code>@classmethod\ndef from_config(cls, config_path: Path | str, server_names: list[str] | None = None) -&gt; Self:\n    \"\"\"Create an MCPToolProvider from a config file.\n\n    Args:\n        config_path: Path to the MCP config file.\n        server_names: Which servers to connect to. If None, connects to all servers in config.\n\n    Returns:\n        MCPToolProvider instance.\n    \"\"\"\n    config = MCPConfig.model_validate_json(Path(config_path).read_text())\n\n    return cls(config=config, server_names=server_names)\n</code></pre>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; list[Tool[Any, ToolUseCountMetadata]]\n</code></pre> <p>Enter async context: connect to MCP servers and return all tools.</p> <p>Returns:</p> Type Description <code>list[Tool[Any, ToolUseCountMetadata]]</code> <p>List of Tool objects, one for each tool available across all connected servers.</p> Source code in <code>src/stirrup/tools/mcp.py</code> <pre><code>async def __aenter__(self) -&gt; list[Tool[Any, ToolUseCountMetadata]]:\n    \"\"\"Enter async context: connect to MCP servers and return all tools.\n\n    Returns:\n        List of Tool objects, one for each tool available across all connected servers.\n    \"\"\"\n    self._exit_stack = AsyncExitStack()\n    await self._exit_stack.__aenter__()\n\n    config = self._config\n    servers_to_connect = self._server_names or list(config.mcp_servers.keys())\n\n    for name in servers_to_connect:\n        if name not in config.mcp_servers:\n            raise KeyError(f\"Server '{name}' not found in config. Available: {list(config.mcp_servers.keys())}\")\n\n        server_config = config.mcp_servers[name]\n\n        # Connect to server based on transport type\n        match server_config:\n            case StdioServerConfig():\n                server_params = StdioServerParameters(\n                    command=server_config.command,\n                    args=server_config.args,\n                    env=server_config.env,\n                    cwd=server_config.cwd,\n                    encoding=server_config.encoding,\n                )\n                read, write = await self._exit_stack.enter_async_context(stdio_client(server_params))\n            case SseServerConfig():\n                read, write = await self._exit_stack.enter_async_context(\n                    sse_client(\n                        url=server_config.url,\n                        headers=server_config.headers,\n                        timeout=server_config.timeout,\n                        sse_read_timeout=server_config.sse_read_timeout,\n                    )\n                )\n            case StreamableHttpServerConfig():\n                read, write, _ = await self._exit_stack.enter_async_context(\n                    streamablehttp_client(\n                        url=server_config.url,\n                        headers=server_config.headers,\n                        timeout=server_config.timeout,\n                        sse_read_timeout=server_config.sse_read_timeout,\n                        terminate_on_close=server_config.terminate_on_close,\n                    )\n                )\n            case WebSocketServerConfig():\n                if websocket_client is None:\n                    raise ImportError(\n                        f\"WebSocket transport for server '{name}' requires the 'websockets' package. \"\n                        \"Install with: pip install websockets\"\n                    )\n                read, write = await self._exit_stack.enter_async_context(websocket_client(url=server_config.url))\n\n        session = await self._exit_stack.enter_async_context(ClientSession(read, write))\n        await session.initialize()\n\n        # Cache session and available tools\n        self._servers[name] = session\n        response = await session.list_tools()\n        self._tools[name] = [\n            {\"name\": t.name, \"description\": t.description, \"schema\": t.inputSchema} for t in response.tools\n        ]\n\n    return self.get_all_tools()\n</code></pre>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit async context: disconnect from MCP servers.</p> Source code in <code>src/stirrup/tools/mcp.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit async context: disconnect from MCP servers.\"\"\"\n    self._servers.clear()\n    self._tools.clear()\n    if self._exit_stack:\n        await self._exit_stack.__aexit__(exc_type, exc_val, exc_tb)\n        self._exit_stack = None\n</code></pre>"},{"location":"api/tools/mcp/#configuration-models","title":"Configuration Models","text":""},{"location":"api/tools/mcp/#mcpconfig","title":"MCPConfig","text":""},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPConfig","title":"stirrup.tools.mcp.MCPConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Root configuration matching mcp.json format.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPConfig.mcp_servers","title":"mcp_servers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mcp_servers: dict[str, MCPServerConfig] = Field(\n    alias=\"mcpServers\"\n)\n</code></pre> <p>Map of server names to their configurations.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.MCPConfig._infer_transport_types","title":"_infer_transport_types  <code>classmethod</code>","text":"<pre><code>_infer_transport_types(\n    data: dict[str, Any],\n) -&gt; dict[str, Any]\n</code></pre> <p>Convert raw server configs to appropriate typed instances.</p> Source code in <code>src/stirrup/tools/mcp.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef _infer_transport_types(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert raw server configs to appropriate typed instances.\"\"\"\n    if \"mcpServers\" in data:\n        data[\"mcpServers\"] = {\n            name: _infer_server_config(config) if isinstance(config, dict) else config\n            for name, config in data[\"mcpServers\"].items()\n        }\n    return data\n</code></pre>"},{"location":"api/tools/mcp/#server-configurations","title":"Server Configurations","text":""},{"location":"api/tools/mcp/#stdioserverconfig","title":"StdioServerConfig","text":"<p>For local command-based MCP servers:</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig","title":"stirrup.tools.mcp.StdioServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for stdio-based MCP servers (local process).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig.command","title":"command  <code>instance-attribute</code>","text":"<pre><code>command: str\n</code></pre> <p>Command to run the MCP server (e.g., \"npx\", \"python\").</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig.args","title":"args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>args: list[str] = Field(default_factory=list)\n</code></pre> <p>Arguments to pass to the command.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig.env","title":"env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env: dict[str, str] | None = None\n</code></pre> <p>Environment variables to set for the server process.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig.cwd","title":"cwd  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cwd: str | None = None\n</code></pre> <p>Working directory for the server process.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StdioServerConfig.encoding","title":"encoding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>encoding: str = 'utf-8'\n</code></pre> <p>Text encoding for messages.</p>"},{"location":"api/tools/mcp/#sseserverconfig","title":"SseServerConfig","text":"<p>For SSE (Server-Sent Events) based MCP servers:</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.SseServerConfig","title":"stirrup.tools.mcp.SseServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SSE-based MCP servers (HTTP GET with Server-Sent Events).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.SseServerConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The SSE endpoint URL (must end with /sse).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.SseServerConfig.headers","title":"headers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>headers: dict[str, str] | None = None\n</code></pre> <p>Optional HTTP headers.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.SseServerConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 5.0\n</code></pre> <p>HTTP timeout for regular operations (seconds).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.SseServerConfig.sse_read_timeout","title":"sse_read_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: float = 300.0\n</code></pre> <p>Timeout for SSE read operations (seconds).</p>"},{"location":"api/tools/mcp/#streamablehttpserverconfig","title":"StreamableHttpServerConfig","text":"<p>For streamable HTTP MCP servers:</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig","title":"stirrup.tools.mcp.StreamableHttpServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Streamable HTTP MCP servers (HTTP POST with optional SSE responses).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The endpoint URL.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig.headers","title":"headers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>headers: dict[str, str] | None = None\n</code></pre> <p>Optional HTTP headers.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre> <p>HTTP timeout (seconds).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig.sse_read_timeout","title":"sse_read_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sse_read_timeout: float = 300.0\n</code></pre> <p>SSE read timeout (seconds).</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.StreamableHttpServerConfig.terminate_on_close","title":"terminate_on_close  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>terminate_on_close: bool = True\n</code></pre> <p>Close session when transport closes.</p>"},{"location":"api/tools/mcp/#websocketserverconfig","title":"WebSocketServerConfig","text":"<p>For WebSocket-based MCP servers:</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.WebSocketServerConfig","title":"stirrup.tools.mcp.WebSocketServerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for WebSocket-based MCP servers.</p>"},{"location":"api/tools/mcp/#stirrup.tools.mcp.WebSocketServerConfig.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>The WebSocket URL (must start with ws:// or wss://).</p>"},{"location":"api/tools/view-image/","title":"View Image Tool Provider","text":"<p>The <code>ViewImageToolProvider</code> allows agents to view images from their execution environment.</p>"},{"location":"api/tools/view-image/#viewimagetoolprovider","title":"ViewImageToolProvider","text":""},{"location":"api/tools/view-image/#stirrup.tools.view_image.ViewImageToolProvider","title":"stirrup.tools.view_image.ViewImageToolProvider","text":"<pre><code>ViewImageToolProvider(\n    exec_env: CodeExecToolProvider | None = None,\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code></p> <p>Tool provider for viewing images from an execution environment.</p> <p>Can be used with an explicit exec_env or will auto-detect from the Agent's session state. Works regardless of tool ordering in the Agent.</p> <p>Examples:</p> <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>client = ChatCompletionsClient(model=\"gpt-5\")</p>"},{"location":"api/tools/view-image/#stirrup.tools.view_image.ViewImageToolProvider--explicit-exec_env","title":"Explicit exec_env","text":"<p>exec_env = LocalCodeExecToolProvider() agent = Agent(     client=client, name=\"assistant\",     tools=[exec_env, ViewImageToolProvider(exec_env)], )</p>"},{"location":"api/tools/view-image/#stirrup.tools.view_image.ViewImageToolProvider--auto-detect-any-order-works","title":"Auto-detect (any order works)","text":"<p>agent = Agent(     client=client, name=\"assistant\",     tools=[ViewImageToolProvider(), LocalCodeExecToolProvider()], )</p> <p>Initialize ViewImageToolProvider.</p> <p>Parameters:</p> Name Type Description Default <code>exec_env</code> <code>CodeExecToolProvider | None</code> <p>Optional execution environment. If None, will auto-detect from the Agent's session state.</p> <code>None</code> <code>name</code> <code>str</code> <p>Tool name (default: \"view_image\").</p> <code>'view_image'</code> <code>description</code> <code>str | None</code> <p>Tool description (default: standard description).</p> <code>None</code> Source code in <code>src/stirrup/tools/view_image.py</code> <pre><code>def __init__(\n    self,\n    exec_env: CodeExecToolProvider | None = None,\n    *,\n    name: str = \"view_image\",\n    description: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize ViewImageToolProvider.\n\n    Args:\n        exec_env: Optional execution environment. If None, will auto-detect\n            from the Agent's session state.\n        name: Tool name (default: \"view_image\").\n        description: Tool description (default: standard description).\n\n    \"\"\"\n    self._exec_env = exec_env\n    self._name = name\n    self._description = description\n</code></pre>"},{"location":"api/tools/view-image/#stirrup.tools.view_image.ViewImageToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; Tool[ViewImageParams, ToolUseCountMetadata]\n</code></pre> <p>Enter async context: resolve exec_env and return view_image tool.</p> Source code in <code>src/stirrup/tools/view_image.py</code> <pre><code>async def __aenter__(self) -&gt; Tool[ViewImageParams, ToolUseCountMetadata]:\n    \"\"\"Enter async context: resolve exec_env and return view_image tool.\"\"\"\n    # Import here to avoid circular dependency\n    from stirrup.core.agent import _SESSION_STATE\n\n    state = _SESSION_STATE.get(None)\n    agent_exec_env = state.exec_env if state else None\n\n    if self._exec_env is not None:\n        # Explicit exec_env provided - validate it matches agent's exec_env\n        if agent_exec_env is not None and self._exec_env is not agent_exec_env:\n            raise ValueError(\n                f\"ViewImageToolProvider exec_env ({type(self._exec_env).__name__}) \"\n                f\"does not match Agent's exec_env ({type(agent_exec_env).__name__}). \"\n                \"Use the same exec_env instance or omit exec_env to auto-detect.\"\n            )\n        exec_env = self._exec_env\n    else:\n        # Auto-detect from session state\n        if agent_exec_env is None:\n            raise RuntimeError(\n                \"ViewImageToolProvider requires a CodeExecToolProvider. \"\n                \"Either pass exec_env explicitly or include a CodeExecToolProvider \"\n                \"in the Agent's tools list.\"\n            )\n        exec_env = agent_exec_env\n\n    return exec_env.get_view_image_tool(\n        name=self._name,\n        description=self._description,\n    )\n</code></pre>"},{"location":"api/tools/view-image/#usage","title":"Usage","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, ViewImageToolProvider\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"image_viewer\",\n    tools=[\n        LocalCodeExecToolProvider(),\n        ViewImageToolProvider(),  # Auto-detects exec environment\n    ],\n)\n\nasync with agent.session() as session:\n    await session.run(\"Create a chart with matplotlib and view it\")\n</code></pre>"},{"location":"api/tools/view-image/#how-it-works","title":"How It Works","text":"<ol> <li>The <code>ViewImageToolProvider</code> is initialized with an optional <code>exec_env</code> parameter</li> <li>If not provided, it automatically detects the execution environment from the session</li> <li>The <code>view_image</code> tool reads image files from the execution environment</li> <li>Images are returned as <code>ImageContentBlock</code> objects for the model to see</li> </ol>"},{"location":"api/tools/web/","title":"Web Tools","text":"<p>The <code>WebToolProvider</code> provides web fetching and search capabilities.</p>"},{"location":"api/tools/web/#webtoolprovider","title":"WebToolProvider","text":""},{"location":"api/tools/web/#stirrup.tools.web.WebToolProvider","title":"stirrup.tools.web.WebToolProvider","text":"<pre><code>WebToolProvider(\n    *,\n    timeout: float = 60 * 3,\n    brave_api_key: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ToolProvider</code></p> <p>Provides web tools (web_fetch, web_search) with managed HTTP client lifecycle.</p> <p>WebToolProvider implements the Tool lifecycle protocol (has_lifecycle=True), so it can be used directly in Agent's tools list. It creates an httpx.AsyncClient on aenter and returns the web tools.</p> <p>Usage as Tool in Agent (preferred):     from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <pre><code>client = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"assistant\",\n    tools=[LocalCodeExecToolProvider(), WebToolProvider(), CALCULATOR_TOOL],\n)\n\nasync with agent.session(output_dir=\"./output\") as session:\n    await session.run(\"Search the web and fetch a page\")\n</code></pre> Standalone usage <p>async with WebToolProvider() as provider:     tools = provider.get_tools()</p> <p>Initialize WebToolProvider.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>HTTP timeout in seconds (default: 180)</p> <code>60 * 3</code> <code>brave_api_key</code> <code>str | None</code> <p>Brave Search API key for web_search tool.           If None, uses BRAVE_API_KEY environment variable.           Web search is only available if API key is provided.</p> <code>None</code> Source code in <code>src/stirrup/tools/web.py</code> <pre><code>def __init__(\n    self,\n    *,\n    timeout: float = 60 * 3,\n    brave_api_key: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize WebToolProvider.\n\n    Args:\n        timeout: HTTP timeout in seconds (default: 180)\n        brave_api_key: Brave Search API key for web_search tool.\n                      If None, uses BRAVE_API_KEY environment variable.\n                      Web search is only available if API key is provided.\n    \"\"\"\n    self._timeout = timeout\n    self._brave_api_key = brave_api_key or os.getenv(\"BRAVE_API_KEY\")\n    self._client: httpx.AsyncClient | None = None\n</code></pre>"},{"location":"api/tools/web/#stirrup.tools.web.WebToolProvider.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; list[Tool[Any, Any]]\n</code></pre> <p>Enter async context: create HTTP client and return web tools.</p> <p>Returns:</p> Type Description <code>list[Tool[Any, Any]]</code> <p>List of Tool objects (web_fetch, and web_search if API key available).</p> Source code in <code>src/stirrup/tools/web.py</code> <pre><code>async def __aenter__(self) -&gt; list[Tool[Any, Any]]:\n    \"\"\"Enter async context: create HTTP client and return web tools.\n\n    Returns:\n        List of Tool objects (web_fetch, and web_search if API key available).\n    \"\"\"\n    self._client = httpx.AsyncClient(\n        timeout=self._timeout,\n        follow_redirects=True,\n    )\n    await self._client.__aenter__()\n    return self.get_tools()\n</code></pre>"},{"location":"api/tools/web/#stirrup.tools.web.WebToolProvider.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit async context: close HTTP client.</p> Source code in <code>src/stirrup/tools/web.py</code> <pre><code>async def __aexit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit async context: close HTTP client.\"\"\"\n    if self._client:\n        await self._client.__aexit__(exc_type, exc_val, exc_tb)\n        self._client = None\n</code></pre>"},{"location":"api/tools/web/#web-fetch-tool","title":"Web Fetch Tool","text":"<p>Fetches a web page and returns its content as markdown.</p>"},{"location":"api/tools/web/#stirrup.tools.web.FetchWebPageParams","title":"stirrup.tools.web.FetchWebPageParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for web page fetch tool.</p>"},{"location":"api/tools/web/#stirrup.tools.web.FetchWebPageParams.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: Annotated[\n    str,\n    Field(\n        description=\"Full HTTP or HTTPS URL of the web page to fetch and extract\"\n    ),\n]\n</code></pre>"},{"location":"api/tools/web/#web-search-tool","title":"Web Search Tool","text":"<p>Searches the web using the Brave Search API.</p> <p>Note</p> <p>Requires <code>BRAVE_API_KEY</code> environment variable.</p>"},{"location":"api/tools/web/#stirrup.tools.web.WebSearchParams","title":"stirrup.tools.web.WebSearchParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for web search tool.</p>"},{"location":"api/tools/web/#stirrup.tools.web.WebSearchParams.query","title":"query  <code>instance-attribute</code>","text":"<pre><code>query: Annotated[\n    str,\n    Field(\n        description=\"Natural language search query for Brave Search (similar to Google search syntax)\"\n    ),\n]\n</code></pre>"},{"location":"api/utils/logging/","title":"Logging Utilities","text":""},{"location":"api/utils/logging/#stirrup.utils.logging","title":"stirrup.utils.logging","text":"<p>Rich logging for agent workflows with visual hierarchy.</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['AgentLogger', 'AgentLoggerBase', 'console']\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.console","title":"console  <code>module-attribute</code>","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.SUBAGENT_INDENT_SPACES","title":"SUBAGENT_INDENT_SPACES  <code>module-attribute</code>","text":"<pre><code>SUBAGENT_INDENT_SPACES: int = 8\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AssistantMessage","title":"AssistantMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response message with optional tool calls and token usage tracking.</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.AssistantMessage.e2e_otps","title":"e2e_otps  <code>property</code>","text":"<pre><code>e2e_otps: float | None\n</code></pre> <p>End-to-end output tokens per second.</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Tool execution result returned to the LLM.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['tool']</code> <p>Always \"tool\"</p> <code>content</code> <code>Content</code> <p>The tool result content</p> <code>tool_call_id</code> <code>str | None</code> <p>ID linking this result to the corresponding tool call</p> <code>name</code> <code>str | None</code> <p>Name of the tool that was called</p> <code>args_was_valid</code> <code>bool</code> <p>Whether the tool arguments were valid</p> <code>success</code> <code>bool</code> <p>Whether the tool executed successfully (used by finish tool to control termination)</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.ToolMessage.tool_duration","title":"tool_duration  <code>property</code>","text":"<pre><code>tool_duration: float | None\n</code></pre> <p>Tool execution duration in seconds.</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.UserMessage","title":"UserMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>User input message to the LLM.</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase","title":"AgentLoggerBase","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for agent loggers.</p> <p>Defines the interface that Agent uses for logging. Implement this to create custom loggers (e.g., for testing, file output, or monitoring services).</p> <p>Properties are set by Agent after construction: - name, model, max_turns, depth: Agent configuration - finish_params, run_metadata, output_dir: Set before exit for final stats</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.__enter__","title":"__enter__  <code>abstractmethod</code>","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter logging context. Called when agent session starts.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef __enter__(self) -&gt; Self:\n    \"\"\"Enter logging context. Called when agent session starts.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.__exit__","title":"__exit__  <code>abstractmethod</code>","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Exit logging context. Called when agent session ends.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Exit logging context. Called when agent session ends.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.on_step","title":"on_step  <code>abstractmethod</code>","text":"<pre><code>on_step(\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None\n</code></pre> <p>Report step progress and stats during agent execution.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef on_step(\n    self,\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None:\n    \"\"\"Report step progress and stats during agent execution.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.assistant_message","title":"assistant_message  <code>abstractmethod</code>","text":"<pre><code>assistant_message(\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None\n</code></pre> <p>Log an assistant message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef assistant_message(\n    self,\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None:\n    \"\"\"Log an assistant message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.user_message","title":"user_message  <code>abstractmethod</code>","text":"<pre><code>user_message(user_message: UserMessage) -&gt; None\n</code></pre> <p>Log a user message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef user_message(self, user_message: UserMessage) -&gt; None:\n    \"\"\"Log a user message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.task_message","title":"task_message  <code>abstractmethod</code>","text":"<pre><code>task_message(task: str | list[Any]) -&gt; None\n</code></pre> <p>Log the initial task/prompt at the start of a run.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef task_message(self, task: str | list[Any]) -&gt; None:\n    \"\"\"Log the initial task/prompt at the start of a run.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.tool_result","title":"tool_result  <code>abstractmethod</code>","text":"<pre><code>tool_result(tool_message: ToolMessage) -&gt; None\n</code></pre> <p>Log a tool execution result.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef tool_result(self, tool_message: ToolMessage) -&gt; None:\n    \"\"\"Log a tool execution result.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.context_summarization_start","title":"context_summarization_start  <code>abstractmethod</code>","text":"<pre><code>context_summarization_start(\n    pct_used: float, cutoff: float\n) -&gt; None\n</code></pre> <p>Log that context summarization is starting.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef context_summarization_start(self, pct_used: float, cutoff: float) -&gt; None:\n    \"\"\"Log that context summarization is starting.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.context_summarization_complete","title":"context_summarization_complete  <code>abstractmethod</code>","text":"<pre><code>context_summarization_complete(\n    summary: str, bridge: str\n) -&gt; None\n</code></pre> <p>Log completed context summarization.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef context_summarization_complete(self, summary: str, bridge: str) -&gt; None:\n    \"\"\"Log completed context summarization.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.debug","title":"debug  <code>abstractmethod</code>","text":"<pre><code>debug(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a debug message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef debug(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a debug message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.info","title":"info  <code>abstractmethod</code>","text":"<pre><code>info(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an info message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef info(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an info message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.warning","title":"warning  <code>abstractmethod</code>","text":"<pre><code>warning(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a warning message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef warning(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a warning message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.error","title":"error  <code>abstractmethod</code>","text":"<pre><code>error(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>@abstractmethod\ndef error(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.pause_live","title":"pause_live","text":"<pre><code>pause_live() -&gt; None\n</code></pre> <p>Pause live display (e.g., spinner) before user interaction.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def pause_live(self) -&gt; None:  # noqa: B027\n    \"\"\"Pause live display (e.g., spinner) before user interaction.\"\"\"\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLoggerBase.resume_live","title":"resume_live","text":"<pre><code>resume_live() -&gt; None\n</code></pre> <p>Resume live display after user interaction.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def resume_live(self) -&gt; None:  # noqa: B027\n    \"\"\"Resume live display after user interaction.\"\"\"\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger","title":"AgentLogger","text":"<pre><code>AgentLogger(\n    *, show_spinner: bool = True, level: int = INFO\n)\n</code></pre> <p>               Bases: <code>AgentLoggerBase</code></p> <p>Rich console logger for agent workflows.</p> <p>Implements AgentLoggerBase with rich formatting, spinners, and visual hierarchy. Each agent (including sub-agents) should have its own logger instance.</p> Usage <p>from stirrup.clients.chat_completions_client import ChatCompletionsClient</p> <p>Initialize the agent logger.</p> <p>Parameters:</p> Name Type Description Default <code>show_spinner</code> <code>bool</code> <p>Whether to show a spinner while agent runs (only for depth=0)</p> <code>True</code> <code>level</code> <code>int</code> <p>Logging level (default: INFO)</p> <code>INFO</code> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __init__(\n    self,\n    *,\n    show_spinner: bool = True,\n    level: int = logging.INFO,\n) -&gt; None:\n    \"\"\"Initialize the agent logger.\n\n    Args:\n        show_spinner: Whether to show a spinner while agent runs (only for depth=0)\n        level: Logging level (default: INFO)\n    \"\"\"\n    # Properties set by Agent before __enter__\n    self.name: str = \"agent\"\n    self.model: str | None = None\n    self.max_turns: int | None = None\n    self.depth: int = 0\n\n    # State set by Agent before __exit__\n    self.finish_params: BaseModel | None = None\n    self.run_metadata: dict[str, list[Any]] | None = None\n    self.output_dir: str | None = None\n\n    # Configuration\n    self._show_spinner = show_spinner\n    self._level = level\n\n    # Spinner state (only used when depth == 0 and show_spinner is True)\n    self._current_step = 0\n    self._tool_calls = 0\n    self._input_tokens = 0\n    self._output_tokens = 0\n    self._live: Live | None = None\n\n    # Configure rich logging on first logger creation\n    self._configure_logging()\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--agent-creates-logger-internally-by-default","title":"Agent creates logger internally by default","text":"<p>client = ChatCompletionsClient(model=\"gpt-4\") agent = Agent(client=client, name=\"assistant\")</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--or-pass-a-pre-configured-logger","title":"Or pass a pre-configured logger","text":"<p>logger = AgentLogger(show_spinner=False) agent = Agent(client=client, name=\"assistant\", logger=logger)</p>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--agent-sets-these-properties-before-calling-enter","title":"Agent sets these properties before calling enter:","text":""},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--loggername-loggermodel-loggermax_turns-loggerdepth","title":"logger.name, logger.model, logger.max_turns, logger.depth","text":""},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--agent-sets-these-before-calling-exit","title":"Agent sets these before calling exit:","text":""},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger--loggerfinish_params-loggerrun_metadata-loggeroutput_dir","title":"logger.finish_params, logger.run_metadata, logger.output_dir","text":""},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter logging context. Logs agent start and starts spinner if depth=0.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter logging context. Logs agent start and starts spinner if depth=0.\"\"\"\n    # Log agent start (rule + system prompt display)\n    indent_spaces = self.depth * SUBAGENT_INDENT_SPACES\n\n    # Build title with optional model info\n    model_str = f\" ({self.model})\" if self.model else \"\"\n    if self.depth == 0:\n        title = f\"\u25b6 {self.name}{model_str}\"\n        console.rule(f\"[bold cyan]{title}[/]\", style=\"cyan\")\n    else:\n        title = f\"\u25b6 {self.name}: Level {self.depth}{model_str}\"\n        rule = Rule(f\"[bold cyan]{title}[/]\", style=\"cyan\")\n        self._print_indented(rule, indent_spaces)\n    console.print()\n\n    # Start spinner only for top-level agent\n    if self.depth == 0 and self._show_spinner:\n        self._live = Live(self._make_spinner(), console=console, refresh_per_second=10)\n        self._live.start()\n\n    return self\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.__exit__","title":"__exit__","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None\n</code></pre> <p>Exit logging context. Stops spinner and logs completion stats.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: object,\n) -&gt; None:\n    \"\"\"Exit logging context. Stops spinner and logs completion stats.\"\"\"\n    # Stop spinner first\n    if self._live:\n        self._live.stop()\n        self._live = None\n\n    error = str(exc_val) if exc_type is not None else None\n    self._log_finish(error=error)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.on_step","title":"on_step","text":"<pre><code>on_step(\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None\n</code></pre> <p>Report step progress and stats during agent execution.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def on_step(\n    self,\n    step: int,\n    tool_calls: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n) -&gt; None:\n    \"\"\"Report step progress and stats during agent execution.\"\"\"\n    self._current_step = step\n    self._tool_calls = tool_calls\n    self._input_tokens = input_tokens\n    self._output_tokens = output_tokens\n    if self._live:\n        self._live.update(self._make_spinner())\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.pause_live","title":"pause_live","text":"<pre><code>pause_live() -&gt; None\n</code></pre> <p>Pause the live spinner display.</p> <p>Call this before prompting for user input to prevent the spinner from interfering with the input prompt.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def pause_live(self) -&gt; None:\n    \"\"\"Pause the live spinner display.\n\n    Call this before prompting for user input to prevent the spinner\n    from interfering with the input prompt.\n    \"\"\"\n    if self._live is not None:\n        self._live.stop()\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.resume_live","title":"resume_live","text":"<pre><code>resume_live() -&gt; None\n</code></pre> <p>Resume the live spinner display.</p> <p>Call this after user input is complete to restart the spinner.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def resume_live(self) -&gt; None:\n    \"\"\"Resume the live spinner display.\n\n    Call this after user input is complete to restart the spinner.\n    \"\"\"\n    if self._live is not None:\n        self._live.start()\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.set_level","title":"set_level","text":"<pre><code>set_level(level: int) -&gt; None\n</code></pre> <p>Set the logging level.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def set_level(self, level: int) -&gt; None:\n    \"\"\"Set the logging level.\"\"\"\n    self._level = level\n    # Also update root logger level\n    logging.getLogger().setLevel(level)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.is_enabled_for","title":"is_enabled_for","text":"<pre><code>is_enabled_for(level: int) -&gt; bool\n</code></pre> <p>Check if a given log level is enabled.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def is_enabled_for(self, level: int) -&gt; bool:\n    \"\"\"Check if a given log level is enabled.\"\"\"\n    return level &gt;= self._level\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.debug","title":"debug","text":"<pre><code>debug(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a debug message (dim style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def debug(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a debug message (dim style).\"\"\"\n    if self._level &lt;= logging.DEBUG:\n        formatted = message % args if args else message\n        console.print(f\"[dim]{formatted}[/]\")\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.info","title":"info","text":"<pre><code>info(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an info message.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def info(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an info message.\"\"\"\n    if self._level &lt;= logging.INFO:\n        formatted = message % args if args else message\n        console.print(formatted)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.warning","title":"warning","text":"<pre><code>warning(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a warning message (yellow style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def warning(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a warning message (yellow style).\"\"\"\n    if self._level &lt;= logging.WARNING:\n        formatted = message % args if args else message\n        console.print(f\"[yellow]\u26a0 {formatted}[/]\")\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.error","title":"error","text":"<pre><code>error(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message (red style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def error(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message (red style).\"\"\"\n    if self._level &lt;= logging.ERROR:\n        formatted = message % args if args else message\n        console.print(f\"[red]\u2717 {formatted}[/]\")\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.critical","title":"critical","text":"<pre><code>critical(message: str, *args: object) -&gt; None\n</code></pre> <p>Log a critical message (bold red style).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def critical(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log a critical message (bold red style).\"\"\"\n    if self._level &lt;= logging.CRITICAL:\n        formatted = message % args if args else message\n        console.print(f\"[bold red]\u2717 CRITICAL: {formatted}[/]\")\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.exception","title":"exception","text":"<pre><code>exception(message: str, *args: object) -&gt; None\n</code></pre> <p>Log an error message with exception traceback (red style with traceback).</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def exception(self, message: str, *args: object) -&gt; None:\n    \"\"\"Log an error message with exception traceback (red style with traceback).\"\"\"\n    if self._level &lt;= logging.ERROR:\n        formatted = message % args if args else message\n        console.print(f\"[red]\u2717 {formatted}[/]\")\n        console.print_exception()\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.assistant_message","title":"assistant_message","text":"<pre><code>assistant_message(\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None\n</code></pre> <p>Log an assistant message with content and tool calls in a panel.</p> <p>Parameters:</p> Name Type Description Default <code>turn</code> <code>int</code> <p>Current turn number (1-indexed)</p> required <code>max_turns</code> <code>int</code> <p>Maximum number of turns</p> required <code>assistant_message</code> <code>AssistantMessage</code> <p>The assistant's response message</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def assistant_message(\n    self,\n    turn: int,\n    max_turns: int,\n    assistant_message: AssistantMessage,\n) -&gt; None:\n    \"\"\"Log an assistant message with content and tool calls in a panel.\n\n    Args:\n        turn: Current turn number (1-indexed)\n        max_turns: Maximum number of turns\n        assistant_message: The assistant's response message\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Build panel content\n    content = Text()\n\n    # Add assistant content if present\n    if assistant_message.content:\n        text = assistant_message.content\n        if isinstance(text, list):\n            text = \"\\n\".join(str(block) for block in text)\n        # Truncate long content\n        if len(text) &gt; 500:\n            text = text[:500] + \"...\"\n        content.append(text, style=\"white\")\n\n    # Add tool calls if present\n    if assistant_message.tool_calls:\n        if assistant_message.content:\n            content.append(\"\\n\\n\")\n        content.append(\"Tool Calls:\\n\", style=\"bold magenta\")\n        for tc in assistant_message.tool_calls:\n            content.append(f\"  \ud83d\udd27 {tc.name}\", style=\"magenta\")\n            if tc.arguments and tc.arguments.strip():\n                args_parsed = json.loads(tc.arguments)\n                args_formatted = json.dumps(args_parsed, indent=2, ensure_ascii=False)\n                args_preview = args_formatted[:1000] + \"...\" if len(args_formatted) &gt; 1000 else args_formatted\n                content.append(args_preview, style=\"dim\")\n\n    # Create and print panel with agent name in title\n    title = f\"[bold]AssistantMessage[/bold] \u2502 {self.name} \u2502 Turn {turn}/{max_turns}\"\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"yellow\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.user_message","title":"user_message","text":"<pre><code>user_message(user_message: UserMessage) -&gt; None\n</code></pre> <p>Log a user message in a panel.</p> <p>Parameters:</p> Name Type Description Default <code>user_message</code> <code>UserMessage</code> <p>The user's message</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def user_message(self, user_message: UserMessage) -&gt; None:\n    \"\"\"Log a user message in a panel.\n\n    Args:\n        user_message: The user's message\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Build panel content\n    content = Text()\n\n    # Add user content\n    if user_message.content:\n        text = user_message.content\n        if isinstance(text, list):\n            text = \"\\n\".join(str(block) for block in text)\n        # Truncate long content\n        if len(text) &gt; 500:\n            text = text[:500] + \"...\"\n        content.append(text, style=\"white\")\n\n    # Create and print panel with agent name in title\n    title = f\"[bold]UserMessage[/bold] \u2502 {self.name}\"\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"blue\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.task_message","title":"task_message","text":"<pre><code>task_message(task: str | list[Any]) -&gt; None\n</code></pre> <p>Log the initial task/prompt at the start of a run.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def task_message(self, task: str | list[Any]) -&gt; None:\n    \"\"\"Log the initial task/prompt at the start of a run.\"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    # Convert list content to string\n    if isinstance(task, list):\n        task = \"\\n\".join(str(block) for block in task)\n\n    # Clean up whitespace from multi-line strings\n    # Normalize each line by stripping leading/trailing whitespace and rejoining\n    lines = [line.strip() for line in task.split(\"\\n\")]\n    task = \" \".join(line for line in lines if line)\n\n    # Use \"Sub Agent\" prefix for nested agents\n    prefix = \"Sub Agent\" if self.depth &gt; 0 else \"Agent\"\n\n    if self.depth &gt; 0:\n        indent = \" \" * (self.depth * SUBAGENT_INDENT_SPACES)\n        console.print(f\"{indent}[bold]{prefix} Task:[/bold]\")\n        console.print()\n        for line in task.split(\"\\n\"):\n            console.print(f\"{indent}{line}\")\n    else:\n        console.print(f\"[bold]{prefix} Task:[/bold]\")\n        console.print()\n        console.print(task)\n\n    console.print()  # Add gap after task section\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.warnings_message","title":"warnings_message","text":"<pre><code>warnings_message(warnings: list[str]) -&gt; None\n</code></pre> <p>Display warnings at run start as simple text.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def warnings_message(self, warnings: list[str]) -&gt; None:\n    \"\"\"Display warnings at run start as simple text.\"\"\"\n    if self._level &gt; logging.INFO or not warnings:\n        return\n\n    console.print(\"[bold orange1]Warnings[/bold orange1]\")\n    console.print()\n    for warning in warnings:\n        console.print(f\"[orange1]\u26a0 {warning}[/orange1]\")\n        console.print()  # Add gap between warnings\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.tool_result","title":"tool_result","text":"<pre><code>tool_result(tool_message: ToolMessage) -&gt; None\n</code></pre> <p>Log a single tool execution result in a panel with XML syntax highlighting.</p> <p>Parameters:</p> Name Type Description Default <code>tool_message</code> <code>ToolMessage</code> <p>The tool execution result</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def tool_result(self, tool_message: ToolMessage) -&gt; None:\n    \"\"\"Log a single tool execution result in a panel with XML syntax highlighting.\n\n    Args:\n        tool_message: The tool execution result\n    \"\"\"\n    if self._level &gt; logging.INFO:\n        return\n\n    tool_name = tool_message.name or \"unknown\"\n\n    # Get result content\n    result_text = tool_message.content\n    if isinstance(result_text, list):\n        result_text = \"\\n\".join(str(block) for block in result_text)\n\n    # Unescape HTML entities (e.g., &amp;lt; -&gt; &lt;, &amp;gt; -&gt; &gt;, &amp;amp; -&gt; &amp;)\n    result_text = html.unescape(result_text)\n\n    # Truncate long results (keeps start and end, removes middle)\n    result_text = truncate_msg(result_text, 1000)\n\n    # Format as XML with syntax highlighting\n    content = Syntax(result_text, \"xml\", theme=\"monokai\", word_wrap=True)\n\n    # Status indicator in title with agent name\n    status = \"\u2713\" if tool_message.args_was_valid else \"\u2717\"\n    status_style = \"green\" if tool_message.args_was_valid else \"red\"\n    title = f\"[{status_style}]{status}[/{status_style}] [bold]ToolResult[/bold] \u2502 {self.name} \u2502 [green]{tool_name}[/green]\"\n\n    panel = Panel(content, title=title, title_align=\"left\", border_style=\"green\", padding=(0, 1))\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.context_summarization_start","title":"context_summarization_start","text":"<pre><code>context_summarization_start(\n    pct_used: float, cutoff: float\n) -&gt; None\n</code></pre> <p>Log context window summarization starting in an orange panel.</p> <p>Parameters:</p> Name Type Description Default <code>pct_used</code> <code>float</code> <p>Percentage of context window currently used (0.0-1.0)</p> required <code>cutoff</code> <code>float</code> <p>The threshold that triggered summarization (0.0-1.0)</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def context_summarization_start(self, pct_used: float, cutoff: float) -&gt; None:\n    \"\"\"Log context window summarization starting in an orange panel.\n\n    Args:\n        pct_used: Percentage of context window currently used (0.0-1.0)\n        cutoff: The threshold that triggered summarization (0.0-1.0)\n    \"\"\"\n    # Build panel content\n    content = Text()\n    content.append(\"Context window limit reached\\n\\n\", style=\"bold\")\n    content.append(\"Used: \", style=\"dim\")\n    content.append(f\"{pct_used:.1%}\", style=\"bold orange1\")\n    content.append(\"  \u2502  \", style=\"dim\")\n    content.append(\"Threshold: \", style=\"dim\")\n    content.append(f\"{cutoff:.1%}\", style=\"bold\")\n    content.append(\"\\n\\n\", style=\"dim\")\n    content.append(\"Summarizing conversation history...\", style=\"italic\")\n\n    panel = Panel(\n        content,\n        title=\"[bold orange1]\ud83d\udcdd Context Summarization[/]\",\n        title_align=\"left\",\n        border_style=\"orange1\",\n        padding=(0, 1),\n    )\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.AgentLogger.context_summarization_complete","title":"context_summarization_complete","text":"<pre><code>context_summarization_complete(\n    summary: str, bridge: str\n) -&gt; None\n</code></pre> <p>Log the completed context summarization with summary content.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>str</code> <p>The generated summary of the conversation</p> required <code>bridge</code> <code>str</code> <p>The bridge message that will be used to continue the conversation</p> required Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def context_summarization_complete(self, summary: str, bridge: str) -&gt; None:\n    \"\"\"Log the completed context summarization with summary content.\n\n    Args:\n        summary: The generated summary of the conversation\n        bridge: The bridge message that will be used to continue the conversation\n    \"\"\"\n    # Truncate long summaries for display\n    summary_display = summary\n    if len(summary_display) &gt; 800:\n        summary_display = summary_display[:800] + \"...\"\n\n    # Build panel content\n    content = Text()\n    content.append(\"Summary:\\n\", style=\"bold\")\n    content.append(summary_display, style=\"white\")\n\n    if self._level &gt; logging.INFO:\n        bridge_display = bridge\n        if len(bridge_display) &gt; 200:\n            bridge_display = bridge_display[:200] + \"...\"\n        content.append(\"\\n\\n\")\n        content.append(\"Bridge Message:\\n\", style=\"bold dim\")\n        content.append(bridge_display, style=\"dim italic\")\n\n    panel = Panel(\n        content,\n        title=\"[bold green]\u2713 Summary Generated[/]\",\n        title_align=\"left\",\n        border_style=\"green\",\n        padding=(0, 1),\n    )\n\n    if self.depth &gt; 0:\n        self._print_indented(panel, self.depth * SUBAGENT_INDENT_SPACES)\n    else:\n        console.print(panel)\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._aggregate_list","title":"_aggregate_list","text":"<pre><code>_aggregate_list(metadata_list: list[T]) -&gt; T | None\n</code></pre> <p>Aggregate a list of metadata using add.</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def _aggregate_list[T: Addable](metadata_list: list[T]) -&gt; T | None:\n    \"\"\"Aggregate a list of metadata using __add__.\"\"\"\n    if not metadata_list:\n        return None\n    aggregated = metadata_list[0]\n    for m in metadata_list[1:]:\n        aggregated = aggregated + m\n    return aggregated\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.aggregate_metadata","title":"aggregate_metadata","text":"<pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: Literal[True] = True,\n) -&gt; object\n</code></pre><pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: Literal[False] = ...,\n) -&gt; dict\n</code></pre> <pre><code>aggregate_metadata(\n    metadata_dict: dict[str, list[Any]],\n    prefix: str = \"\",\n    return_json_serializable: bool = True,\n) -&gt; dict | object\n</code></pre> <p>Aggregate metadata lists and flatten sub-agents into a single-level dict with hierarchical keys.</p> <p>For entries with nested run_metadata (e.g., SubAgentMetadata), flattens sub-agents using dot notation. Each sub-agent's value is a dict mapping its direct tool names to their aggregated metadata (excluding nested sub-agent data, which gets its own top-level key).</p> <p>At the root level, token_usage is rolled up to include all sub-agent token usage.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_dict</code> <code>dict[str, list[Any]]</code> <p>Dict mapping names (tools or agents) to lists of metadata instances</p> required <code>prefix</code> <code>str</code> <p>Key prefix for nested calls (used internally for recursion)</p> <code>''</code> <p>Returns:</p> Name Type Description <code>dict | object</code> <p>Flat dict with dot-notation keys for sub-agents.</p> <code>Example</code> <code>dict | object</code> <p>{ \"token_usage\": , \"web_browsing_sub_agent\": {\"web_search\": , \"token_usage\": }, \"web_browsing_sub_agent.web_fetch_sub_agent\": {\"fetch_web_page\": , \"token_usage\": } <code>dict | object</code> <p>}</p> Source code in <code>src/stirrup/core/models.py</code> <pre><code>def aggregate_metadata(\n    metadata_dict: dict[str, list[Any]], prefix: str = \"\", return_json_serializable: bool = True\n) -&gt; dict | object:\n    \"\"\"Aggregate metadata lists and flatten sub-agents into a single-level dict with hierarchical keys.\n\n    For entries with nested run_metadata (e.g., SubAgentMetadata), flattens sub-agents using dot notation.\n    Each sub-agent's value is a dict mapping its direct tool names to their aggregated metadata\n    (excluding nested sub-agent data, which gets its own top-level key).\n\n    At the root level, token_usage is rolled up to include all sub-agent token usage.\n\n    Args:\n        metadata_dict: Dict mapping names (tools or agents) to lists of metadata instances\n        prefix: Key prefix for nested calls (used internally for recursion)\n\n    Returns:\n        Flat dict with dot-notation keys for sub-agents.\n        Example: {\n            \"token_usage\": &lt;combined from all agents&gt;,\n            \"web_browsing_sub_agent\": {\"web_search\": &lt;aggregated&gt;, \"token_usage\": &lt;aggregated&gt;},\n            \"web_browsing_sub_agent.web_fetch_sub_agent\": {\"fetch_web_page\": &lt;aggregated&gt;, \"token_usage\": &lt;aggregated&gt;}\n        }\n    \"\"\"\n    result: dict = {}\n\n    # First pass: aggregate all entries in this level (skip internal keys prefixed with _)\n    aggregated_level: dict = {}\n    for name, metadata_list in metadata_dict.items():\n        if name.startswith(\"_\") or not metadata_list:\n            continue\n        aggregated_level[name] = _aggregate_list(metadata_list)\n\n    # Second pass: separate nested sub-agents from direct tools, and recurse\n    direct_tools: dict = {}\n    for name, aggregated in aggregated_level.items():\n        if hasattr(aggregated, \"run_metadata\") and isinstance(aggregated.run_metadata, dict):\n            # This is a sub-agent - recurse into it\n            full_key = f\"{prefix}.{name}\" if prefix else name\n            nested = aggregate_metadata(aggregated.run_metadata, prefix=full_key, return_json_serializable=False)\n            result.update(nested)\n        else:\n            # This is a direct tool/metadata - keep it at this level\n            direct_tools[name] = aggregated\n\n    # Store direct tools under the current prefix\n    if prefix:\n        result[prefix] = direct_tools\n    else:\n        # At root level, merge direct tools into result\n        result.update(direct_tools)\n\n    # At root level, roll up all token_usage from sub-agents\n    if not prefix:\n        total_token_usage = _collect_all_token_usage(result)\n        if total_token_usage.total &gt; 0:\n            result[\"token_usage\"] = [total_token_usage]\n\n    if return_json_serializable:\n        # Convert all Pydantic models to JSON-serializable dicts\n        return to_json_serializable(result)\n    return result\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging.truncate_msg","title":"truncate_msg","text":"<pre><code>truncate_msg(msg: str, max_length: int) -&gt; str\n</code></pre> <p>Truncate long messages by removing middle portion, keeping start and end with ellipsis indicator.</p> Source code in <code>src/stirrup/utils/text.py</code> <pre><code>def truncate_msg(msg: str, max_length: int) -&gt; str:\n    \"\"\"Truncate long messages by removing middle portion, keeping start and end with ellipsis indicator.\"\"\"\n    msg_len = len(msg)\n    if msg_len &lt;= max_length:\n        return msg\n    else:\n        return (\n            msg[: max_length // 2]\n            + f\"\\n... This content has been truncated from an original {msg_len} characters to stay below {max_length} characters ...\\n\"\n            + msg[-max_length // 2 :]\n        )\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._is_subagent_metadata","title":"_is_subagent_metadata","text":"<pre><code>_is_subagent_metadata(data: object) -&gt; bool\n</code></pre> <p>Check if data represents sub-agent metadata.</p> <p>Sub-agent metadata can be: - A Pydantic SubAgentMetadata object with run_metadata attribute - A dict where all values are dicts/objects (from aggregate_metadata flattening)</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def _is_subagent_metadata(data: object) -&gt; bool:\n    \"\"\"Check if data represents sub-agent metadata.\n\n    Sub-agent metadata can be:\n    - A Pydantic SubAgentMetadata object with run_metadata attribute\n    - A dict where all values are dicts/objects (from aggregate_metadata flattening)\n    \"\"\"\n    # Check for Pydantic SubAgentMetadata object\n    if hasattr(data, \"run_metadata\") and isinstance(data.run_metadata, dict):\n        return True\n    # Check for flattened dict of dicts (from aggregate_metadata)\n    if isinstance(data, dict) and data:\n        return all(isinstance(v, dict) or hasattr(v, \"model_dump\") for v in data.values())\n    return False\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._format_token_usage","title":"_format_token_usage","text":"<pre><code>_format_token_usage(data: object) -&gt; str\n</code></pre> <p>Format token_usage (dict or TokenUsage object) as a human-readable string.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def _format_token_usage(data: object) -&gt; str:\n    \"\"\"Format token_usage (dict or TokenUsage object) as a human-readable string.\"\"\"\n    if isinstance(data, dict):\n        # Dict representation\n        data_dict = cast(dict[str, Any], data)\n        input_tokens: int = data_dict.get(\"input\", 0)\n        answer_tokens: int = data_dict.get(\"answer\", 0)\n        reasoning_tokens: int = data_dict.get(\"reasoning\", 0)\n    elif hasattr(data, \"input\") and hasattr(data, \"answer\"):\n        # Pydantic TokenUsage object - use getattr for type safety\n        input_tokens = int(getattr(data, \"input\", 0))\n        answer_tokens = int(getattr(data, \"answer\", 0))\n        reasoning_tokens = int(getattr(data, \"reasoning\", 0))\n    else:\n        return str(data)\n    total = input_tokens + answer_tokens + reasoning_tokens\n    return f\"{total:,} tokens\"\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._collect_model_speed_stats","title":"_collect_model_speed_stats","text":"<pre><code>_collect_model_speed_stats(\n    run_metadata: dict[str, list[Any]],\n) -&gt; dict[str, dict[str, float | int | str]]\n</code></pre> <p>Collect _model_speed stats from run_metadata, merging across sub-agents by model slug.</p> <p>Each agent stores _model_speed as a flat dict with a single model_slug. This function merges stats from the current agent and all sub-agents, grouping by model_slug to produce per-model totals.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def _collect_model_speed_stats(\n    run_metadata: dict[str, list[Any]],\n) -&gt; dict[str, dict[str, float | int | str]]:\n    \"\"\"Collect _model_speed stats from run_metadata, merging across sub-agents by model slug.\n\n    Each agent stores _model_speed as a flat dict with a single model_slug.\n    This function merges stats from the current agent and all sub-agents,\n    grouping by model_slug to produce per-model totals.\n    \"\"\"\n    merged: dict[str, dict[str, float | int | str]] = {}\n\n    def _merge(stats: dict) -&gt; None:\n        slug = stats.get(\"model_slug\")\n        if not slug:\n            return\n        if slug not in merged:\n            merged[slug] = {\"model_slug\": slug, \"num_calls\": 0, \"output_tokens\": 0, \"duration\": 0.0}\n        merged[slug][\"num_calls\"] = int(merged[slug][\"num_calls\"]) + int(stats.get(\"num_calls\", 0))\n        merged[slug][\"output_tokens\"] = int(merged[slug][\"output_tokens\"]) + int(stats.get(\"output_tokens\", 0))\n        merged[slug][\"duration\"] = float(merged[slug][\"duration\"]) + float(stats.get(\"duration\", 0.0))\n\n    # Direct _model_speed at this level (flat dict with model_slug key)\n    direct = run_metadata.get(\"_model_speed\")\n    if isinstance(direct, dict) and direct:\n        _merge(direct)\n\n    # Recurse into sub-agent metadata\n    for key, value_list in run_metadata.items():\n        if key.startswith(\"_\"):\n            continue\n        if not isinstance(value_list, list):\n            continue\n        for item in value_list:\n            if hasattr(item, \"run_metadata\") and isinstance(item.run_metadata, dict):\n                nested = _collect_model_speed_stats(item.run_metadata)\n                for stats in nested.values():\n                    _merge(stats)\n\n    # Recompute e2e_otps after merging\n    for stats in merged.values():\n        dur = float(stats[\"duration\"])\n        out = int(stats[\"output_tokens\"])\n        stats[\"e2e_otps\"] = out / dur if dur &gt; 0 else 0.0\n\n    return merged\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._get_nested_tools","title":"_get_nested_tools","text":"<pre><code>_get_nested_tools(data: object) -&gt; dict[str, object]\n</code></pre> <p>Extract nested tools dict from sub-agent metadata.</p> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def _get_nested_tools(data: object) -&gt; dict[str, object]:\n    \"\"\"Extract nested tools dict from sub-agent metadata.\"\"\"\n    if hasattr(data, \"run_metadata\"):\n        # Pydantic SubAgentMetadata - return its run_metadata\n        run_metadata = data.run_metadata\n        if isinstance(run_metadata, dict):\n            return cast(dict[str, object], run_metadata)\n    if isinstance(data, dict):\n        # Already a dict\n        return cast(dict[str, object], data)\n    return {}\n</code></pre>"},{"location":"api/utils/logging/#stirrup.utils.logging._add_tool_branch","title":"_add_tool_branch","text":"<pre><code>_add_tool_branch(\n    parent: Tree,\n    tool_name: str,\n    tool_data: object,\n    skip_fields: set[str],\n    tool_durations: dict[str, list[float]] | None = None,\n) -&gt; None\n</code></pre> <p>Add a tool entry to the tree, handling nested sub-agent data recursively.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Tree</code> <p>The tree or branch to add to</p> required <code>tool_name</code> <code>str</code> <p>Name of the tool or sub-agent</p> required <code>tool_data</code> <code>object</code> <p>The tool's metadata (dict, Pydantic model, list, or scalar)</p> required <code>skip_fields</code> <code>set[str]</code> <p>Fields to skip when displaying dict contents</p> required <code>tool_durations</code> <code>dict[str, list[float]] | None</code> <p>Optional mapping of tool name to list of execution durations</p> <code>None</code> Source code in <code>src/stirrup/utils/logging.py</code> <pre><code>def _add_tool_branch(\n    parent: Tree,\n    tool_name: str,\n    tool_data: object,\n    skip_fields: set[str],\n    tool_durations: dict[str, list[float]] | None = None,\n) -&gt; None:\n    \"\"\"Add a tool entry to the tree, handling nested sub-agent data recursively.\n\n    Args:\n        parent: The tree or branch to add to\n        tool_name: Name of the tool or sub-agent\n        tool_data: The tool's metadata (dict, Pydantic model, list, or scalar)\n        skip_fields: Fields to skip when displaying dict contents\n        tool_durations: Optional mapping of tool name to list of execution durations\n    \"\"\"\n    # Special case: token_usage formatted as total tokens\n    if tool_name == \"token_usage\":\n        if isinstance(tool_data, list) and tool_data:\n            parent.add(f\"[dim]token_usage:[/] {_format_token_usage(tool_data[0])}\")\n        else:\n            parent.add(f\"[dim]token_usage:[/] {_format_token_usage(tool_data)}\")\n        return\n    # Case 1: List \u2192 aggregate using __add__, then recurse\n    if isinstance(tool_data, list) and tool_data:\n        aggregated = _aggregate_list(tool_data)\n        if aggregated is not None:\n            _add_tool_branch(parent, tool_name, aggregated, skip_fields, tool_durations)\n        return\n\n    # Case 2: SubAgentMetadata \u2192 recurse into run_metadata only\n    if _is_subagent_metadata(tool_data):\n        branch = parent.add(f\"[magenta]{tool_name}[/]\")\n        for nested_name, nested_data in sorted(_get_nested_tools(tool_data).items()):\n            if nested_name.startswith(\"_\"):\n                continue\n            _add_tool_branch(branch, nested_name, nested_data, skip_fields, tool_durations)\n        return\n\n    # Case 3: Leaf node - display fields as branches\n    # Convert to dict if Pydantic model\n    if hasattr(tool_data, \"model_dump\"):\n        data_dict = cast(Callable[[], dict[str, Any]], tool_data.model_dump)()\n    elif isinstance(tool_data, dict):\n        data_dict = cast(dict[str, Any], tool_data)\n    else:\n        # Scalar value - just show it inline\n        parent.add(f\"[magenta]{tool_name}[/]: {tool_data}\")\n        return\n\n    # Show num_uses inline with the tool name if present, plus avg duration\n    num_uses = data_dict.get(\"num_uses\")\n    durations = tool_durations.get(tool_name, []) if tool_durations else []\n    avg_duration_str = \"\"\n    if durations:\n        avg_dur = sum(durations) / len(durations)\n        avg_duration_str = f\", avg {avg_dur:.2f}s\"\n\n    if num_uses is not None:\n        branch = parent.add(f\"[magenta]{tool_name}[/]: {num_uses} call(s){avg_duration_str}\")\n    else:\n        branch = parent.add(f\"[magenta]{tool_name}[/]\")\n\n    for k, v in data_dict.items():\n        if k not in skip_fields and v is not None:\n            branch.add(f\"[dim]{k}:[/] {v}\")\n</code></pre>"},{"location":"api/utils/text/","title":"Text Utilities","text":""},{"location":"api/utils/text/#stirrup.utils.text","title":"stirrup.utils.text","text":""},{"location":"api/utils/text/#stirrup.utils.text.truncate_msg","title":"truncate_msg","text":"<pre><code>truncate_msg(msg: str, max_length: int) -&gt; str\n</code></pre> <p>Truncate long messages by removing middle portion, keeping start and end with ellipsis indicator.</p> Source code in <code>src/stirrup/utils/text.py</code> <pre><code>def truncate_msg(msg: str, max_length: int) -&gt; str:\n    \"\"\"Truncate long messages by removing middle portion, keeping start and end with ellipsis indicator.\"\"\"\n    msg_len = len(msg)\n    if msg_len &lt;= max_length:\n        return msg\n    else:\n        return (\n            msg[: max_length // 2]\n            + f\"\\n... This content has been truncated from an original {msg_len} characters to stay below {max_length} characters ...\\n\"\n            + msg[-max_length // 2 :]\n        )\n</code></pre>"},{"location":"extending/clients/","title":"Custom Clients","text":"<p>This guide covers implementing custom LLM clients for Stirrup.</p>"},{"location":"extending/clients/#built-in-clients","title":"Built-in Clients","text":"<p>Stirrup provides three built-in clients:</p> Client API Best For <code>ChatCompletionsClient</code> OpenAI Chat Completions OpenAI, OpenRouter, vLLM, Ollama, and other OpenAI-compatible APIs <code>OpenResponsesClient</code> OpenAI Responses API Providers implementing the newer Responses API format <code>LiteLLMClient</code> LiteLLM Multi-provider support (Anthropic, Google, Azure, etc.)"},{"location":"extending/clients/#llmclient-protocol","title":"LLMClient Protocol","text":"<p>All LLM clients must implement the <code>LLMClient</code> protocol:</p> Member Type Description <code>generate()</code> <code>async method</code> Generate next message with optional tool calls <code>model_slug</code> <code>property</code> Model identifier string (e.g., <code>\"openai/gpt-4o\"</code>) <code>max_tokens</code> <code>property</code> Maximum context window size"},{"location":"extending/clients/#basic-implementation","title":"Basic Implementation","text":"<pre><code>from stirrup import (\n    AssistantMessage,\n    ChatMessage,\n    Tool,\n    TokenUsage,\n)\n\n\nclass MyCustomClient:\n    \"\"\"Custom LLM client implementation.\"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        max_tokens: int = 64_000,\n        api_key: str | None = None,\n    ):\n        self._model = model\n        self._max_tokens = max_tokens\n        self._api_key = api_key\n\n    @property\n    def model_slug(self) -&gt; str:\n        return self._model\n\n    @property\n    def max_tokens(self) -&gt; int:\n        return self._max_tokens\n\n    async def generate(\n        self,\n        messages: list[ChatMessage],\n        tools: dict[str, Tool],\n    ) -&gt; AssistantMessage:\n        # Convert messages to your API format\n        api_messages = self._convert_messages(messages)\n\n        # Convert tools to your API format\n        api_tools = self._convert_tools(tools)\n\n        # Call your LLM API\n        response = await self._call_api(api_messages, api_tools)\n\n        # Convert response to AssistantMessage\n        return self._parse_response(response)\n</code></pre>"},{"location":"extending/clients/#using-with-agent","title":"Using with Agent","text":"<pre><code>from stirrup import Agent\n\nclient = MyCustomClient(\n    model=\"my-model-id\",\n    max_tokens=100_000,\n    api_key=\"...\",\n)\n\n# Pass custom client directly to Agent\nagent = Agent(\n    client=client,\n    name=\"custom_agent\",\n)\n</code></pre>"},{"location":"extending/clients/#openai-api-example","title":"OpenAI API Example","text":"<p>Stirrup message types use OpenAI-compatible field names (<code>role</code>, <code>content</code>, <code>tool_call_id</code>), so conversion is straightforward. The main difference is the <code>tool_calls</code> structure\u2014OpenAI nests them under <code>function</code>.</p> <pre><code>import openai\nfrom stirrup import AssistantMessage, ChatMessage, Tool, ToolCall, ToolMessage, TokenUsage\n\n\nclass OpenAIClient:\n    \"\"\"Direct OpenAI API client.\"\"\"\n\n    def __init__(self, model: str = \"gpt-4o\", max_tokens: int = 128_000):\n        self._model = model\n        self._max_tokens = max_tokens\n        self._client = openai.AsyncOpenAI()\n\n    @property\n    def model_slug(self) -&gt; str:\n        return f\"openai/{self._model}\"\n\n    @property\n    def max_tokens(self) -&gt; int:\n        return self._max_tokens\n\n    def _convert_message(self, msg: ChatMessage) -&gt; dict:\n        \"\"\"Convert a message to OpenAI format.\"\"\"\n        # SystemMessage, UserMessage, ToolMessage have compatible structure\n        if isinstance(msg, AssistantMessage):\n            result = {\"role\": \"assistant\", \"content\": str(msg.content)}\n            if msg.tool_calls:\n                result[\"tool_calls\"] = [\n                    {\"id\": tc.tool_call_id, \"type\": \"function\", \"function\": {\"name\": tc.name, \"arguments\": tc.arguments}}\n                    for tc in msg.tool_calls\n                ]\n            return result\n        elif isinstance(msg, ToolMessage):\n            return {\"role\": \"tool\", \"tool_call_id\": msg.tool_call_id, \"content\": str(msg.content)}\n        else:\n            # SystemMessage and UserMessage: just use role and content\n            return {\"role\": msg.role, \"content\": str(msg.content)}\n\n    async def generate(self, messages: list[ChatMessage], tools: dict[str, Tool]) -&gt; AssistantMessage:\n        api_messages = [self._convert_message(m) for m in messages]\n        api_tools = [\n            {\"type\": \"function\", \"function\": {\"name\": t.name, \"description\": t.description, \"parameters\": t.parameters.model_json_schema()}}\n            for t in tools.values()\n        ] or None\n\n        response = await self._client.chat.completions.create(model=self._model, messages=api_messages, tools=api_tools)\n        message = response.choices[0].message\n\n        return AssistantMessage(\n            content=message.content or \"\",\n            tool_calls=[ToolCall(name=tc.function.name, arguments=tc.function.arguments, tool_call_id=tc.id) for tc in (message.tool_calls or [])],\n            token_usage=TokenUsage(input=response.usage.prompt_tokens, answer=response.usage.completion_tokens),\n        )\n</code></pre> <p>You can optionally populate <code>request_start_time</code> and <code>request_end_time</code> on <code>AssistantMessage</code> to track generation speed. The derived <code>e2e_otps</code> property computes output tokens per second.</p>"},{"location":"extending/clients/#testing-with-mock-client","title":"Testing with Mock Client","text":"<pre><code>class MockClient:\n    \"\"\"Mock client for testing.\"\"\"\n\n    def __init__(self, responses: list[AssistantMessage]):\n        self._responses = responses\n        self._call_count = 0\n\n    @property\n    def model_slug(self) -&gt; str:\n        return \"mock/test-model\"\n\n    @property\n    def max_tokens(self) -&gt; int:\n        return 10_000\n\n    async def generate(self, messages, tools) -&gt; AssistantMessage:\n        response = self._responses[self._call_count]\n        self._call_count += 1\n        return response\n\n\n# Use in tests\nmock = MockClient([\n    AssistantMessage(content=\"Hello!\", tool_calls=[], token_usage=TokenUsage()),\n])\n\nagent = Agent(client=mock, name=\"test\")\n</code></pre>"},{"location":"extending/clients/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Tools - Advanced tool patterns</li> <li>Custom Loggers - Logging customization</li> </ul>"},{"location":"extending/code_backends/","title":"Custom Backends","text":"<p>This guide covers implementing custom code execution backends.</p>"},{"location":"extending/code_backends/#codeexectoolprovider","title":"CodeExecToolProvider","text":"<p>All code execution backends extend <code>CodeExecToolProvider</code>. Key methods to implement:</p> Method Purpose <code>__aenter__</code> Initialize environment and return <code>code_exec</code> tool <code>__aexit__</code> Cleanup environment (temp files, connections) <code>run_command</code> Execute a shell command and return <code>CommandResult</code> <code>read_file_bytes</code> Read file content from execution environment <code>write_file_bytes</code> Write file content to execution environment <p>The base class provides:</p> <ul> <li><code>get_code_exec_tool()</code> - Returns the standard <code>code_exec</code> tool</li> <li><code>allowed_commands</code> - Optional regex patterns to restrict commands</li> <li>File upload/download utilities</li> </ul>"},{"location":"extending/code_backends/#minimal-implementation","title":"Minimal Implementation","text":"<pre><code>from stirrup.tools.code_backends import (\n    CodeExecToolProvider,\n    CommandResult,\n    format_result,\n)\nfrom stirrup import Tool, ToolResult\n\n\nclass SimpleExecProvider(CodeExecToolProvider):\n    \"\"\"Simple execution in current directory.\"\"\"\n\n    async def __aenter__(self) -&gt; Tool:\n        return self.get_code_exec_tool()\n\n    async def __aexit__(self, *args):\n        pass  # No cleanup needed\n\n    async def run_command(self, cmd: str, *, timeout: int = 300) -&gt; CommandResult:\n        import asyncio\n\n        try:\n            proc = await asyncio.create_subprocess_shell(\n                cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n\n            stdout, stderr = await asyncio.wait_for(\n                proc.communicate(),\n                timeout=timeout,\n            )\n\n            return CommandResult(\n                exit_code=proc.returncode or 0,\n                stdout=stdout.decode(),\n                stderr=stderr.decode(),\n            )\n\n        except asyncio.TimeoutError:\n            return CommandResult(\n                exit_code=1,\n                stdout=\"\",\n                stderr=\"\",\n                error_kind=\"timeout\",\n                advice=f\"Command timed out after {timeout} seconds\",\n            )\n\n    async def read_file_bytes(self, path: str) -&gt; bytes:\n        with open(path, \"rb\") as f:\n            return f.read()\n\n    async def write_file_bytes(self, path: str, content: bytes) -&gt; None:\n        with open(path, \"wb\") as f:\n            f.write(content)\n</code></pre>"},{"location":"extending/code_backends/#command-allowlist","title":"Command Allowlist","text":"<p>Restrict what commands can be executed:</p> <pre><code>provider = MyCodeExecProvider(\n    allowed_commands=[\n        r\"python.*\",           # Allow Python commands\n        r\"pip install.*\",      # Allow pip install\n        r\"ls.*\",               # Allow ls\n        r\"cat.*\",              # Allow cat\n    ]\n)\n</code></pre> <p>The base class validates commands before execution.</p>"},{"location":"extending/code_backends/#next-steps","title":"Next Steps","text":"<ul> <li>Code Execution Guide - Using built-in backends</li> <li>Custom Tools - Advanced tool patterns</li> </ul>"},{"location":"extending/full-customization/","title":"Full Customization","text":"<p>This guide covers cloning and importing Stirrup locally for deep customization of the framework internals.</p>"},{"location":"extending/full-customization/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/ArtificialAnalysis/Stirrup.git\ncd stirrup\n\n# Install in editable mode\npip install -e .      # or: uv venv &amp;&amp; uv pip install -e .\n\n# Or with all optional dependencies\npip install -e '.[all]'  # or: uv venv &amp;&amp; uv pip install -e '.[all]'\n</code></pre>"},{"location":"extending/full-customization/#importing-in-your-project","title":"Importing in Your Project","text":"<p>After editable installation, import as usual:</p> <pre><code>from stirrup import Agent, DEFAULT_TOOLS\nfrom stirrup.core.models import SystemMessage, UserMessage\nfrom stirrup.clients import ChatCompletionsClient\n</code></pre>"},{"location":"extending/full-customization/#project-structure","title":"Project Structure","text":"<pre><code>src/stirrup/\n\u251c\u2500\u2500 clients/          # LLM client implementations\n\u251c\u2500\u2500 core/             # Agent class, models, exceptions\n\u251c\u2500\u2500 tools/            # Tool implementations\n\u2502   \u2514\u2500\u2500 code_backends/  # Code execution backends\n\u251c\u2500\u2500 utils/            # Logging and text utilities\n\u2514\u2500\u2500 prompts/          # System prompt templates\n</code></pre> <p>Common customization points:</p> Directory Use Case <code>clients/</code> Custom LLM providers or API modifications <code>core/agent.py</code> Agent loop behavior changes <code>tools/</code> New tools or modifying existing ones <code>tools/code_backends/</code> Custom execution environments"},{"location":"extending/full-customization/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Clients - Implementing LLM client protocol</li> <li>Custom Tools - Advanced tool patterns</li> <li>Custom Backends - Execution environment customization</li> </ul>"},{"location":"extending/loggers/","title":"Custom Loggers","text":"<p>This guide covers implementing custom loggers for Stirrup.</p>"},{"location":"extending/loggers/#agentloggerbase","title":"AgentLoggerBase","text":"<p>All loggers must implement the <code>AgentLoggerBase</code> abstract class. Key methods to implement:</p> Method Purpose <code>__enter__</code> Called when agent session starts <code>__exit__</code> Called when agent session ends <code>on_step</code> Called after each agent step with progress stats <code>assistant_message</code> Log LLM responses <code>user_message</code> Log user inputs <code>task_message</code> Log initial task/prompt <code>tool_result</code> Log tool execution results <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code> Standard logging methods <p>Properties set by Agent before <code>__enter__</code>: <code>name</code>, <code>model</code>, <code>max_turns</code>, <code>depth</code></p> <p>Properties set before <code>__exit__</code>: <code>finish_params</code>, <code>run_metadata</code>, <code>output_dir</code></p>"},{"location":"extending/loggers/#minimal-implementation","title":"Minimal Implementation","text":"<pre><code>from stirrup.utils.logging import AgentLoggerBase\n\n\nclass MinimalLogger(AgentLoggerBase):\n    \"\"\"Minimal logger that prints step summaries.\"\"\"\n\n    def __enter__(self):\n        print(f\"Starting agent: {self.name}\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            print(f\"Agent failed: {exc_val}\")\n        else:\n            print(f\"Agent completed: {self.finish_params}\")\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        print(f\"Turn {turn}: {total_tools} tools, {input_tokens + output_tokens} tokens\")\n</code></pre>"},{"location":"extending/loggers/#file-logger","title":"File Logger","text":"<pre><code>import json\nfrom pathlib import Path\nfrom datetime import datetime\n\n\nclass FileLogger(AgentLoggerBase):\n    \"\"\"Logger that writes to a JSON file.\"\"\"\n\n    def __init__(self, log_dir: str = \"./logs\"):\n        self.log_dir = Path(log_dir)\n        self.log_file: Path | None = None\n        self.steps: list[dict] = []\n\n    def __enter__(self):\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.log_file = self.log_dir / f\"{self.name}_{timestamp}.json\"\n        self.steps = []\n\n        # Write initial entry\n        self._write({\n            \"event\": \"start\",\n            \"agent\": self.name,\n            \"model\": self.model,\n            \"max_turns\": self.max_turns,\n            \"timestamp\": datetime.now().isoformat(),\n        })\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Write final entry\n        self._write({\n            \"event\": \"end\",\n            \"success\": exc_type is None,\n            \"error\": str(exc_val) if exc_val else None,\n            \"finish_params\": self.finish_params.model_dump() if self.finish_params else None,\n            \"steps\": self.steps,\n            \"timestamp\": datetime.now().isoformat(),\n        })\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        step = {\n            \"turn\": turn,\n            \"total_tools\": total_tools,\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens,\n            \"timestamp\": datetime.now().isoformat(),\n        }\n        self.steps.append(step)\n\n    def _write(self, data: dict):\n        if self.log_file:\n            with open(self.log_file, \"w\") as f:\n                json.dump(data, f, indent=2)\n</code></pre>"},{"location":"extending/loggers/#metrics-logger","title":"Metrics Logger","text":"<pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\n\n@dataclass\nclass RunMetrics:\n    \"\"\"Aggregated metrics for a run.\"\"\"\n    total_turns: int = 0\n    total_tools: int = 0\n    total_input_tokens: int = 0\n    total_output_tokens: int = 0\n    start_time: float = 0\n    end_time: float = 0\n\n    @property\n    def duration_seconds(self) -&gt; float:\n        return self.end_time - self.start_time\n\n    @property\n    def total_tokens(self) -&gt; int:\n        return self.total_input_tokens + self.total_output_tokens\n\n\nclass MetricsLogger(AgentLoggerBase):\n    \"\"\"Logger that collects metrics for analysis.\"\"\"\n\n    def __init__(self):\n        self.metrics: RunMetrics | None = None\n        self._runs: list[RunMetrics] = []\n\n    def __enter__(self):\n        import time\n        self.metrics = RunMetrics(start_time=time.time())\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        import time\n        if self.metrics:\n            self.metrics.end_time = time.time()\n            self._runs.append(self.metrics)\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        if self.metrics:\n            self.metrics.total_turns = turn\n            self.metrics.total_tools += total_tools\n            self.metrics.total_input_tokens += input_tokens\n            self.metrics.total_output_tokens += output_tokens\n\n    def get_all_runs(self) -&gt; list[RunMetrics]:\n        \"\"\"Get metrics from all runs.\"\"\"\n        return self._runs.copy()\n\n    def get_summary(self) -&gt; dict[str, Any]:\n        \"\"\"Get summary statistics across all runs.\"\"\"\n        if not self._runs:\n            return {}\n\n        return {\n            \"total_runs\": len(self._runs),\n            \"avg_turns\": sum(r.total_turns for r in self._runs) / len(self._runs),\n            \"avg_tokens\": sum(r.total_tokens for r in self._runs) / len(self._runs),\n            \"avg_duration\": sum(r.duration_seconds for r in self._runs) / len(self._runs),\n        }\n</code></pre>"},{"location":"extending/loggers/#callback-logger","title":"Callback Logger","text":"<pre><code>from collections.abc import Callable\n\n\nclass CallbackLogger(AgentLoggerBase):\n    \"\"\"Logger that invokes callbacks on events.\"\"\"\n\n    def __init__(\n        self,\n        on_start: Callable[[str], None] | None = None,\n        on_end: Callable[[bool, Any], None] | None = None,\n        on_step_callback: Callable[[int, int, int, int], None] | None = None,\n    ):\n        self._on_start = on_start\n        self._on_end = on_end\n        self._on_step_callback = on_step_callback\n\n    def __enter__(self):\n        if self._on_start:\n            self._on_start(self.name)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self._on_end:\n            self._on_end(exc_type is None, self.finish_params)\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        if self._on_step_callback:\n            self._on_step_callback(turn, total_tools, input_tokens, output_tokens)\n</code></pre>"},{"location":"extending/loggers/#using-custom-loggers","title":"Using Custom Loggers","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n# With MetricsLogger\nmetrics_logger = MetricsLogger()\nclient = ChatCompletionsClient(model=\"gpt-5\")\n\nagent = Agent(\n    client=client,\n    name=\"my_agent\",\n    logger=metrics_logger,\n)\n\nasync with agent.session() as session:\n    await session.run(\"Do something\")\n\n# Access metrics after run\nprint(metrics_logger.get_summary())\n</code></pre>"},{"location":"extending/loggers/#combining-loggers","title":"Combining Loggers","text":"<pre><code>class CompositeLogger(AgentLoggerBase):\n    \"\"\"Logger that delegates to multiple loggers.\"\"\"\n\n    def __init__(self, *loggers: AgentLoggerBase):\n        self._loggers = loggers\n\n    def __enter__(self):\n        for logger in self._loggers:\n            # Copy properties to child loggers\n            logger.name = self.name\n            logger.model = self.model\n            logger.max_turns = self.max_turns\n            logger.depth = self.depth\n            logger.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for logger in self._loggers:\n            logger.finish_params = self.finish_params\n            logger.run_metadata = self.run_metadata\n            logger.__exit__(exc_type, exc_val, exc_tb)\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        for logger in self._loggers:\n            logger.on_step(turn, total_tools, input_tokens, output_tokens)\n\n\n# Use multiple loggers\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"my_agent\",\n    logger=CompositeLogger(\n        FileLogger(\"./logs\"),\n        MetricsLogger(),\n    ),\n)\n</code></pre>"},{"location":"extending/loggers/#sub-agent-awareness","title":"Sub-Agent Awareness","text":"<p>The <code>depth</code> property indicates nesting level:</p> <pre><code>class IndentedLogger(AgentLoggerBase):\n    \"\"\"Logger that indents output based on sub-agent depth.\"\"\"\n\n    def __enter__(self):\n        indent = \"  \" * self.depth\n        print(f\"{indent}Starting: {self.name} (depth={self.depth})\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        indent = \"  \" * self.depth\n        print(f\"{indent}Finished: {self.name}\")\n\n    def on_step(self, turn, total_tools, input_tokens, output_tokens):\n        indent = \"  \" * self.depth\n        print(f\"{indent}  Turn {turn}\")\n</code></pre>"},{"location":"extending/loggers/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Backends - Code execution backends</li> <li>Custom Clients - LLM client customization</li> </ul>"},{"location":"extending/tools/","title":"Custom Tools (Advanced)","text":"<p>This guide covers advanced patterns for creating sophisticated tools.</p>"},{"location":"extending/tools/#tools-with-external-apis","title":"Tools with External APIs","text":"<pre><code>import httpx\nfrom pydantic import BaseModel, Field\nfrom stirrup import Tool, ToolResult, ToolUseCountMetadata\n\n\nclass WeatherParams(BaseModel):\n    city: str = Field(description=\"City name\")\n    units: str = Field(default=\"celsius\", description=\"Temperature units\")\n\n\nclass WeatherToolProvider:\n    \"\"\"Weather tool with shared HTTP client.\"\"\"\n\n    has_lifecycle = True\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self._client: httpx.AsyncClient | None = None\n\n    async def __aenter__(self) -&gt; Tool:\n        self._client = httpx.AsyncClient(\n            base_url=\"https://api.weather.com\",\n            headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n        )\n\n        async def get_weather(params: WeatherParams) -&gt; ToolResult[ToolUseCountMetadata]:\n            response = await self._client.get(\n                \"/current\",\n                params={\"city\": params.city, \"units\": params.units},\n            )\n            data = response.json()\n            return ToolResult(\n                content=f\"Weather in {params.city}: {data['temp']}\u00b0 {data['condition']}\",\n                metadata=ToolUseCountMetadata(),\n            )\n\n        return Tool(\n            name=\"get_weather\",\n            description=\"Get current weather for a city\",\n            parameters=WeatherParams,\n            executor=get_weather,\n        )\n\n    async def __aexit__(self, *args):\n        if self._client:\n            await self._client.aclose()\n</code></pre>"},{"location":"extending/tools/#tools-with-state","title":"Tools with State","text":"<pre><code>class ConversationMemoryProvider:\n    \"\"\"Tool that remembers context across calls.\"\"\"\n\n    has_lifecycle = True\n\n    def __init__(self):\n        self._memories: list[str] = []\n\n    async def __aenter__(self) -&gt; list[Tool]:\n        return [self._create_remember_tool(), self._create_recall_tool()]\n\n    def _create_remember_tool(self) -&gt; Tool:\n        async def remember(params: RememberParams) -&gt; ToolResult[ToolUseCountMetadata]:\n            self._memories.append(params.fact)\n            return ToolResult(\n                content=f\"Remembered: {params.fact}\",\n                metadata=ToolUseCountMetadata(),\n            )\n\n        return Tool(\n            name=\"remember\",\n            description=\"Store a fact for later recall\",\n            parameters=RememberParams,\n            executor=remember,\n        )\n\n    def _create_recall_tool(self) -&gt; Tool:\n        async def recall(params: RecallParams) -&gt; ToolResult[ToolUseCountMetadata]:\n            relevant = [m for m in self._memories if params.query.lower() in m.lower()]\n            if relevant:\n                return ToolResult(content=\"\\n\".join(relevant), metadata=ToolUseCountMetadata())\n            return ToolResult(content=\"No relevant memories found\", metadata=ToolUseCountMetadata())\n\n        return Tool(\n            name=\"recall\",\n            description=\"Recall previously stored facts\",\n            parameters=RecallParams,\n            executor=recall,\n        )\n\n    async def __aexit__(self, *args):\n        self._memories.clear()\n</code></pre>"},{"location":"extending/tools/#custom-metadata-types","title":"Custom Metadata Types","text":"<pre><code>from pydantic import BaseModel\nfrom stirrup import Addable\n\n\nclass APICallMetadata(BaseModel, Addable):\n    \"\"\"Track API call statistics.\"\"\"\n\n    calls: int = 1\n    tokens_used: int = 0\n    cost_usd: float = 0.0\n    latency_ms: float = 0.0\n\n    def __add__(self, other: \"APICallMetadata\") -&gt; \"APICallMetadata\":\n        return APICallMetadata(\n            calls=self.calls + other.calls,\n            tokens_used=self.tokens_used + other.tokens_used,\n            cost_usd=self.cost_usd + other.cost_usd,\n            latency_ms=self.latency_ms + other.latency_ms,\n        )\n\n\nasync def my_tool(params: MyParams) -&gt; ToolResult[APICallMetadata]:\n    start = time.time()\n    response = await call_api(params)\n    latency = (time.time() - start) * 1000\n\n    return ToolResult(\n        content=response.text,\n        metadata=APICallMetadata(\n            tokens_used=response.tokens,\n            cost_usd=response.cost,\n            latency_ms=latency,\n        ),\n    )\n</code></pre>"},{"location":"extending/tools/#tools-returning-images","title":"Tools Returning Images","text":"<pre><code>from stirrup import ImageContentBlock, ToolResult\n\n\nasync def chart_tool(params: ChartParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    # Generate chart with matplotlib\n    import matplotlib.pyplot as plt\n    import io\n\n    plt.figure()\n    plt.plot(params.x_data, params.y_data)\n    plt.title(params.title)\n\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png')\n    buf.seek(0)\n    image_bytes = buf.read()\n    plt.close()\n\n    return ToolResult(\n        content=[\n            f\"Generated chart: {params.title}\",\n            ImageContentBlock(data=image_bytes),\n        ],\n        metadata=ToolUseCountMetadata(),\n    )\n</code></pre>"},{"location":"extending/tools/#testing-tools","title":"Testing Tools","text":"<pre><code>import pytest\nfrom stirrup import ToolResult\n\n\n@pytest.mark.asyncio\nasync def test_weather_tool():\n    provider = WeatherToolProvider(api_key=\"test\")\n\n    async with provider as tool:\n        # Test the tool directly\n        result = await tool.executor(WeatherParams(city=\"London\"))\n\n        assert isinstance(result, ToolResult)\n        assert \"London\" in result.content\n        assert result.metadata is not None\n\n\n@pytest.mark.asyncio\nasync def test_tool_error_handling():\n    async def failing_executor(params):\n        raise ConnectionError(\"Network error\")\n\n    # Test that errors are handled gracefully\n    # ...\n</code></pre>"},{"location":"extending/tools/#next-steps","title":"Next Steps","text":"<ul> <li>Tool Providers - Provider pattern basics</li> <li>Custom Loggers - Logging customization</li> <li>Custom Backends - Code execution backends</li> </ul>"},{"location":"guides/caching/","title":"Caching and Resumption","text":"<p>Stirrup automatically caches agent state on interruptions, allowing you to resume long-running tasks.</p>"},{"location":"guides/caching/#enabling-resume","title":"Enabling Resume","text":"<p>Pass <code>resume=True</code> to <code>session()</code>:</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import DEFAULT_TOOLS\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(client=client, name=\"researcher\", tools=DEFAULT_TOOLS, max_turns=50)\n\nasync with agent.session(output_dir=\"./output\", resume=True) as session:\n    await session.run(\"Analyze all datasets in the data folder\")\n</code></pre>"},{"location":"guides/caching/#how-it-works","title":"How It Works","text":"<ol> <li> <p>On interruption (Ctrl+C, error, or max turns): Stirrup saves conversation state and execution environment files to <code>~/.cache/stirrup/&lt;task_hash&gt;/</code></p> </li> <li> <p>On next run with <code>resume=True</code>: If a cache exists for the same prompt, the agent restores state and continues from the last turn</p> </li> <li> <p>On successful completion: The cache is automatically cleared (configurable via <code>clear_on_success</code>)</p> </li> </ol> <pre><code># First run (interrupted at turn 15)\n$ python my_agent.py\n^C\nCached state for task abc123...\n\n# Second run (resumes from turn 15)\n$ python my_agent.py\nResuming from cached state at turn 15\n</code></pre>"},{"location":"guides/caching/#what-gets-cached","title":"What Gets Cached","text":"<ul> <li>Conversation messages and history</li> <li>Current turn number</li> <li>Tool metadata</li> <li>All files in the execution environment</li> </ul>"},{"location":"guides/caching/#preserving-caches-on-success","title":"Preserving Caches on Success","text":"<p>By default, caches are cleared on successful completion. To preserve them for inspection or debugging:</p> <pre><code>async with agent.session(\n    resume=True,\n    clear_cache_on_success=False,  # Keep cache after success\n) as session:\n    await session.run(\"Analyze the data\")\n</code></pre>"},{"location":"guides/caching/#managing-caches","title":"Managing Caches","text":"<pre><code>from stirrup.core.cache import CacheManager\n\ncache_manager = CacheManager()\n\n# List all caches\nfor task_hash in cache_manager.list_caches():\n    info = cache_manager.get_cache_info(task_hash)\n    print(f\"{task_hash}: turn {info['turn']}\")\n\n# Clear a specific cache\ncache_manager.clear_cache(\"abc123def456\")\n</code></pre>"},{"location":"guides/caching/#notes","title":"Notes","text":"<ul> <li>Cache key is computed from the initial prompt\u2014same prompt = same cache</li> <li>Caches are stored locally in <code>~/.cache/stirrup/</code></li> <li>Caches are automatically cleared on successful completion (by default)</li> </ul>"},{"location":"guides/code-execution/","title":"Code Execution","text":"<p>Stirrup provides multiple backends for executing code in isolated environments.</p>"},{"location":"guides/code-execution/#overview","title":"Overview","text":"<p>All code execution backends implement <code>CodeExecToolProvider</code>, which provides:</p> <ul> <li>A <code>code_exec</code> tool for running shell commands</li> <li>File upload/download capabilities</li> <li>Isolated execution environment</li> </ul>"},{"location":"guides/code-execution/#available-backends","title":"Available Backends","text":"Backend Isolation Use Case <code>LocalCodeExecToolProvider</code> Temp directory Development, trusted code <code>DockerCodeExecToolProvider</code> Container Production, semi-trusted code <code>E2BCodeExecToolProvider</code> Cloud sandbox Production, untrusted code"},{"location":"guides/code-execution/#localcodeexectoolprovider","title":"LocalCodeExecToolProvider","text":"<p>Executes code in an isolated temporary directory on the host machine.</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"local_coder\",\n    tools=[LocalCodeExecToolProvider()],\n)\n\nasync with agent.session(output_dir=\"./output\") as session:\n    await session.run(\"Write and run a Python hello world script\")\n</code></pre>"},{"location":"guides/code-execution/#configuration","title":"Configuration","text":"<pre><code>from stirrup.tools import LocalCodeExecToolProvider\n\nprovider = LocalCodeExecToolProvider(\n    allowed_commands=None,      # Regex patterns for allowed commands (None = all)\n    temp_base_dir=None,         # Base directory for temp folder (default: system temp)\n)\n</code></pre>"},{"location":"guides/code-execution/#security-considerations","title":"Security Considerations","text":"<ul> <li>Code runs with your user permissions</li> <li>Absolute paths outside temp directory are blocked</li> <li>Use <code>allowed_commands</code> to restrict what can be executed</li> </ul> <pre><code>from stirrup.tools import LocalCodeExecToolProvider\n\n# Only allow Python and pip\nprovider = LocalCodeExecToolProvider(\n    allowed_commands=[\"python.*\", \"pip.*\", \"uv.*\"],\n)\n</code></pre>"},{"location":"guides/code-execution/#dockercodeexectoolprovider","title":"DockerCodeExecToolProvider","text":"<p>Executes code in a Docker container for better isolation.</p> <p>Note</p> <p>Requires <code>pip install stirrup[docker]</code> (or: <code>uv add stirrup[docker]</code>) and Docker daemon running.</p>"},{"location":"guides/code-execution/#from-image","title":"From Image","text":"<pre><code>from stirrup import Agent\nfrom stirrup.tools.code_backends.docker import DockerCodeExecToolProvider\n\nprovider = DockerCodeExecToolProvider.from_image(\n    \"python:3.12-slim\",\n    env_vars=[\"OPENAI_API_KEY\"],  # Forward these env vars\n)\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"docker_coder\",\n    tools=[provider],\n)\n</code></pre>"},{"location":"guides/code-execution/#from-dockerfile","title":"From Dockerfile","text":"<pre><code>from stirrup.tools.code_backends.docker import DockerCodeExecToolProvider\n\nprovider = DockerCodeExecToolProvider.from_dockerfile(\n    dockerfile_path=\"./Dockerfile\",\n    build_args={\"PYTHON_VERSION\": \"3.12\"},\n)\n</code></pre>"},{"location":"guides/code-execution/#configuration-options","title":"Configuration Options","text":"<pre><code>from stirrup.tools.code_backends.docker import DockerCodeExecToolProvider\n\nprovider = DockerCodeExecToolProvider.from_image(\n    \"python:3.12-slim\",\n    env_vars=[\"API_KEY\", \"SECRET\"],  # Environment variables to forward\n)\n</code></pre>"},{"location":"guides/code-execution/#e2bcodeexectoolprovider","title":"E2BCodeExecToolProvider","text":"<p>Executes code in E2B cloud sandboxes for maximum isolation.</p> <p>Note</p> <p>Requires <code>pip install stirrup[e2b]</code> (or: <code>uv add stirrup[e2b]</code>) and <code>E2B_API_KEY</code> environment variable.</p> <pre><code>from stirrup import Agent\nfrom stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\n\nprovider = E2BCodeExecToolProvider()\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"e2b_coder\",\n    tools=[provider],\n    max_turns=20,\n)\n\nasync with agent.session(\n    input_files=\"data/*.csv\",\n    output_dir=\"./results\",\n) as session:\n    await session.run(\"Analyze the CSV files and create a report\")\n</code></pre>"},{"location":"guides/code-execution/#with-template","title":"With Template","text":"<pre><code>from stirrup.tools.code_backends.e2b import E2BCodeExecToolProvider\n\nprovider = E2BCodeExecToolProvider(\n    template=\"custom-python-template\",  # Your E2B template ID\n)\n</code></pre>"},{"location":"guides/code-execution/#file-operations","title":"File Operations","text":""},{"location":"guides/code-execution/#uploading-files","title":"Uploading Files","text":"<p>Use <code>input_files</code> in session to upload files to the execution environment:</p> <pre><code>async with agent.session(\n    input_files=\"data.csv\",           # Single file\n    # input_files=[\"a.csv\", \"b.csv\"], # Multiple files\n    # input_files=\"data/*.csv\",       # Glob pattern\n    # input_files=\"./data/\",          # Directory (recursive)\n) as session:\n    await session.run(\"Process the uploaded files\")\n</code></pre>"},{"location":"guides/code-execution/#saving-output-files","title":"Saving Output Files","text":"<p>When the agent calls the finish tool with file paths, they're saved to <code>output_dir</code>:</p> <pre><code>async with agent.session(output_dir=\"./output\") as session:\n    finish_params, _, _ = await session.run(\n        \"Create a report and save it as report.pdf\"\n    )\n    # Files in finish_params.paths are saved to ./output/\n    print(f\"Saved: {finish_params.paths}\")\n</code></pre>"},{"location":"guides/code-execution/#view-image-tool","title":"View Image Tool","text":"<p>Use <code>ViewImageToolProvider</code> to let the agent view images from the execution environment:</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, ViewImageToolProvider\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"image_viewer\",\n    tools=[\n        LocalCodeExecToolProvider(),\n        ViewImageToolProvider(),  # Auto-detects exec env\n    ],\n)\n\nasync with agent.session() as session:\n    await session.run(\"Create a matplotlib chart and view it\")\n</code></pre>"},{"location":"guides/code-execution/#next-steps","title":"Next Steps","text":"<ul> <li>Sub-Agents - File transfer between agents</li> <li>Extending Backends - Custom execution backends</li> </ul>"},{"location":"guides/mcp/","title":"MCP Integration","text":"<p>Stirrup supports the Model Context Protocol (MCP) for connecting to external tool servers.</p> <p>Note</p> <p>Requires <code>pip install stirrup[mcp]</code> (or: <code>uv add stirrup[mcp]</code>)</p>"},{"location":"guides/mcp/#quick-start","title":"Quick Start","text":"<pre><code>async def main() -&gt; None:\n    \"\"\"Run an agent with MCP tools.\"\"\"\n    # Create client for OpenRouter\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n        max_tokens=50_000,\n    )\n\n    # Create agent with default tools + MCP tools\n    agent = Agent(\n        client=client,\n        name=\"mcp_example_agent\",\n        tools=[*DEFAULT_TOOLS, MCPToolProvider.from_config(\".mcp/mcp.json\")],\n        max_turns=20,\n    )\n\n    # Run with session context - handles tool lifecycle, logging, and file saving\n    async with agent.session(output_dir=Path(\"./output/mcp_example\")) as session:\n        task = \"\"\"You have access to MCP server tools and a code execution environment.\n            Using the same implementation as TheAlgorithms/Python (you can use DeepWiki MCP\n            to research), write a Python file quicksort.py that implements quicksort and\n            another that tests (and times) it.\n            When done, call the finish tool including your findings.\"\"\"\n\n        _finish_params, _history, _metadata = await session.run(task)\n</code></pre>"},{"location":"guides/mcp/#configuration","title":"Configuration","text":"<p>Create an <code>mcp.json</code> file (e.g., <code>.mcp/mcp.json</code>) with your server configurations. The path is passed to <code>MCPToolProvider.from_config()</code>\u2014use an absolute path or a path relative to where you run your script.</p>"},{"location":"guides/mcp/#sse-server-http","title":"SSE Server (HTTP)","text":"<p>For remote MCP servers that use Server-Sent Events:</p> <pre><code>{\n  \"mcpServers\": {\n    \"deepwiki\": {\n      \"url\": \"https://mcp.deepwiki.com/sse\"\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp/#stdio-server-local-process","title":"Stdio Server (Local Process)","text":"<p>For local MCP servers that run as command-line processes:</p> <pre><code>{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@anthropics/mcp-server-filesystem\", \"/path/to/files\"]\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp/#tool-naming","title":"Tool Naming","text":"<p>MCP tools are prefixed with the server name:</p> <ul> <li>Server <code>deepwiki</code> with tool <code>search</code> \u2192 <code>deepwiki__search</code></li> <li>Server <code>filesystem</code> with tool <code>read_file</code> \u2192 <code>filesystem__read_file</code></li> </ul>"},{"location":"guides/mcp/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR_NAME}</code> syntax for secrets:</p> <pre><code>{\n  \"mcpServers\": {\n    \"api_server\": {\n      \"url\": \"https://api.example.com/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${API_KEY}\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"guides/mcp/#next-steps","title":"Next Steps","text":"<ul> <li>Creating Tools - Build your own tools</li> <li>Tool Providers - Provider pattern</li> </ul>"},{"location":"guides/skills/","title":"Skills","text":"<p>Skills are modular packages that extend agent capabilities with domain-specific instructions, scripts, and resources. They provide a structured way to give agents expertise in specific areas like data analysis or report writing.</p> <p>Learn more at agentskills.io</p>"},{"location":"guides/skills/#overview","title":"Overview","text":"<p>A skill is a directory containing:</p> <ul> <li>SKILL.md - Main instruction file with YAML frontmatter (name, description) and detailed guidance</li> <li>reference/ - Optional subdirectory with reference documentation</li> <li>scripts/ - Optional subdirectory with ready-to-use Python scripts</li> </ul> <p>When skills are loaded, the agent receives:</p> <ol> <li>A list of available skills in the system prompt</li> <li>Access to skill files in the execution environment</li> <li>Instructions on how to read and use the skills</li> </ol>"},{"location":"guides/skills/#quick-start","title":"Quick Start","text":""},{"location":"guides/skills/#1-create-a-skills-directory","title":"1. Create a Skills Directory","text":"<pre><code>skills/\n\u2514\u2500\u2500 data_analysis/\n    \u251c\u2500\u2500 SKILL.md\n    \u251c\u2500\u2500 reference/\n    \u2502   \u251c\u2500\u2500 loading.md\n    \u2502   \u2514\u2500\u2500 transformations.md\n    \u2514\u2500\u2500 scripts/\n        \u251c\u2500\u2500 explore_data.py\n        \u2514\u2500\u2500 summary_stats.py\n</code></pre>"},{"location":"guides/skills/#2-create-skillmd-with-frontmatter","title":"2. Create SKILL.md with Frontmatter","text":"<pre><code>---\nname: data_analysis\ndescription: High-performance data analysis using Polars - load, transform, aggregate, and export tabular data.\n---\n\n# Data Analysis Skill\n\nComprehensive data analysis toolkit using **Polars**.\n\n## Quick Start\n\n```python\nimport polars as pl\ndf = pl.read_csv(\"data.csv\")\ndf.describe()\n</code></pre>"},{"location":"guides/skills/#when-to-use-this-skill","title":"When to Use This Skill","text":"<ul> <li>Loading datasets (CSV, JSON, Parquet)</li> <li>Data cleaning and transformation</li> <li>Statistical analysis ... <pre><code>### 3. Pass Skills to the Agent Session\n\n```python\nasync def main() -&gt; None:\n    \"\"\"Run an agent with skills for data analysis.\"\"\"\n    client = ChatCompletionsClient(\n        base_url=\"https://openrouter.ai/api/v1\",\n        model=\"anthropic/claude-sonnet-4.5\",\n    )\n\n    agent = Agent(\n        client=client,\n        name=\"agent\",\n        max_turns=20,\n        tools=[DockerCodeExecToolProvider.from_dockerfile(dockerfile=\"examples/skills/Dockerfile\")],\n    )\n\n    async with agent.session(\n        input_files=[\"examples/skills/sample_data.csv\"],\n        output_dir=\"output/skills_example\",\n        skills_dir=\"skills\",\n    ) as session:\n        await session.run(\"Read the input sample_data.csv file and run full data analysis.\")\n</code></pre></li> </ul>"},{"location":"guides/skills/#how-skills-work","title":"How Skills Work","text":"<p>When you specify <code>skills_dir</code> in <code>session()</code>:</p> <ol> <li>Discovery: Stirrup scans the directory for subdirectories containing <code>SKILL.md</code> files</li> <li>Metadata Extraction: YAML frontmatter (name, description) is parsed from each <code>SKILL.md</code></li> <li>System Prompt: Available skills are listed in the agent's system prompt</li> <li>File Upload: The skills directory is uploaded to the execution environment</li> </ol> <p>The agent sees something like this in its system prompt:</p> <pre><code>## Available Skills\n\nYou have access to the following skills located in the `skills/` directory.\nEach skill contains a SKILL.md file with detailed instructions and potentially bundled scripts.\n\nTo use a skill:\n1. Read the full instructions: `cat &lt;skill_path&gt;/SKILL.md`\n2. Follow the instructions and use any bundled resources as described\n\n- **data_analysis**: High-performance data analysis using Polars (`skills/data_analysis/SKILL.md`)\n</code></pre>"},{"location":"guides/skills/#creating-effective-skills","title":"Creating Effective Skills","text":""},{"location":"guides/skills/#skillmd-structure","title":"SKILL.md Structure","text":"<p>A well-structured <code>SKILL.md</code> should include:</p> <pre><code>---\nname: skill_name\ndescription: One-line description shown in the system prompt\n---\n\n# Skill Title\n\nBrief overview of what this skill provides.\n\n## Quick Start\n\nMinimal working example the agent can use immediately.\n\n## When to Use This Skill\n\nBullet list of use cases to help the agent decide when to apply this skill.\n\n## Skill Contents\n\nList reference docs and scripts included in the skill.\n\n## Core Patterns\n\nCommon patterns and code snippets for the domain.\n\n## Best Practices\n\nTips for effective use of the skill.\n</code></pre>"},{"location":"guides/skills/#including-scripts","title":"Including Scripts","text":"<p>Bundle ready-to-use scripts that the agent can execute:</p> <pre><code>skills/data_analysis/scripts/\n\u251c\u2500\u2500 explore_data.py      # Quick dataset profiling\n\u2514\u2500\u2500 summary_stats.py     # Generate statistics report\n</code></pre> <p>Reference them in SKILL.md:</p> <pre><code>### Ready-to-Use Scripts\n\n- `scripts/explore_data.py` - Quick dataset exploration\n  ```bash\n  python scripts/explore_data.py data.csv --output report.txt\n  ```\n</code></pre>"},{"location":"guides/skills/#including-reference-documentation","title":"Including Reference Documentation","text":"<p>For complex domains, split documentation into focused reference files:</p> <pre><code>skills/data_analysis/reference/\n\u251c\u2500\u2500 loading.md           # Data loading patterns\n\u251c\u2500\u2500 transformations.md   # Column operations\n\u251c\u2500\u2500 aggregations.md      # Group by, window functions\n\u2514\u2500\u2500 visualization.md     # Creating charts\n</code></pre> <p>Reference them in SKILL.md:</p> <pre><code>### Reference Documentation\n\n- `reference/loading.md` - Loading data from all supported formats\n- `reference/transformations.md` - Column operations, filtering, sorting\n</code></pre>"},{"location":"guides/skills/#example-data-analysis-skill","title":"Example: Data Analysis Skill","text":"<p>See the included example at <code>skills/data_analysis/</code>:</p> <pre><code>skills/data_analysis/\n\u251c\u2500\u2500 SKILL.md                          # Main instructions\n\u251c\u2500\u2500 reference/\n\u2502   \u251c\u2500\u2500 aggregations.md               # Group by, window functions\n\u2502   \u251c\u2500\u2500 loading.md                    # File format support\n\u2502   \u251c\u2500\u2500 statistics.md                 # Statistical analysis\n\u2502   \u251c\u2500\u2500 time_series.md                # Date/time operations\n\u2502   \u251c\u2500\u2500 transformations.md            # Data transformations\n\u2502   \u2514\u2500\u2500 visualization.md              # Chart creation\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 explore_data.py               # Dataset profiling script\n    \u2514\u2500\u2500 summary_stats.py              # Statistics report script\n</code></pre>"},{"location":"guides/skills/#skills-with-docker","title":"Skills with Docker","text":"<p>When using skills that require specific dependencies, create a Dockerfile:</p> <pre><code>FROM ghcr.io/astral-sh/uv:python3.12-bookworm-slim\n\nRUN uv pip install --system polars matplotlib\n\nWORKDIR /workspace\n</code></pre> <p>Then pass it to <code>DockerCodeExecToolProvider.from_dockerfile()</code> as shown in the example above.</p>"},{"location":"guides/skills/#best-practices","title":"Best Practices","text":"<ol> <li>Keep skills focused: Each skill should cover one domain well</li> <li>Provide working examples: Quick start code that runs immediately</li> <li>Include decision guidance: Help the agent know when to use the skill</li> <li>Bundle common scripts: Save agent turns with ready-to-run utilities</li> <li>Use reference docs for depth: Keep SKILL.md scannable, put details in reference/</li> </ol>"},{"location":"guides/skills/#api-reference","title":"API Reference","text":""},{"location":"guides/skills/#session-parameter","title":"Session Parameter","text":"<pre><code>agent.session(\n    skills_dir=\"skills\",  # Path to skills directory (str or Path)\n    ...\n)\n</code></pre>"},{"location":"guides/skills/#skillmetadata","title":"SkillMetadata","text":""},{"location":"guides/skills/#stirrup.skills.SkillMetadata","title":"stirrup.skills.SkillMetadata  <code>dataclass</code>","text":"<pre><code>SkillMetadata(name: str, description: str, path: str)\n</code></pre> <p>Metadata extracted from a skill's SKILL.md frontmatter.</p>"},{"location":"guides/skills/#loading-functions","title":"Loading Functions","text":""},{"location":"guides/skills/#stirrup.skills.load_skills_metadata","title":"stirrup.skills.load_skills_metadata","text":"<pre><code>load_skills_metadata(\n    skills_dir: Path,\n) -&gt; list[SkillMetadata]\n</code></pre> <p>Scan skills directory for SKILL.md files and extract metadata.</p> <p>Parameters:</p> Name Type Description Default <code>skills_dir</code> <code>Path</code> <p>Path to the skills directory</p> required <p>Returns:</p> Type Description <code>list[SkillMetadata]</code> <p>List of SkillMetadata for each valid skill found.</p> <code>list[SkillMetadata]</code> <p>Returns empty list if skills_dir doesn't exist or has no skills.</p>"},{"location":"guides/skills/#stirrup.skills.format_skills_section","title":"stirrup.skills.format_skills_section","text":"<pre><code>format_skills_section(skills: list[SkillMetadata]) -&gt; str\n</code></pre> <p>Format skills metadata as a system prompt section.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>list[SkillMetadata]</code> <p>List of skill metadata to include</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string for inclusion in system prompt.</p> <code>str</code> <p>Returns empty string if no skills provided.</p>"},{"location":"guides/sub-agents/","title":"Sub-Agents","text":"<p>Sub-agents allow you to break complex tasks into specialized agents that work together.</p>"},{"location":"guides/sub-agents/#overview","title":"Overview","text":"<p>Any <code>Agent</code> can be converted to a <code>Tool</code> using the <code>.to_tool()</code> method. This allows a parent agent to delegate tasks to specialized child agents.</p>"},{"location":"guides/sub-agents/#basic-pattern","title":"Basic Pattern","text":"<pre><code>import asyncio\n\nfrom stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import WebToolProvider\n\n\nasync def main():\n    # Create client for OpenAI (shared across agents)\n    client = ChatCompletionsClient(\n        base_url=\"https://api.openai.com/v1\",\n        model=\"gpt-5\",\n    )\n\n    # Create a specialist agent\n    researcher = Agent(\n        client=client,\n        name=\"researcher\",\n        tools=[WebToolProvider()],\n        system_prompt=\"You are a research specialist. Find accurate information.\",\n    )\n\n    # Convert to a tool for the parent\n    research_tool = researcher.to_tool(\n        description=\"Research a topic and return findings\",\n    )\n\n    # Parent agent uses the sub-agent\n    orchestrator = Agent(\n        client=client,\n        name=\"orchestrator\",\n        tools=[research_tool],\n    )\n\n    async with orchestrator.session() as session:\n        await session.run(\n            \"Research the latest developments in quantum computing\"\n        )\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/sub-agents/#the-to_tool-method","title":"The <code>to_tool()</code> Method","text":"<pre><code>agent.to_tool(\n    description=\"Description of what this sub-agent does\",\n    system_prompt=None,  # Optional override for sub-agent's system prompt\n)\n</code></pre> <p>Returns a <code>Tool[SubAgentParams, SubAgentMetadata]</code>.</p>"},{"location":"guides/sub-agents/#subagentparams","title":"SubAgentParams","text":"<p>When the parent calls the sub-agent tool, it provides:</p> <pre><code>class SubAgentParams(BaseModel):\n    task: str              # The task for the sub-agent\n    input_files: list[str] # Files to pass from parent to sub-agent\n</code></pre>"},{"location":"guides/sub-agents/#subagentmetadata","title":"SubAgentMetadata","text":"<p>The sub-agent returns metadata about its execution:</p> <pre><code>class SubAgentMetadata(BaseModel):\n    message_history: list[list[ChatMessage]]  # Sub-agent's conversation\n    run_metadata: dict[str, list[Any]]        # Tool metadata from sub-agent\n</code></pre>"},{"location":"guides/sub-agents/#file-transfer-between-agents","title":"File Transfer Between Agents","text":"<p>Critical Requirement</p> <p>If a sub-agent has a code execution environment and produces files, the parent agent must also have a <code>CodeExecToolProvider</code>.</p>"},{"location":"guides/sub-agents/#parent-without-code-exec-no-file-transfer","title":"Parent Without Code Exec (No File Transfer)","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import WebToolProvider\n\nclient = ChatCompletionsClient(\n    base_url=\"https://api.openai.com/v1\",\n    model=\"gpt-5\",\n)\n\n# Sub-agent does web research only (no files produced)\nresearcher = Agent(\n    client=client,\n    name=\"researcher\",\n    tools=[WebToolProvider()],  # No code exec\n)\n\n# Parent doesn't need code exec\nparent = Agent(\n    client=client,\n    name=\"parent\",\n    tools=[researcher.to_tool(description=\"Web researcher\")],\n)\n</code></pre>"},{"location":"guides/sub-agents/#parent-with-code-exec-file-transfer-enabled","title":"Parent With Code Exec (File Transfer Enabled)","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider\n\nclient = ChatCompletionsClient(\n    base_url=\"https://api.openai.com/v1\",\n    model=\"gpt-5\",\n)\n\n# Sub-agent creates files\ncoder = Agent(\n    client=client,\n    name=\"coder\",\n    tools=[LocalCodeExecToolProvider()],\n)\n\n# Parent MUST have code exec to receive files\nparent = Agent(\n    client=client,\n    name=\"parent\",\n    tools=[\n        LocalCodeExecToolProvider(),  # Required!\n        coder.to_tool(description=\"Write and test code\"),\n    ],\n)\n</code></pre>"},{"location":"guides/sub-agents/#how-file-transfer-works","title":"How File Transfer Works","text":"<ol> <li>Sub-agent creates files in its execution environment</li> <li>Sub-agent calls finish tool with <code>paths</code> listing the files</li> <li>Files are automatically transferred to parent's execution environment</li> <li>Parent can reference these files in subsequent operations</li> </ol>"},{"location":"guides/sub-agents/#multiple-sub-agents","title":"Multiple Sub-Agents","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, WebToolProvider\n\nclient = ChatCompletionsClient(\n    base_url=\"https://api.openai.com/v1\",\n    model=\"gpt-5\",\n)\n\n# Specialist agents\nresearcher = Agent(\n    client=client,\n    name=\"researcher\",\n    tools=[WebToolProvider()],\n)\n\ncoder = Agent(\n    client=client,\n    name=\"coder\",\n    tools=[LocalCodeExecToolProvider()],\n)\n\nreviewer = Agent(\n    client=client,\n    name=\"reviewer\",\n    tools=[LocalCodeExecToolProvider()],\n)\n\n# Orchestrator with all sub-agents\norchestrator = Agent(\n    client=client,\n    name=\"orchestrator\",\n    tools=[\n        LocalCodeExecToolProvider(),  # For file transfer\n        researcher.to_tool(description=\"Research a topic\"),\n        coder.to_tool(description=\"Write code based on requirements\"),\n        reviewer.to_tool(description=\"Review and test code\"),\n    ],\n)\n\nasync with orchestrator.session(output_dir=\"./output\") as session:\n    await session.run(\n        \"Research best practices for Python CLI tools, write one, and review it\"\n    )\n</code></pre>"},{"location":"guides/sub-agents/#nested-sub-agents","title":"Nested Sub-Agents","text":"<p>Sub-agents can themselves have sub-agents:</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider, WebToolProvider\n\nclient = ChatCompletionsClient(\n    base_url=\"https://api.openai.com/v1\",\n    model=\"gpt-5\",\n)\n\n# Level 2: Specialist\ndata_fetcher = Agent(\n    client=client,\n    name=\"data_fetcher\",\n    tools=[WebToolProvider()],\n)\n\n# Level 1: Analyst uses data fetcher\nanalyst = Agent(\n    client=client,\n    name=\"analyst\",\n    tools=[\n        LocalCodeExecToolProvider(),\n        data_fetcher.to_tool(description=\"Fetch data from APIs\"),\n    ],\n)\n\n# Level 0: Orchestrator uses analyst\norchestrator = Agent(\n    client=client,\n    name=\"orchestrator\",\n    tools=[\n        LocalCodeExecToolProvider(),\n        analyst.to_tool(description=\"Analyze data and produce reports\"),\n    ],\n)\n</code></pre>"},{"location":"guides/sub-agents/#accessing-sub-agent-results","title":"Accessing Sub-Agent Results","text":"<p>Sub-agent metadata is included in the parent's run metadata:</p> <pre><code>finish_params, history, metadata = await session.run(\"Use sub-agent\")\n\nfrom stirrup import aggregate_metadata\n\naggregated = aggregate_metadata(metadata)\n\n# Access sub-agent metadata\nif \"researcher\" in aggregated:\n    sub_meta = aggregated[\"researcher\"]\n    print(f\"Sub-agent history: {len(sub_meta.message_history)} groups\")\n</code></pre>"},{"location":"guides/sub-agents/#use-cases","title":"Use Cases","text":""},{"location":"guides/sub-agents/#research-code-test-pipeline","title":"Research \u2192 Code \u2192 Test Pipeline","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import LocalCodeExecToolProvider\n\nclient = ChatCompletionsClient(base_url=\"https://api.openai.com/v1\", model=\"gpt-5\")\n\n# Each agent specializes in one task\nresearcher = Agent(client=client, name=\"researcher\", ...)\ncoder = Agent(client=client, name=\"coder\", ...)\ntester = Agent(client=client, name=\"tester\", ...)\n\n# Orchestrator coordinates the workflow\norchestrator = Agent(\n    client=client,\n    name=\"orchestrator\",\n    tools=[\n        LocalCodeExecToolProvider(),\n        researcher.to_tool(description=\"Research requirements\"),\n        coder.to_tool(description=\"Implement the solution\"),\n        tester.to_tool(description=\"Write and run tests\"),\n    ],\n)\n</code></pre>"},{"location":"guides/sub-agents/#parallel-research","title":"Parallel Research","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\nclient = ChatCompletionsClient(base_url=\"https://api.openai.com/v1\", model=\"gpt-5\")\n\n# Multiple research specialists\nweb_researcher = Agent(client=client, name=\"web_researcher\", ...)\ndoc_researcher = Agent(client=client, name=\"doc_researcher\", ...)\n\n# Parent can call them in any order\nparent = Agent(\n    client=client,\n    name=\"coordinator\",\n    tools=[\n        web_researcher.to_tool(description=\"Search the web\"),\n        doc_researcher.to_tool(description=\"Search documentation\"),\n    ],\n)\n</code></pre>"},{"location":"guides/sub-agents/#limitations","title":"Limitations","text":"<ul> <li>Sub-agent runs are synchronous (parent waits for completion)</li> <li>All sub-agent messages are returned to parent (may use context)</li> <li>File transfer only works with code execution environments</li> </ul>"},{"location":"guides/sub-agents/#next-steps","title":"Next Steps","text":"<ul> <li>Code Execution - Execution backend options</li> <li>MCP Integration - External tool servers</li> </ul>"},{"location":"guides/tool-providers/","title":"Tool Providers","text":"<p>Tool Providers manage resources and return tools via async context manager. Use them when your tools need lifecycle management.</p>"},{"location":"guides/tool-providers/#when-to-use-toolproviders","title":"When to Use ToolProviders","text":"<p>Use a <code>ToolProvider</code> when your tool requires:</p> <ul> <li>Connections - HTTP clients, database connections, websockets</li> <li>Temporary resources - Temp directories, sandboxes, processes</li> <li>Cleanup logic - Releasing resources, closing connections</li> <li>Shared state - State shared across multiple tool calls</li> </ul>"},{"location":"guides/tool-providers/#toolprovider-protocol","title":"ToolProvider Protocol","text":"<p>All providers must implement the <code>ToolProvider</code> protocol:</p> Member Description <code>has_lifecycle = True</code> Required marker to identify as a provider <code>__aenter__()</code> Setup resources and return <code>Tool</code> or <code>list[Tool]</code> <code>__aexit__()</code> Cleanup resources (close connections, delete temp files)"},{"location":"guides/tool-providers/#basic-example","title":"Basic Example","text":"<p>An HTTP client provider that shares a connection pool:</p> <pre><code>import httpx\nfrom pydantic import BaseModel, Field\nfrom stirrup import Tool, ToolResult, ToolUseCountMetadata\n\n\nclass FetchParams(BaseModel):\n    url: str = Field(description=\"URL to fetch\")\n\n\nclass HTTPToolProvider:\n    \"\"\"Provides an HTTP fetch tool with shared client.\"\"\"\n\n    has_lifecycle = True  # Required marker\n\n    def __init__(self, timeout: float = 30.0):\n        self.timeout = timeout\n        self._client: httpx.AsyncClient | None = None\n\n    async def __aenter__(self) -&gt; Tool:\n        # Setup: create HTTP client\n        self._client = httpx.AsyncClient(timeout=self.timeout)\n\n        async def fetch(params: FetchParams) -&gt; ToolResult[ToolUseCountMetadata]:\n            response = await self._client.get(params.url)\n            return ToolResult(\n                content=response.text[:5000],  # Truncate for context\n                metadata=ToolUseCountMetadata(),\n            )\n\n        return Tool(\n            name=\"http_fetch\",\n            description=\"Fetch content from a URL\",\n            parameters=FetchParams,\n            executor=fetch,\n        )\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        # Cleanup: close HTTP client\n        if self._client:\n            await self._client.aclose()\n            self._client = None\n</code></pre>"},{"location":"guides/tool-providers/#returning-multiple-tools","title":"Returning Multiple Tools","text":"<p>A provider can return multiple related tools:</p> <pre><code>class DatabaseToolProvider:\n    \"\"\"Provides query and insert tools for a database.\"\"\"\n\n    has_lifecycle = True\n\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self._conn = None\n\n    async def __aenter__(self) -&gt; list[Tool]:\n        self._conn = await connect(self.connection_string)\n\n        return [\n            self._create_query_tool(),\n            self._create_insert_tool(),\n        ]\n\n    def _create_query_tool(self) -&gt; Tool:\n        async def query(params: QueryParams) -&gt; ToolResult[ToolUseCountMetadata]:\n            results = await self._conn.execute(params.sql)\n            return ToolResult(content=str(results), metadata=ToolUseCountMetadata())\n\n        return Tool(\n            name=\"db_query\",\n            description=\"Execute a SQL query\",\n            parameters=QueryParams,\n            executor=query,\n        )\n\n    def _create_insert_tool(self) -&gt; Tool:\n        # Similar implementation...\n        pass\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        if self._conn:\n            await self._conn.close()\n</code></pre>"},{"location":"guides/tool-providers/#using-with-agents","title":"Using with Agents","text":"<p>The agent's <code>session()</code> automatically manages ToolProvider lifecycle:</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\n\n# Provider is set up when session starts, cleaned up when it ends\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"my_agent\",\n    tools=[\n        HTTPToolProvider(timeout=60),\n        DatabaseToolProvider(\"postgresql://...\"),\n    ],\n)\n\nasync with agent.session() as session:\n    # Tools are available here\n    await session.run(\"Fetch data from the API and store in database\")\n# Providers are cleaned up automatically\n</code></pre>"},{"location":"guides/tool-providers/#built-in-toolproviders","title":"Built-in ToolProviders","text":"<p>Stirrup includes several ToolProviders:</p> Provider Tools Provided Description <code>LocalCodeExecToolProvider</code> <code>code_exec</code> Local temp directory execution <code>DockerCodeExecToolProvider</code> <code>code_exec</code> Docker container execution <code>E2BCodeExecToolProvider</code> <code>code_exec</code> E2B cloud sandbox <code>WebToolProvider</code> <code>web_fetch</code>, <code>web_search</code> Web tools with shared client <code>ViewImageToolProvider</code> <code>view_image</code> View images from exec env <code>MCPToolProvider</code> varies MCP server tools"},{"location":"guides/tool-providers/#mixing-tools-and-providers","title":"Mixing Tools and Providers","text":"<p>You can mix regular tools and providers:</p> <pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL, WebToolProvider\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"mixed_agent\",\n    tools=[\n        CALCULATOR_TOOL,           # Regular tool (no lifecycle)\n        HTTPToolProvider(),        # Provider (has lifecycle)\n        WebToolProvider(),         # Built-in provider\n    ],\n)\n</code></pre>"},{"location":"guides/tool-providers/#error-handling","title":"Error Handling","text":"<p>Handle setup/cleanup errors gracefully:</p> <pre><code>class RobustProvider:\n    has_lifecycle = True\n\n    async def __aenter__(self) -&gt; Tool:\n        try:\n            self._resource = await acquire_resource()\n        except ConnectionError as e:\n            # Log and re-raise with context\n            raise RuntimeError(f\"Failed to connect: {e}\") from e\n\n        return self._create_tool()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        if self._resource:\n            try:\n                await self._resource.close()\n            except Exception:\n                # Log but don't raise during cleanup\n                pass\n</code></pre>"},{"location":"guides/tool-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Code Execution - Execution backend providers</li> <li>MCP Integration - MCP server provider</li> <li>Extending Backends - Custom execution backends</li> </ul>"},{"location":"guides/tools/","title":"Creating Tools","text":"<p>This guide covers how to create custom tools for your agents.</p>"},{"location":"guides/tools/#tool-anatomy","title":"Tool Anatomy","text":"<p>A <code>Tool</code> consists of four parts:</p> Component Type Description <code>name</code> <code>str</code> Unique identifier for the tool <code>description</code> <code>str</code> What the tool does (shown to the LLM) <code>parameters</code> <code>type[BaseModel]</code> Pydantic model defining input schema (defaults to <code>EmptyParams</code>) <code>executor</code> <code>Callable</code> Function that executes the tool"},{"location":"guides/tools/#basic-example","title":"Basic Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom stirrup import Tool, ToolResult, ToolUseCountMetadata\n\n\nclass GreetParams(BaseModel):\n    \"\"\"Parameters for the greet tool.\"\"\"\n    name: str = Field(description=\"Name of the person to greet\")\n    formal: bool = Field(default=False, description=\"Use formal greeting\")\n\n\ndef greet(params: GreetParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    if params.formal:\n        greeting = f\"Good day, {params.name}.\"\n    else:\n        greeting = f\"Hey {params.name}!\"\n\n    return ToolResult(\n        content=greeting,\n        metadata=ToolUseCountMetadata(),\n    )\n\n\nGREET_TOOL = Tool(\n    name=\"greet\",\n    description=\"Greet someone by name\",\n    parameters=GreetParams,\n    executor=greet,\n)\n</code></pre>"},{"location":"guides/tools/#parameter-schemas","title":"Parameter Schemas","text":"<p>Use Pydantic models with <code>Field</code> descriptions to define tool parameters. The descriptions are included in the tool schema sent to the LLM.</p>"},{"location":"guides/tools/#required-vs-optional-parameters","title":"Required vs Optional Parameters","text":"<pre><code>class SearchParams(BaseModel):\n    query: str = Field(description=\"Search query\")  # Required\n    max_results: int = Field(default=10, description=\"Max results\")  # Optional\n    include_images: bool = Field(default=False, description=\"Include images\")\n</code></pre>"},{"location":"guides/tools/#complex-types","title":"Complex Types","text":"<pre><code>from typing import Literal\n\nclass AnalyzeParams(BaseModel):\n    text: str = Field(description=\"Text to analyze\")\n    language: Literal[\"en\", \"es\", \"fr\"] = Field(description=\"Language code\")\n    options: list[str] = Field(default_factory=list, description=\"Analysis options\")\n</code></pre>"},{"location":"guides/tools/#annotated-types","title":"Annotated Types","text":"<pre><code>from typing import Annotated\n\nclass CalculateParams(BaseModel):\n    expression: Annotated[str, Field(description=\"Mathematical expression\")]\n    precision: Annotated[int, Field(default=2, ge=0, le=10, description=\"Decimal places\")]\n</code></pre>"},{"location":"guides/tools/#parameterless-tools","title":"Parameterless Tools","text":"<p>For tools that don't require any parameters, use <code>EmptyParams</code>:</p> <pre><code>from stirrup import Tool, ToolResult, ToolUseCountMetadata, EmptyParams\n\nTIME_TOOL = Tool[EmptyParams, ToolUseCountMetadata](\n    name=\"get_time\",\n    description=\"Get the current time\",\n    executor=lambda _: ToolResult(\n        content=datetime.now().isoformat(),\n        metadata=ToolUseCountMetadata(),\n    ),\n)\n</code></pre> <p>Since <code>parameters</code> defaults to <code>EmptyParams</code>, you can also omit it:</p> <pre><code>TIME_TOOL = Tool(\n    name=\"get_time\",\n    description=\"Get the current time\",\n    executor=lambda _: ToolResult(content=datetime.now().isoformat()),\n)\n</code></pre>"},{"location":"guides/tools/#sync-vs-async-executors","title":"Sync vs Async Executors","text":"<p>Tools can use either synchronous or asynchronous executors:</p>"},{"location":"guides/tools/#synchronous","title":"Synchronous","text":"<pre><code>def my_tool(params: MyParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    result = do_something(params)\n    return ToolResult(content=result, metadata=ToolUseCountMetadata())\n</code></pre>"},{"location":"guides/tools/#asynchronous","title":"Asynchronous","text":"<pre><code>async def my_async_tool(params: MyParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    result = await do_something_async(params)\n    return ToolResult(content=result, metadata=ToolUseCountMetadata())\n</code></pre> <p>By default, synchronous executors run in a separate thread (<code>run_sync_in_thread=True</code>).</p>"},{"location":"guides/tools/#tool-results","title":"Tool Results","text":"<p>Tools return <code>ToolResult[M]</code> where <code>M</code> is the metadata type:</p> <pre><code>from stirrup import ToolResult, ToolUseCountMetadata\n\n# Simple text result\nreturn ToolResult(\n    content=\"Operation completed successfully\",\n    metadata=ToolUseCountMetadata(),\n)\n\n# Result with structured content\nreturn ToolResult(\n    content=f\"Found {len(results)} items:\\n\" + \"\\n\".join(results),\n    metadata=ToolUseCountMetadata(),\n)\n</code></pre>"},{"location":"guides/tools/#returning-images","title":"Returning Images","text":"<pre><code>from stirrup import ImageContentBlock\n\nasync def screenshot_tool(params: ScreenshotParams) -&gt; ToolResult[ToolUseCountMetadata]:\n    image_bytes = await take_screenshot()\n\n    return ToolResult(\n        content=[\n            \"Here's the screenshot:\",\n            ImageContentBlock(data=image_bytes),\n        ],\n        metadata=ToolUseCountMetadata(),\n    )\n</code></pre>"},{"location":"guides/tools/#tool-metadata","title":"Tool Metadata","text":"<p>Metadata is aggregated across tool calls in a run. Use it to track usage statistics.</p>"},{"location":"guides/tools/#built-in-metadata","title":"Built-in Metadata","text":"<pre><code>from stirrup import ToolUseCountMetadata\n\n# Tracks number of times tool was called\nreturn ToolResult(content=\"done\", metadata=ToolUseCountMetadata())\n</code></pre>"},{"location":"guides/tools/#custom-metadata","title":"Custom Metadata","text":"<p>Create custom metadata by implementing the <code>Addable</code> protocol:</p> <pre><code>from stirrup import Addable\nfrom pydantic import BaseModel\n\n\nclass APICallMetadata(BaseModel, Addable):\n    \"\"\"Track API calls and costs.\"\"\"\n    calls: int = 1\n    tokens_used: int = 0\n    cost_usd: float = 0.0\n\n    def __add__(self, other: \"APICallMetadata\") -&gt; \"APICallMetadata\":\n        return APICallMetadata(\n            calls=self.calls + other.calls,\n            tokens_used=self.tokens_used + other.tokens_used,\n            cost_usd=self.cost_usd + other.cost_usd,\n        )\n\n\nasync def api_tool(params: APIParams) -&gt; ToolResult[APICallMetadata]:\n    response = await call_api(params)\n\n    return ToolResult(\n        content=response.text,\n        metadata=APICallMetadata(\n            tokens_used=response.tokens,\n            cost_usd=response.cost,\n        ),\n    )\n</code></pre>"},{"location":"guides/tools/#accessing-aggregated-metadata","title":"Accessing Aggregated Metadata","text":"<pre><code>from stirrup import aggregate_metadata\n\nfinish_params, history, metadata = await session.run(\"task\")\n\n# metadata is dict[str, list[Any]] - tool_name -&gt; list of metadata objects\naggregated = aggregate_metadata(metadata)\n\n# Access aggregated values\nprint(f\"API calls: {aggregated['api_tool'].calls}\")\nprint(f\"Total cost: ${aggregated['api_tool'].cost_usd:.2f}\")\n</code></pre>"},{"location":"guides/tools/#using-tools-with-agents","title":"Using Tools with Agents","text":""},{"location":"guides/tools/#adding-to-default-tools","title":"Adding to Default Tools","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import DEFAULT_TOOLS\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"my_agent\",\n    tools=[*DEFAULT_TOOLS, GREET_TOOL, MY_OTHER_TOOL],\n)\n</code></pre>"},{"location":"guides/tools/#replacing-default-tools","title":"Replacing Default Tools","text":"<pre><code>from stirrup import Agent\nfrom stirrup.clients.chat_completions_client import ChatCompletionsClient\nfrom stirrup.tools import CALCULATOR_TOOL\n\nclient = ChatCompletionsClient(model=\"gpt-5\")\nagent = Agent(\n    client=client,\n    name=\"custom_agent\",\n    tools=[GREET_TOOL, CALCULATOR_TOOL],  # Only these tools available\n)\n</code></pre>"},{"location":"guides/tools/#next-steps","title":"Next Steps","text":"<ul> <li>Tool Providers - Tools with lifecycle management</li> <li>Code Execution - Execution backends</li> <li>Extending Tools - Advanced tool patterns</li> </ul>"}]}